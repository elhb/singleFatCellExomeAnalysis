#!/usr/bin/env python

helpMessage = """
############ fnuttglugg ################
a pipe for DNA sequence analysis

use:
fnuttglugg addSample <sampleName>
fnuttglugg addFastq <sampleName> <r1> <r2>
fnuttglugg createScripts <uppmaxAccount> <emailAdress>
fnuttglugg submitScripts
...more to come ...
fnuttglugg report
"""

def main():
    
    import sys
    
    for term in ['h','help','-h','--help']:
	if term in sys.argv:
	    print helpMessage
	    sys.exit()

    app = AnalysisPipe()
    app.run()

def extractData(infile=None,pattern=None):
    import re
    try:
	with open(infile) as infile:
	    data = infile.read()
	    p = re.compile(pattern)
	    m = p.match(data)
	    if m: return m.groupdict()
	    else: return 'NoMatchFound'
    except IOError, e:
	assert e.errno == 2;
	return 'NoFileFound'

def bufcount(filename):
	""" returns the number of lines in a file
	"""
	import gzip
	if filename.split('.')[-1] in ['gz','gzip']: f = gzip.open(filename)
	else: f = open(filename)
	lines = 0
	buf_size = 1024 * 1024
	read_f = f.read # loop optimization
	
	buf = read_f(buf_size)
	while buf:
		lines += buf.count('\n')
		buf = read_f(buf_size)
		f.close
	return lines

def submitSbatch(filename,dependency=None):
    import subprocess
    import sys
    if dependency: command = ['sbatch','--dependency=afterok:'+':'.join(dependency),filename]
    else:          command = ['sbatch',filename]
    sbatch = subprocess.Popen( command, stdout=subprocess.PIPE, stderr=subprocess.PIPE )
    sbatch_out, errdata = sbatch.communicate()
    if sbatch.returncode != 0:
	print 'sbatch view Error code', sbatch.returncode, errdata
	print sbatch_out
	print filename
	sys.exit()
    jobid = sbatch_out.split('\n')[0].split(' ')[3]
    return jobid

class AnalysisPipe(object):

    def __init__(self):
	
	import sys
	import os
	import time

	try:
	    path = sys.argv[1]
	    path = os.path.abspath(path)
	    if path[-1] == '/': path = path[:-1]
	    AnalysisPipe.path = path
	    AnalysisPipe.scriptPath = '/'+'/'.join(os.path.abspath(sys.argv[0]).split('/')[:-1])
	    AnalysisPipe.referencePath = '/'+'/'.join(os.path.abspath(sys.argv[0]).split('/')[:-2])+'/references'
	except IndexError:
	    sys.stderr.write('#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a path on your commandline (currently: "'+' '.join(sys.argv)+'")\n')
	    sys.exit(1)

	AnalysisPipe.bowtie2Reference = '~/singleFatCellExomeAnalysis/references/GATKbundle/human_g1k_v37.fasta'
	AnalysisPipe.picardLocation = '~/bin/picard-tools-1.114'
	AnalysisPipe.gatkLocation = '~/singleFatCellExomeAnalysis/bin/GenomeAnalysisTK-3.1-1/GenomeAnalysisTK.jar'
	AnalysisPipe.gatkBundleLocation = '~/singleFatCellExomeAnalysis/references/GATKbundle/'

	self.openLogfileConnection()
	
	return None

    def getDataBase(self):

	import os
	import time

	AnalysisPipe.database = Database(self.path+'/data.db')
	if not os.path.exists(AnalysisPipe.database.path): AnalysisPipe.database.create()

    def run(self, ):
	
	import time
	import sys
	
	try: self.action = sys.argv[2]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply an action on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	
	self.getDataBase()
	
	import os
	
	AnalysisPipe.database.addToRunsTable(time.time(),self.action,' '.join(sys.argv),False,os.getpid())
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Running '+' '.join(sys.argv)+'\n')

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Action is '+sys.argv[2]+'\n')
	if self.action == 'addSample':self.addSample()
	elif self.action == 'addFastq':self.addFastq()
	elif self.action == 'report':self.report()
	elif self.action == 'createScripts':self.createScripts()
	elif self.action == 'submitScripts':self.submitScripts()
	else:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a Valid action on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

    def addFastq(self):

	import sys
	import time
	
	try: sample = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a sample name on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	try:
	    f1 = sys.argv[4]
	    f2 = sys.argv[5]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a pair of fastq files on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	AnalysisPipe.database.addFastqs(sample,f1,f2)

    def addSample(self, ):
	import sys
	import time
	AnalysisPipe.database.addSample(sys.argv[3])

    def openLogfileConnection(self,):
        """ open a connection to the logfile, creates a logfile if none is present """
        
        #
        # Imports
        #
        import sys
        import time
        import os
        
        #
        # for logmessages
        #        
        tmpLogMessages = []
        
        #
        # check if logfile present open connection or create
        #
        AnalysisPipe.logfile = self.path + '/logfile.txt'
        if os.path.isfile(AnalysisPipe.logfile):
            if 'initiateAnalysis' in sys.argv:
                sys.stderr.write('#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# The logfile already exists please use another path to initiate the analysis.\n')
                sys.exit(1)
            else:
                AnalysisPipe.logfile = open(AnalysisPipe.logfile,'a',1)
                AnalysisPipe.logfile.write('----------------\n#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Connection to logfile '+AnalysisPipe.logfile.name+' opened.\n')
                return 0
        else:
            tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Creating the logfile "'+AnalysisPipe.logfile+'".'
            tmpLogMessages.append(tmpLogMessage)
            tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Looking for folder '+self.path+'...'
            tmpLogMessages.append(tmpLogMessage)
	    if not os.path.isdir(self.path):
		tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Path '+self.path+' not found creating...'
		tmpLogMessages.append(tmpLogMessage)
		os.mkdir(self.path)
            AnalysisPipe.logfile = open(AnalysisPipe.logfile,'w',1)
        
	AnalysisPipe.logfile.write('\n'.join(tmpLogMessages)+'\n')
	
        return tmpLogMessages

    def report(self, ):

	import time
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Creating a report located at '+self.path+'/report.htm'+' \n')
	import operator
    
	samples = AnalysisPipe.database.getSamples()
	samplesbyName = {}
	samplesbyId = {}
	for sample in samples:
	    samplesbyId[sample.id]=sample
	    samplesbyName[sample.name]=sample
	    sample.getStats()
	
	reportFile = open(self.path+'/report.htm','w')
	reportFile.write('<html>')
	reportFile.write("""<head><style>
	    body {background-color:white}
	    h1   {color:black}
	    p    {color:green}
	    table, th, td {
		border: 1px solid black;
		border-collapse: collapse;
	    }
	    th {text-align: left;background-color:darkgray;color:white;}
	    table {border-spacing: 5px;}
	    th,td {padding: 15px;}
	</style></head>""")

	reportFile.write('<body>')
	reportFile.write('<h1>Analysis Report '+self.path+'</h1>\n')
	
	reportFile.write('<h2>List of samples:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>Sample Id</th>')
	reportFile.write('<th>Sample Name</th> ')
	reportFile.write('<th>Read Count</th> ')
	reportFile.write('</tr>')
	for sampleId, sample in sorted(samplesbyId.iteritems(), key=operator.itemgetter(0)):
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(sampleId)+'</td><td>'+sample.name+'</td><td>'+str(sample.readCount)+'</td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')
	
	reportFile.write('<h2>List of fastqs:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>File Id</th>')
	reportFile.write('<th># Read Pairs</th> ')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>File Name (r1)</th>')
	reportFile.write('</tr>')
	for filePairId,readCount,fastq1,fastq2,sampleId in sorted(AnalysisPipe.database.getFastqs(), key=operator.itemgetter(0)):
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(filePairId)+'</td>'
			     '<td>'+str(readCount)+'</td>'
			     '<td>'+samplesbyId[int(sampleId)].name+'</td>'
			     '<td>'+fastq1+'</td>')
	    reportFile.write('</tr>')
	reportFile.write('</table>')
	reportFile.write('</body></html>\n')
	
	return 0

    def createScripts(self, ):

	import time
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Creating sbatch scripts:\n')

	for sample in AnalysisPipe.database.getSamples():
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# sample: '+sample.name+' ... \n')
	    try: sample.getFastqs().next()
	    except StopIteration:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		continue
	    sample.trimFastqs()
	    sample.mapFastqs()
	    sample.mergeMapped()
	    sample.filterAndFixMerged()
	    sample.realignerTargetCreator()
	    sample.reAlignAndReCalibrate()
	    sample.haplotypeCalling()
	    sample.qcSteps()

    def submitScripts(self,sampleNameOrId=None):

	import time

	allSampleDependency = []
	for sample in AnalysisPipe.database.getSamples():

	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitting sbatches for sample: '+sample.name+' ... \n')
	    try: sample.getFastqs().next()
	    except StopIteration:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		continue

	    dependency = []
	    for filePairId,readCount,fastq1,fastq2,sampleId in sample.getFastqs():

		fileName = sample.scriptPath+'/trimming.'+sample.name+'.'+str(filePairId)+'.sh'
		jobid = submitSbatch(fileName)
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted trimming of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

		fileName = sample.scriptPath+'/mapping.'+sample.name+'.'+str(filePairId)+'.sh'
		jobid = submitSbatch(fileName,dependency=[jobid])
		dependency.append(jobid)
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted mapping of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/mergeMapped.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=dependency)
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted mering of mapped data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/FilterAndFix.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted fixing the of merged data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/realignerTargetCreator.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted realignerTargetCreator of data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/reAlignAndReCalibrate.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted reAlignAndReCalibrate of data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/haplotypeCalling.'+sample.name+'.sh'
	    hapJobid = submitSbatch(fileName,dependency=[jobid])
	    allSampleDependency.append(hapJobid)
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted haplotypecalling for '+sample.name+' with job id '+str(jobid)+' \n')
	    
	    fileName = sample.scriptPath+'/qcSteps.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted qc steps for '+sample.name+' with job id '+str(jobid)+' \n')

class Database(object):
    
    def __init__(self, dbPath):
	self.path = dbPath

    def getConnection(self,):
	#
	# Import useful stuff
	#
	import sqlite3
	import sys

	#
	# Create database and set
	#
	try: self.conn = sqlite3.connect(self.path)
	except sqlite3.OperationalError:
	    print 'ERROR: Trouble with the database, plase check your commandline.'
	    sys.exit()
	self.c = self.conn.cursor()

    def commitAndClose(self,):
	#
	# commit changes and close connection
	#
	self.conn.commit()
	self.conn.close()

    def create(self,):
	""" creates the database holding all information used in the analysis """
	
	self.getConnection()
	
	#
	# Create tables
	#
	self.c.execute('''CREATE TABLE runs (startTime,command,commandLine,finishedSuccessfully,masterPid)''')
	self.c.execute('''CREATE TABLE fastqs (filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId,PRIMARY KEY (filePairId))''');
	self.c.execute('''CREATE TABLE settings (variableName,defaultValue,value,setTime,PRIMARY KEY (variableName))''')
	self.c.execute('''CREATE TABLE results (resultName,defaultValue,value,setTime,PRIMARY KEY (resultName))''')
	self.c.execute('''CREATE TABLE samples (sampleId,sampleName,PRIMARY KEY (sampleId))''')
	
	self.commitAndClose()

    def addToRunsTable(self, startTime, command, commandLine, finishedSuccessfully, masterPid):
	
	self.getConnection()
	
	#
	# check if pid already in database
	#
	t = (masterPid,)
	data = self.c.execute('SELECT masterPid, startTime FROM runs WHERE masterPid=?',t).fetchall()        
	if data:
	    for tmp1,tmp2 in data:

	#
	# if pid and startTime matches update the "finishedSuccessfully" entry
	#
		if tmp1 == masterPid and tmp2 == startTime:
		    values = (startTime, command, commandLine, finishedSuccessfully, masterPid)
		    self.c.execute('UPDATE runs SET finishedSuccessfully=? WHERE masterPid=? AND startTime=?', (finishedSuccessfully,masterPid,startTime))
	
	#
	# if not in the database add a new row
	#
	else:
	    values = (startTime, command, commandLine, finishedSuccessfully, masterPid)
	    self.c.execute('INSERT INTO runs VALUES (?,?,?,?,?)', values)
	
	self.commitAndClose()
	
	return 0

    def addSample(self, newSampleName):
	
	#
	# Imports
	#
	import sys
	import time
	
	#
	# open connection to database
	#
	self.getConnection()
	
	sampleNames = []
	sampleIds = []
	
	#
	# check if any of the fastqs already in database
	#
	data = self.c.execute('SELECT sampleId,sampleName FROM samples').fetchall()
	if data:
	    for (sampleId,sampleName) in data:
		#sampleName = sampleName[0]
		sampleNames.append(sampleName)
		sampleIds.append(sampleId)
	    if newSampleName in sampleNames:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# SampleName must be uniq, there is already a sample with name '+newSampleName+' , exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	
	if sampleIds:  sampleId = max(sampleIds)+1
	else:          sampleId = 0 
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Adding sample '+newSampleName+' to database with id '+str(sampleId)+'.\n')
	values = (sampleId,newSampleName)
	self.c.execute('INSERT INTO samples VALUES (?,?)', values)
	
	sample = Sample(sampleName=newSampleName, sampleId=sampleId)
	sample.createDirs()
	
	self.commitAndClose()
	
	return 0

    def getSamples(self):
	#
	# Imports
	#
	import sys
	import time
	
	#
	# open connection to database
	#
	self.getConnection()
	
	samples = []
	
	data = self.c.execute('SELECT sampleId,sampleName FROM samples').fetchall()
	if data:
	    for (sampleId,sampleName) in data: samples.append( Sample(sampleName=sampleName,sampleId=int(sampleId)) )
	
	self.commitAndClose()
	
	return samples

    def addFastqs(self, sampleNameOrId, fastq1, fastq2):
	
	#
	# Imports
	#
	import sys
	import os
	import time
	
	fastq1 = os.path.abspath(fastq1)
	fastq2 = os.path.abspath(fastq2)
	
	samples = AnalysisPipe.database.getSamples()
	samplesbyName = {}
	samplesbyId = {}
	for sample in samples:
	    samplesbyId[sample.id]=sample
	    samplesbyName[sample.name]=sample
	sampleName = None
	sampleId = None
	if int(sampleNameOrId) in [int(value.id)  for value in samplesbyName.values()]:sampleId = sampleNameOrId; sampleName = samplesbyId[int(sampleId)].name
	elif   sampleNameOrId  in samplesbyName.keys():sampleName = sampleNameOrId; sampleId = samplesbyName[sampleName].id
	else:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# SampleName (or id) must be registered in the database, there is no sample with name or id '+str(sampleNameOrId)+' , exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	#
	# open connection to database
	#
	self.getConnection()
	
	filePairId = None
	filePairIds = []
	
	#
	# check if any of the fastqs already in database
	#
	data = self.c.execute('SELECT filePairId,fastq1,fastq2 FROM fastqs').fetchall()
	if data:
	    for filePair in data:
		filePairId = int(filePair[0])
		filePairIds.append(filePairId)
		for fastq in [fastq1, fastq2]:
		    if fastq in filePair:
			message = 'ERROR: '+fastq+' already in the database.\nExiting after error.'
			print message
			AnalysisPipe.logfile.write(message+'\n')
			sys.exit(1)
	#
	# if not in the database add a new row
	#
	AnalysisPipe.logfile.write('Getting readcount for file'+fastq1+' ... \n')
	readCount = bufcount(fastq1)/4 #one read is four lines
	AnalysisPipe.logfile.write('...done. The file has '+str(readCount)+' reads.\n')
	addedToReadsTable = False#SEAseqPipeLine.startTimeStr
	minReadLength = 'NA'

	if filePairIds: filePairId = max(filePairIds)+1
	else: filePairId = 0
	values = (filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId)
	self.c.execute('INSERT INTO fastqs VALUES (?,?,?,?,?,?,?)', values)
	
	self.commitAndClose()
	
	return 0

    def getFastqs(self,):
	#
	# Imports
	#
	import sys
	
	#
	# open connection to database
	#
	self.getConnection()
		
	#
	# get att data in fastqs table
	#
	filePairs = self.c.execute('SELECT filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId FROM fastqs').fetchall()
	
	self.commitAndClose()
	
	#return [[readCount,fastq1,fastq2] if (not addedToReadsTable) else None for filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength in filePairs]
	return [[filePairId,readCount,fastq1,fastq2,sampleId] for filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId in filePairs]

    def getRuns(self, runTypes):
	
	self.getConnection()
	
	runsInfo = []
	data = self.c.execute('SELECT * FROM runs').fetchall()
	for startTime, command, commandLine, finishedSuccessfully, masterPid in data:
	    if command in runTypes: runsInfo.append([startTime, command, commandLine, finishedSuccessfully, masterPid])
	
	self.commitAndClose()
	
	return runsInfo

class Sample(object):

    def __init__(self, sampleName=None,sampleId=None):
	self.name = sampleName
	self.id = int(sampleId)
	self.path = AnalysisPipe.path+'/samples/'+self.name
	self.scriptPath = AnalysisPipe.path+'/samples/'+self.name+'/script'
	self.dataPath   = AnalysisPipe.path+'/samples/'+self.name+'/data'
	self.logPath    = AnalysisPipe.path+'/samples/'+self.name+'/logs'
	self.fastqcPath = AnalysisPipe.path+'/samples/'+self.name+'/fastQC'
	self.dependencies = {}

    @property
    def readCount(self, ):
	tmpCounter = 0
	for filePairId,readCount,fastq1,fastq2,sampleId in AnalysisPipe.database.getFastqs():
	    if int(sampleId) == self.id:tmpCounter+= readCount
	return tmpCounter

    def getFastqs(self):
	self.fastqIds = []
	for filePairId,readCount,fastq1,fastq2,sampleId in AnalysisPipe.database.getFastqs():
	    if int(sampleId) == self.id:
		self.fastqIds.append(filePairId)
		yield [filePairId,readCount,fastq1,fastq2,sampleId]

    def createDirs(self):
        import os
        try: os.makedirs(self.path)
        except OSError:pass
        try: os.makedirs(self.scriptPath)
        except OSError:pass
        try: os.makedirs(self.dataPath)
        except OSError:pass
        try: os.makedirs(self.fastqcPath)
        except OSError:pass
        try: os.makedirs(self.logPath)
        except OSError:pass

    def trimFastqs(self):
	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():

	    import sys
	    import time
	    try: project = sys.argv[3]
	    except IndexError:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	    #
	    # sbatch header
	    #
	    output = ''
	    output += '#! /bin/bash -l'+'\n'
	    output += '#SBATCH -A '+project+'\n'
	    output += '#SBATCH -n 2 -p core'+'\n'
	    output += '#SBATCH -t 72:00:00'+'\n'
	    output += '#SBATCH -J trim.'+self.name+'.'+str(filePairId)+'\n'
	    output += '#SBATCH -e '+self.logPath+'/stderr.trimming.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    output += '#SBATCH -o '+self.logPath+'/stdout.trimming.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    
	    try:
		output += '#SBATCH --mail-type=All'+'\n'
		output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	    except IndexError: pass
	    
	    #
	    # define variebles and go to path
	    #
	    output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	    output += 'cd '+self.path+'\n'
	    output += 'echo "-----"'+'\n'
	    
	    output += 'module load bioinfo-tools cutadapt/1.5.0 FastQC'+'\n'

	    #
	    # WGA adapter trimming
	    #
	    output += ''+AnalysisPipe.scriptPath+'/wgaAdapterTrimmer.py -i '+fastq1+' > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq 2> '+self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += ''+AnalysisPipe.scriptPath+'/wgaAdapterTrimmer.py -i '+fastq2+' > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq 2> '+self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'
	    
	    output += '\n'
	    output += 'cutadapt -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq  2> '+self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += 'cutadapt -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq  2> '+self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'
	    output += '\n'
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq\n'
	    
	    #
	    # illumina  adapter trimming
	    #
	    adaptersToTrim = '-a CTGTCTCTTATACACATCTGACGCTGCCGACGA -a CTGTCTCTTATACACATCTCCGAGCCCACGAGAC -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -a AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT'
	    output += 'cutadapt -n 3 '+adaptersToTrim+' '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq 2> '+self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += 'cutadapt -n 3 '+adaptersToTrim+' '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq 2> '+self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'

	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq \n'
	    output += 'wait\n'
	    
	    #
	    # quality trimmming
	    #
	    output += ''+AnalysisPipe.scriptPath+'/TrimBWAstyle.pl -q 20 '+self.dataPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq 2> '+self.logPath+'/qualityTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += ''+AnalysisPipe.scriptPath+'/TrimBWAstyle.pl -q 20 '+self.dataPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq 2> '+self.logPath+'/qualityTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'
	    
	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq\n'
	    output += 'wait\n'
	    
	    #
	    # remove empty or "N" only sequences
	    #
	    output += 'python '+AnalysisPipe.scriptPath+'/removeEmptyReads.py '
	    output += self.dataPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.singletts.fq '
	    output += '>&2 2> '+self.logPath+'/qualityTrimming.'+str(filePairId)+'.removeEmptyReads.log.txt\n'
	    
	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq\n'
	    output += 'wait\n'
	    
	    #
	    # compress files
	    #
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq &\n'
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq  &\n'
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.singletts.fq &\n'
	    output += 'wait\n'
	    
	    #
	    # FASTQC
	    #
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq.gz &\n'
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq.gz &\n'
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.singletts.fq.gz &\n'
	    output += 'wait\n'
	    output += 'mv -v '+self.dataPath+'/*fastqc* '+self.fastqcPath+'/\n'
	    
	    #
	    # Final output and write script to file
	    #
	    output += 'echo'+'\n'
	    output += 'wait'+'\n'
	    output += 'echo "$(date) AllDone"'+'\n'
	    output += 'echo "$(date) AllDone" >&2'+'\n'
	    with open(self.scriptPath+'/trimming.'+self.name+'.'+str(filePairId)+'.sh','w') as outfile: outfile.write(output)

    def mapFastqs(self):

	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():

	    import sys
	    import time
	    try: project = sys.argv[3]
	    except IndexError:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	    #
	    # sbatch header
	    #
	    output = '#! /bin/bash -l'+'\n'
	    output += '#SBATCH -A '+project+'\n'
	    output += '#SBATCH -n 16 -p node'+'\n'
	    output += '#SBATCH -t 5:00:00'+'\n'
	    output += '#SBATCH -J map.'+self.name+'.'+str(filePairId)+'\n'
	    output += '#SBATCH -e '+self.logPath+'/stderr.mapping.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    output += '#SBATCH -o '+self.logPath+'/stdout.mapping.'+self.name+'.'+str(filePairId)+'.txt'+'\n'

	    try:
		output += '#SBATCH --mail-type=All'+'\n'
		output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	    except IndexError: pass

	    #
	    # define variebles and go to path
	    #
	    output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	    output += 'cd '+self.path+'\n'
	    output += 'echo'+'\n'

	    #
	    # Bowtie2 mapping
	    #output += 'module load bioinfo-tools bwa/0.7.8\n'
	    #output += 'bwa mem -t 16 /sw/data/uppnex/reference/Homo_sapiens/GRCh37/program_files/bwa/concat.fa '+self.r1files[0]+' '+self.r2files[0]+' > '+self.sam+'\n'
	    #output += 'bowtie2 -1 '+self.r1files[0]+' -2 '+self.r2files[0]+' --very-sensitive-local -p16 -x '+self.reference+' > '+self.sam+'\n'
	    output += 'bowtie2 --maxins 2000 -p16 '
	    output += '-1 '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq.gz '
	    output += '-2 '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq.gz '
	    output += '-x '+AnalysisPipe.bowtie2Reference+' '
	    output += '> '+self.dataPath+'/'+str(filePairId)+'.sam '
	    output += '2> '+self.logPath+'/stderr.bowtie2.'+str(filePairId)+'.txt \n'
	    output += 'echo -e "mapping Done. $(date) Running on: $(hostname)" 1>&2'+'\n'

	    #
	    # Final output and write script to file
	    #
	    output += 'wait'+'\n'
	    output += 'echo "$(date) AllDone"'+'\n'
	    output += 'echo "$(date) AllDone" >&2'+'\n'
	    with open(self.scriptPath+'/mapping.'+self.name+'.'+str(filePairId)+'.sh','w') as outfile: outfile.write(output)

    def mergeMapped(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	#
	# sbatch header
	#
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 5:00:00'+'\n'
	output += '#SBATCH -J merge.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.merge.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.merge.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass

	#
	# define variebles and go to path
	#
	output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	output += 'cd '+self.path+'\n'
	output += 'echo'+'\n'

	#
	# merge
	#
	inputFiles = ' INPUT='+' INPUT='.join([self.dataPath+'/'+str(filePairId)+'.sam' for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs()])
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/MergeSamFiles.jar '+inputFiles+' OUTPUT='+self.dataPath+'/'+self.name+'.merged.sam '
	output += '1>&2  2>  '+self.logPath+'/stderr.merging.'+self.name+'.txt \n'
	output += 'echo -e "mapping Done. $(date) Running on: $(hostname)" 1>&2'+'\n'

	#
	# Final output and write script to file
	#
	output += 'wait'+'\n'
	output += 'echo "$(date) AllDone"'+'\n'
	output += 'echo "$(date) AllDone" >&2'+'\n'
	with open(self.scriptPath+'/mergeMapped.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def filterAndFixMerged(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 5:00:00'+'\n'
	output += '#SBATCH -J fnf.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.filterAndFix.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.filterAndFix.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass

        #
        # define variebles and go to path
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'cd '+self.path+'\n'
        output += 'echo'+'\n'
        
        #
        # convert to bam file
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/SamFormatConverter.jar MAX_RECORDS_IN_RAM=2500000 '
	output += 'INPUT='+ self.dataPath+'/'+self.name+'.merged.sam '
	output += 'OUTPUT='+self.dataPath+'/'+self.name+'.merged.bam '
	output += '1>&2  2> '+self.logPath+'/stderr.sam2bam.'+self.name+'.txt \n'
        output += 'echo -e "sam2bam Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.merged.sam\n'

        #
        # sort the bam file
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/SortSam.jar MAX_RECORDS_IN_RAM=2500000 SORT_ORDER=coordinate '
	output += 'INPUT='+ self.dataPath+'/'+self.name+'.merged.bam '
        output += 'OUTPUT='+self.dataPath+'/'+self.name+'.sorted.bam '
	output += 'CREATE_INDEX=true 1>&2  2> '
	output += self.logPath+'/stderr.sortBam.'+self.name+'.txt \n'
        output += 'echo -e "bam2sort Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.merged.bam\n'

        #
        # mark duplicates
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/MarkDuplicates.jar MAX_RECORDS_IN_RAM=2500000 VALIDATION_STRINGENCY=LENIENT '
	output += 'INPUT='+ self.dataPath+'/'+self.name+'.sorted.bam '
	output += 'OUTPUT='+self.dataPath+'/'+self.name+'.marked.bam '
	output += 'METRICS_FILE='+self.logPath+'/markeDuplicatesMetrix.'+self.name+'.txt '
	output += '1>&2  2> '+self.logPath+'/stderr.markeDuplicates.'+self.name+'.txt \n'
        output += 'echo -e "mark Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.sorted.bam\n'

        #
        # fix missing information
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/AddOrReplaceReadGroups.jar '
        output += 'MAX_RECORDS_IN_RAM=2500000 '
        output += 'INPUT='+ self.dataPath+'/'+self.name+'.marked.bam '
        output += 'OUTPUT='+self.dataPath+'/'+self.name+'.fixed.bam '
        output += 'CREATE_INDEX=true RGID='+self.name+' RGLB='+self.name+' RGPL=ILLUMINA RGSM='+self.name+' RGCN="NA" RGPU="NA"'+'  '
	output += '1>&2  2> '+self.logPath+'/stderr.addAndReplaceReadGroups.'+self.name+'.txt \n'
        output += 'echo "addorreplace Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.marked.bam\n'

        #
        # samtools flagstat
        #
        output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.fixed.bam'+' > '+self.logPath+'/fixedBamFlagstat.'+self.name+'.txt \n'
        output += 'echo "flagstat Done. $(date) Running on: $(hostname)" 1>&2'+'\n'

        #
        # Final output and write script to file
        #
        output += 'wait'+'\n'
        output += 'echo "$(date) AllDone"'+'\n'
        output += 'echo "$(date) AllDone" >&2'+'\n'
	with open(self.scriptPath+'/FilterAndFix.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def realignerTargetCreator(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #

	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 16 -p node'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J realTC.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.realTC.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.realTC.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass

        #
        # define variebles and go to path
        #
        output += 'echo "$(date) Running on: $(hostname)"\n'

        #
        # Find targets for indel realignment
        #
        output += 'echo -e "-> RealignerTargetCreator <-"\n'
        output += 'java -Xmx72g -jar '+AnalysisPipe.gatkLocation+' -T RealignerTargetCreator '
	output += '-nt 16 '
	output += '-I '+self.dataPath+'/'+self.name+'.fixed.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-o '+self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/Mills_and_1000G_gold_standard.indels.b37.vcf'
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/1000G_phase1.indels.b37.vcf '
	output += '1>&2 2> '+self.logPath+'/stderr.RealignerTargetCreator.'+self.name+'.txt;'
        output += '\n'
        
        with open(self.scriptPath+'/realignerTargetCreator.'+self.name+'.sh','w') as outfile: outfile.write(output)
    
    def reAlignAndReCalibrate(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J reAlign.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.reAlign.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.reAlign.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'

        #
        # Realign reads around indels
        #
        output += 'echo -e "-> IndelRealigner <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T IndelRealigner '
	output += '-I '+self.dataPath+'/'+self.name+'.fixed.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-targetIntervals '+self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
        output += ' -o '+self.dataPath+'/'+self.name+'.reAligned.bam'+' '
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/Mills_and_1000G_gold_standard.indels.b37.vcf'
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/1000G_phase1.indels.b37.vcf  '
	output += '1>&2 2> '+self.logPath+'/stderr.indelRealigner.'+self.name+'.txt;'+'\n'
        output += '\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'echo "$(date) Running on: $(hostname)"\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.fixed.bam'+'\n'
        
        #
        # Quality recalibration
        #
        output += 'echo -e "-> BaseRecalibrator <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T BaseRecalibrator '
	output += '-I '+self.dataPath+'/'+self.name+'.reAligned.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-o '+self.dataPath+'/'+self.name+'.BQSR.grp'+' '
        output += ' -knownSites '+AnalysisPipe.gatkBundleLocation+'/dbsnp_138.b37.vcf '
	output += '1>&2 2> '+self.logPath+'/stderr.baseRecalibrator.'+self.name+'.txt;'+'\n'

        output += '\n'
        output += 'echo -e "-> PrintReads <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T PrintReads '
	output += '-I '+self.dataPath+'/'+self.name+'.reAligned.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-BQSR '+self.dataPath+'/'+self.name+'.BQSR.grp'+' '
	output += '-o '+self.dataPath+'/'+self.name+'.reCalibrated.bam'+' '
	output += '1>&2 2> '+self.logPath+'/stderr.printreads.txt ;\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.reAligned.bam'+'\n'

	output += 'samtools view -b -F 4 '   +self.dataPath+'/'+self.name+'.reCalibrated.bam > '+self.dataPath+'/'+self.name+'.unmapRemoved.bam  2> '+self.logPath+'/stderr.samtoolsView.removeUnmap.'+self.name+'.txt \n'
	output += 'samtools view -b -q 20 '  +self.dataPath+'/'+self.name+'.unmapRemoved.bam > '+self.dataPath+'/'+self.name+'.qualFiltered.bam  2> '+self.logPath+'/stderr.samtoolsView.qualFilter.'+self.name+'.txt \n'
	output += 'samtools view -b -F 1024 '+self.dataPath+'/'+self.name+'.qualFiltered.bam > '+self.dataPath+'/'+self.name+'.noDuplicates.bam  2> '+self.logPath+'/stderr.samtoolsView.removeDups.'+self.name+'.txt \n'

	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.dataPath+'/'+self.name+'.unmapRemoved.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex1.'+self.name+'.txt \n'
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.dataPath+'/'+self.name+'.qualFiltered.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex2.'+self.name+'.txt \n'
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.dataPath+'/'+self.name+'.noDuplicates.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex3.'+self.name+'.txt \n'

        output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.reCalibrated.bam > '+self.logPath+'/reCalibratedBamFlagstat.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.unmapRemoved.bam > '+self.logPath+'/unmapRemovedBamFlagstat.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.qualFiltered.bam > '+self.logPath+'/qualFilteredBamFlagstat.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.noDuplicates.bam > '+self.logPath+'/noDuplicatesBamFlagstat.'+self.name+'.txt \n'

        #output += 'rm -v '+self.dataPath+'/'+self.name+'.reCalibrated.bam\n'
	#output += 'rm -v '+self.dataPath+'/'+self.name+'.unmapRemoved.bam\n'
	#output += 'rm -v '+self.dataPath+'/'+self.name+'.qualFiltered.bam\n'

        #
        # Final output and write script to file
        #
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'wait\n'
        output += 'echo "$(date) AllDone"\n'
        with open(self.scriptPath+'/reAlignAndReCalibrate.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def haplotypeCalling(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J hapCal.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.haplotypeCalling.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.haplotypeCalling.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'

        output += 'echo "HC" '+'\n'
        
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' '
        output += '-T HaplotypeCaller '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
        output += '-I '+self.dataPath+'/'+self.name+'.noDuplicates.bam '
        output += '--genotyping_mode DISCOVERY '
        output += '-stand_emit_conf 10 '
        output += '-stand_call_conf 30 '
        output += '-L '+AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem.bed '
        output += '--dbsnp $GATKbundle/dbsnp_138.b37.vcf '
        output += '--annotation AlleleBalance --annotation AlleleBalanceBySample --annotation BaseCounts --annotation BaseQualityRankSumTest '
        output += '--annotation ChromosomeCounts --annotation ClippingRankSumTest --annotation Coverage --annotation DepthPerAlleleBySample '
        output += '--annotation DepthPerSampleHC --annotation FisherStrand --annotation GCContent --annotation HaplotypeScore --annotation HardyWeinberg '
        output += '--annotation HomopolymerRun --annotation InbreedingCoeff --annotation LikelihoodRankSumTest --annotation LowMQ '
        output += '--annotation MVLikelihoodRatio --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation MappingQualityZeroBySample '
        output += '--annotation NBaseCount --annotation QualByDepth --annotation RMSMappingQuality --annotation ReadPosRankSumTest --annotation SampleList '
        output += '--annotation SnpEff --annotation SpanningDeletions --annotation StrandBiasBySample --annotation TandemRepeatAnnotator '
        output += '--annotation TransmissionDisequilibriumTest --annotation VariantType '#--annotation StrandOddsRatio 
        output += '--emitRefConfidence GVCF --variant_index_type LINEAR --variant_index_parameter 128000 '
        output += '-o '+self.dataPath+'/'+self.name+'.gvcf '
	output += '1>&2 2> '+self.logPath+'/stderr.haplotypeCallerGatk.'+self.name+'.txt &'+'\n'
        output += 'wait'+'\n'

        #
        # Final output and write script to file
        #
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'wait\n'
        output += 'echo "$(date) AllDone"\n'
        with open(self.scriptPath+'/haplotypeCalling.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def qcSteps(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J qcSteps.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.qcSteps.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.qcSteps.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'

        #
        # GATK callable Loci
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo -e "-> CallableLoci <-"'+'\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T CallableLoci '
	output +='-I '+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output +='-summary '+self.dataPath+'/'+self.name+'.callableLociSummary.txt '
	output +='-o '+self.dataPath+'/'+self.name+'.callableLoci.bed '
	output +='-R '+AnalysisPipe.bowtie2Reference+' '+'\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'
        output += 'echo'+'\n'
        output += 'echo "-----"'+'\n'

        #
        # qacompute
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo -e "-> Pauls qacompute <-"'+'\n'
        output += '/proj/b2010052/scripts/qaCompute -d -q 10 '
	output += '-m '+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output += self.dataPath+'/'+self.name+'.qacompute.out '
	output += '> '+self.logPath+'/'+self.name+'.qacompute.stdout.txt '
	output += '2> '+self.logPath+'/'+self.name+'.qacompute.stderr.txt '+'\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'

        #
        # picard HS metrics
        #
        output += 'java -Xmx3g -jar $picard/CalculateHsMetrics.jar '
	output += 'BAIT_INTERVALS='+AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem '
	output += 'TARGET_INTERVALS='+AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem '
	output += 'INPUT='+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output += 'OUTPUT='+self.dataPath+'/'+self.name+'.hs_metrics.summary.txt '
	output += 'PER_TARGET_COVERAGE='+self.dataPath+'/'+self.name+'.hs_metrics.perTargetCoverage.txt '
	output += 'REFERENCE_SEQUENCE='+AnalysisPipe.bowtie2Reference+'  '
	output += '1>&2 2> '+self.logPath+'/'+self.name+'.stderr.caluclateHsmetrics.txt \n'

        #
        # Final output and write script to file
        #
        output += 'echo'+'\n'
        output += 'wait'+'\n'
        output += 'echo "$(date) AllDone"'+'\n'
        output += 'echo "$(date) AllDone" >&2'+'\n'
        with open(self.scriptPath+'/qcSteps.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def getStats(self):
	
	import re
	
	stats = {}
	stats['rubiconWgaTrimming'] = {}
	stats['malbacWgaTrimming']  = {}
	stats['illuminaAndNexteraTrimming']  = {}
	stats['qualityTrimming']  = {}
	stats['removeEmptyReads']  = {}
	stats['bowtie2'] = {}
	
	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
	    stats['rubiconWgaTrimming'][filePairId] = {'r1':None,'r2':None}
	    stats['malbacWgaTrimming'][filePairId]  = {'r1':None,'r2':None}
	    stats['illuminaAndNexteraTrimming'][filePairId]  = {'r1':None,'r2':None}
	    stats['qualityTrimming'][filePairId]    = {'r1':None,'r2':None}
	    
	    for read in ['r1','r2']:

		#try:
		#    with open(self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.'+read+'.log.txt') as infile:
		#	data = infile.read()
		#	p = re.compile("Running wgaAdapterTrimmer.py\nProcessed a total of\t(?P<totalReads>\d+)\treads. \(.+\)\nProcessed a total of\t(?P<totalBases>\d+)\tbases \(.+\).\ntrimmed a total of\t(?P<trimmedBases>\d+)\tbases in the start of reads \(.+\).\nwgaAdapterTrimmer.py done exiting ...\n?")
		#	m = p.match(data)
		#	if m: stats['rubiconWgaTrimming'][filePairId][read] = m.groupdict()
		#except IOError, e:
		#    assert e.errno == 2;
		#    stats['rubiconWgaTrimming'][filePairId][read] = 'NotFinished'
		stats['rubiconWgaTrimming'][filePairId][read] = extractData(infile=self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.'+read+'.log.txt',pattern="Running wgaAdapterTrimmer.py\nProcessed a total of\t(?P<totalReads>\d+)\treads. \(.+\)\nProcessed a total of\t(?P<totalBases>\d+)\tbases \(.+\).\ntrimmed a total of\t(?P<trimmedBases>\d+)\tbases in the start of reads \(.+\).\nwgaAdapterTrimmer.py done exiting ...\n?")

		try:
		    with open(self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.'+read+'.log.txt') as infile:
			data = infile.read()
			p = re.compile("cutadapt version .+\nCommand line parameters: -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC .+\nMaximum error rate\: .+\%\n\s+No. of adapters\: 2\n\s+Processed reads\:\s+(?P<totalReads>\d+)\n\s+Processed bases\:\s+(?P<totalBases>\d+) bp \(.+ Mbp\)\n\s+Trimmed reads\:\s+(?P<trimmedReads>\d+) \(.+\%\)\n\s+Trimmed bases\:\s+(?P<trimmedBases>\d+) bp \(.+ Mbp\) \(.+\% of total\)\n\s+Too short reads\:\s+.+ \(.+\% of processed reads\)\n\s+Too long reads\:\s+.+ \(.+\% of processed reads\)\n\s+Total time\:\s+.+ s\n\s+Time per read\:\s+.+ ms")
			m = p.match(data)
			if m: stats['malbacWgaTrimming'][filePairId][read] = m.groupdict()
		except IOError, e:
		    assert e.errno == 2;
		    stats['malbacWgaTrimming'][filePairId][read] = 'NotFinished'

		try:
		    with open(self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.'+read+'.log.txt') as infile:
			data = infile.read()
			p = re.compile("cutadapt version .+\nCommand line parameters: -n .+\nMaximum error rate\: .+\%\n\s+No. of adapters\: 4\n\s+Processed reads\:\s+(?P<totalReads>\d+)\n\s+Processed bases\:\s+(?P<totalBases>\d+) bp \(.+ Mbp\)\n\s+Trimmed reads\:\s+(?P<trimmedReads>\d+) \(.+\%\)\n\s+Trimmed bases\:\s+(?P<trimmedBases>\d+) bp \(.+ Mbp\) \(.+\% of total\)\n\s+Too short reads\:\s+.+ \(.+\% of processed reads\)\n\s+Too long reads\:\s+.+ \(.+\% of processed reads\)\n\s+Total time\:\s+.+ s\n\s+Time per read\:\s+.+ ms")
			m = p.match(data)
			if m: stats['illuminaAndNexteraTrimming'][filePairId][read] = m.groupdict()
		except IOError, e:
		    assert e.errno == 2;
		    stats['illuminaAndNexteraTrimming'][filePairId][read] = 'NotFinished'

		try:
		    with open(self.logPath+'/qualityTrimming.'+str(filePairId)+'.'+read+'.log.txt') as infile:
			data = infile.read()
			p = re.compile('(?P<totalBasess>\d+)\tbases\n(?P<trimmedBases>\d+)\ttrimmed')
			m = p.match(data)
			if m: stats['qualityTrimming'][filePairId][read] = m.groupdict()
		except IOError, e:
		    assert e.errno == 2;
		    stats['qualityTrimming'][filePairId][read] = 'NotFinished'

	    try:
		with open(self.logPath+'/qualityTrimming.'+str(filePairId)+'.removeEmptyReads.log.txt') as infile:
		    data = infile.read()
		    p = re.compile("""Running removeEmptyReads.py:\nHeader one is empty exiting.\n(?P<totalReads>\d+) read pairs processed.\n(?P<pairsOut>\d+) read pairs to outfiles .+.\n(?P<singlets>\d+) single reads to outfile .+.\nremoveEmptyReads Exiting.""")
		    m = p.match(data)
		    if m:stats['removeEmptyReads'][filePairId] = m.groupdict()
	    except IOError, e:
		assert e.errno == 2;
		stats['removeEmptyReads'][filePairId] = 'NotFinished'

	    #self.logPath+'/stderr.trimming.'+self.name+'.'+str(filePairId)+'.txt'
	    #self.logPath+'/stdout.trimming.'+self.name+'.'+str(filePairId)+'.txt'

	    #self.fastqcPath+'/\n'

	    #self.logPath+'/stderr.mapping.'+self.name+'.'+str(filePairId)+'.txt'
	    #self.logPath+'/stdout.mapping.'+self.name+'.'+str(filePairId)+'.txt'

	    try:
		with open(self.logPath+'/stderr.bowtie2.'+str(filePairId)+'.txt') as infile:
		    data = infile.read()
		    p = re.compile("""(?P<totalReads>\d+) reads; of these:\n\s+\d+ \(\d+.\d+\%\) were paired; of these:\n\s+\d+ \(\d+.\d+\%\) aligned concordantly 0 times\n\s+\d+ \(\d+.\d+\%\) aligned concordantly exactly 1 time\n\s+\d+ \(\d+.\d+\%\) aligned concordantly >1 times\n    ----\n\s+\d+ pairs aligned concordantly 0 times; of these:\n\s+(?P<discordantPairs>\d+) \(\d+.\d+\%\) aligned discordantly 1 time\n    ----\n\s+\d+ pairs aligned 0 times concordantly or discordantly; of these:\n\s+\d+ mates make up the pairs; of these:\n\s+\d+ \(\d+.\d+\%\) aligned 0 times\n\s+(?P<singleSingleMap>\d+) \(\d+.\d+\%\) aligned exactly 1 time\n\s+(?P<singleMultiMap>\d+) \(\d+.\d+\%\) aligned >1 times\n(?P<overallAlignmentRate>\d+.\d+)\% overall alignment rate""")
		    m = p.match(data)
		    if m:stats['bowtie2'][filePairId] = m.groupdict()
		    else:stats['bowtie2'][filePairId] = 'NoMatch'
	    except IOError, e:
		assert e.errno == 2;
		stats['bowtie2'][filePairId] = 'NotFinished'

	#self.logPath+'/stderr.merge.'+self.name+'.txt'
	#self.logPath+'/stdout.merge.'+self.name+'.txt'
	try:
	    with open(self.logPath+'/stderr.merging.'+self.name+'.txt') as infile:
		data = infile.read()
		p = re.compile("Finished reading inputs.+\n.+picard.sam.MergeSamFiles done. Elapsed time")
		m = p.search(data)
		if m:stats['merging'] = 'Finished OK'
		else: stats['merging'] = 'NoMatch'
	except IOError, e:
	    assert e.errno == 2;
	    stats['merging'] = 'NotFinished'

	#self.logPath+'/stderr.filterAndFix.'+self.name+'.txt'
	#self.logPath+'/stdout.filterAndFix.'+self.name+'.txt'
	#self.logPath+'/stderr.sam2bam.'+self.name+'.txt'
	#self.logPath+'/stderr.sortBam.'+self.name+'.txt'
	try:
	    with open(self.logPath+'/markeDuplicatesMetrix.'+self.name+'.txt') as infile:
		data = infile.read()
		p = re.compile("""LIBRARY\tUNPAIRED_READS_EXAMINED\tREAD_PAIRS_EXAMINED\tUNMAPPED_READS\tUNPAIRED_READ_DUPLICATES\tREAD_PAIR_DUPLICATES\tREAD_PAIR_OPTICAL_DUPLICATES\tPERCENT_DUPLICATION\tESTIMATED_LIBRARY_SIZE\n(?P<Library>.+)\s+(?P<unPairedReads>\d+)\s+(?P<totalReads>\d+)\s+(?P<unMapped>\d+)\s+(?P<unPairedDups>\d+)\s+(?P<pairDups>\d+)\s+(?P<opticalDups>\d+)\s+(?P<percentageDuplication>\d+\,\d+)\s+(?P<estLibSize>\d+)""")
		m = p.search(data)
		if m:stats['markeDuplicatesMetrix'] = m.groupdict()
	except IOError, e:
	    assert e.errno == 2;
	    stats['markeDuplicatesMetrix'] = 'NotFinished'
	#self.logPath+'/stderr.markeDuplicates.'+self.name+'.txt'
	#self.logPath+'/stderr.addAndReplaceReadGroups.'+self.name+'.txt'
	try:
	    with open(self.logPath+'/fixedBamFlagstat.'+self.name+'.txt') as infile:
		data = infile.read()
		p = re.compile("""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
		m = p.match(data)
		if m:stats['fixedBamFlagstat'] = m.groupdict()
	except IOError, e:
	    assert e.errno == 2;
	    stats['fixedBamFlagstat'] = 'NotFinished'

	#self.logPath+'/stderr.realTC.'+self.name+'.txt'
	#self.logPath+'/stdout.realTC.'+self.name+'.txt'
	#self.logPath+'/stderr.RealignerTargetCreator.'+self.name+'.txt;'

	#self.logPath+'/stderr.reAlign.'+self.name+'.txt'
	#self.logPath+'/stdout.reAlign.'+self.name+'.txt'
	#self.logPath+'/stderr.indelRealigner.'+self.name+'.txt;'+''
	#self.logPath+'/stderr.baseRecalibrator.'+self.name+'.txt;'+''
	#self.logPath+'/stderr.printreads.txt
	#self.logPath+'/stderr.samtoolsView.removeUnmap.'+self.name+'.txt'
	#self.logPath+'/stderr.samtoolsView.qualFilter.'+self.name+'.txt'
	#self.logPath+'/stderr.samtoolsView.removeDups.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex1.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex2.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex3.'+self.name+'.txt'
	
	try:
	    with open(self.logPath+'/reCalibratedBamFlagstat.'+self.name+'.txt') as infile:
		data = infile.read()
		p = re.compile("""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
		m = p.match(data)
		if m:stats['reCalibratedBamFlagstat'] = m.groupdict()
	except IOError, e:
	    assert e.errno == 2;
	    stats['reCalibratedBamFlagstat'] = 'NotFinished'
	
	try:
	    with open(self.logPath+'/unmapRemovedBamFlagstat.'+self.name+'.txt') as infile:
		data = infile.read()
		p = re.compile("""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
		m = p.match(data)
		if m:stats['unmapRemovedBamFlagstat'] = m.groupdict()
	except IOError, e:
	    assert e.errno == 2;
	    stats['unmapRemovedBamFlagstat'] = 'NotFinished'
	
	try:
	    with open(self.logPath+'/qualFilteredBamFlagstat.'+self.name+'.txt') as infile:
		data = infile.read()
		p = re.compile("""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
		m = p.match(data)
		if m:stats['qualFilteredBamFlagstat'] = m.groupdict()
	except IOError, e:
	    assert e.errno == 2;
	    stats['qualFilteredBamFlagstat'] = 'NotFinished'
	
	try:
	    with open(self.logPath+'/noDuplicatesBamFlagstat.'+self.name+'.txt') as infile:
		data = infile.read()
		p = re.compile("""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
		m = p.match(data)
		if m:stats['noDuplicatesBamFlagstat'] = m.groupdict()
	except IOError, e:
	    assert e.errno == 2;
	    stats['noDuplicatesBamFlagstat'] = 'NotFinished'

	#self.logPath+'/stderr.haplotypeCallerGatk.'+self.name+'.txt'
	#self.logPath+'/stderr.haplotypeCalling.'+self.name+'.txt'
	#self.logPath+'/stdout.haplotypeCalling.'+self.name+'.txt'
	#self.dataPath+'/'+self.name+'.gvcf '

	#self.logPath+'/stderr.qcSteps.'+self.name+'.txt'
	#self.logPath+'/stdout.qcSteps.'+self.name+'.txt'
	self.logPath+'/'+self.name+'.qacompute.stdout.txt'
	self.logPath+'/'+self.name+'.qacompute.stderr.txt'
	self.dataPath+'/'+self.name+'.qacompute.out '
	#self.logPath+'/'+self.name+'.stderr.caluclateHsmetrics.txt'
	self.dataPath+'/'+self.name+'.hs_metrics.summary.txt '
	#self.dataPath+'/'+self.name+'.hs_metrics.perTargetCoverage.txt '

	#self.dataPath+'/'+str(filePairId)+'.sam'
	#self.dataPath+'/'+self.name+'.merged.sam'
	#self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
	#self.dataPath+'/'+self.name+'.BQSR.grp'+' '
	#self.dataPath+'/'+self.name+'.reCalibrated.bam\n'
	#self.dataPath+'/'+self.name+'.unmapRemoved.bam\n'
	#self.dataPath+'/'+self.name+'.qualFiltered.bam\n'
	#self.dataPath+'/'+self.name+'.noDuplicates.bam '
	#self.dataPath+'/'+self.name+'.callableLociSummary.txt '
	#self.dataPath+'/'+self.name+'.callableLoci.bed '
	#

	#try:
	#    with open() as infile:
	#	data = infile.read()
	#	p = re.compile()
	#	m = p.match(data)
	#	if m:stats[''] = m.groupdict()
	#except IOError, e:
	#    assert e.errno == 2;
	#    stats[''] = 'NotFinished'

	for program in ['illuminaAndNexteraTrimming','malbacWgaTrimming','qualityTrimming','rubiconWgaTrimming']:
	    for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
		for read in ['r1','r2']: pass
	for program in ['removeEmptyReads','bowtie2']:
	    for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
		pass#stats['bowtie2'][filePairId]
	    
    
	print self.name
	for key,value in stats.iteritems():
	    print '    ',key
	    try:
		for key2,value2 in value.iteritems():
		    assert type(value2) == dict
		    print '        ',key2,value2
	    except:print '        ',value

if __name__ == "__main__": main()