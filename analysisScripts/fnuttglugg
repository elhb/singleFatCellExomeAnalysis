#!/usr/bin/env python

helpMessage = """
############ fnuttglugg ################
a pipe for DNA sequence analysis

use:
fnuttglugg addSample <sampleName>
fnuttglugg addFastq <sampleName> <r1> <r2>
fnuttglugg createScripts <uppmaxAccount> <emailAdress>
fnuttglugg submitScripts
...more to come ...
fnuttglugg report
"""

def main():
    
    import sys
    
    for term in ['h','help','-h','--help']:
	if term in sys.argv:
	    print helpMessage
	    sys.exit()

    app = AnalysisPipe()
    app.run()

def thousandString(string):
    outstring = ''
    for i in range(len(string)):
	outstring += string[-(i+1)]
	if (i+1)%3 == 0: outstring += ' '
    return outstring[::-1]

def percentage(count,total):
    if 'NA' in [total,count]: return 'NA'
    if float(total) <=0.0: return 'NA'
    #return round(float(count) / float(total),4)
    return round(100* float(count) / float(total),2)

def extractData(infile=None,pattern=None,checkType='data'):
    import re
    if not pattern: return 'NoPatternGiven'
    if not infile: return 'NoInfileGiven'
    try:
	with open(infile) as infile:
	    data = infile.read()
	    p = re.compile(pattern)
	    m = p.search(data)
	    if m:
		if   checkType=='data':   return m.groupdict()
		elif checkType=='program':return 'FinishedOK'
	    else: return 'NoMatchFound'
    except IOError, e:
	assert e.errno == 2;
	return 'NoFileFound'

def bufcount(filename):
	""" returns the number of lines in a file
	"""
	import gzip
	if filename.split('.')[-1] in ['gz','gzip']: f = gzip.open(filename)
	else: f = open(filename)
	lines = 0
	buf_size = 1024 * 1024
	read_f = f.read # loop optimization
	
	buf = read_f(buf_size)
	while buf:
		lines += buf.count('\n')
		buf = read_f(buf_size)
		f.close
	return lines

def submitSbatch(filename,dependency=None):
    import subprocess
    import sys
    if dependency: command = ['sbatch','--dependency=afterok:'+':'.join(dependency),filename]
    else:          command = ['sbatch',filename]
    sbatch = subprocess.Popen( command, stdout=subprocess.PIPE, stderr=subprocess.PIPE )
    sbatch_out, errdata = sbatch.communicate()
    if sbatch.returncode != 0:
	print 'sbatch view Error code', sbatch.returncode, errdata
	print sbatch_out
	print filename
	sys.exit()
    jobid = sbatch_out.split('\n')[0].split(' ')[3]
    return jobid

def graphForeachSample(Sample):
    pass

class AnalysisPipe(object):

    def __init__(self):
	
	import sys
	import os
	import time

	try:
	    path = sys.argv[1]
	    path = os.path.abspath(path)
	    if path[-1] == '/': path = path[:-1]
	    AnalysisPipe.path = path
	    AnalysisPipe.programPath = os.path.abspath(sys.argv[0])
	    AnalysisPipe.scriptPath    = '/'+'/'.join(AnalysisPipe.programPath.split('/')[:-1])
	    AnalysisPipe.referencePath = '/'+'/'.join(AnalysisPipe.programPath.split('/')[:-2])+'/references'
	except IndexError:
	    sys.stderr.write('#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a path on your commandline (currently: "'+' '.join(sys.argv)+'")\n')
	    sys.exit(1)

	AnalysisPipe.bowtie2Reference = '~/singleFatCellExomeAnalysis/references/GATKbundle/human_g1k_v37.fasta'
	AnalysisPipe.picardLocation = '~/bin/picard-tools-1.114'
	AnalysisPipe.gatkLocation = '~/singleFatCellExomeAnalysis/bin/GenomeAnalysisTK-3.1-1/GenomeAnalysisTK.jar'
	AnalysisPipe.gatkBundleLocation = '~/singleFatCellExomeAnalysis/references/GATKbundle/'

	self.openLogfileConnection()
	
	return None

    def getDataBase(self):

	import os
	import time

	AnalysisPipe.database = Database(self.path+'/data.db')
	if not os.path.exists(AnalysisPipe.database.path): AnalysisPipe.database.create()

    def run(self, ):
	
	import time
	import sys
	
	try: self.action = sys.argv[2]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply an action on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	
	self.getDataBase()
	
	import os
	
	AnalysisPipe.database.addToRunsTable(time.time(),self.action,' '.join(sys.argv),False,os.getpid())
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Running '+' '.join(sys.argv)+'\n')

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Action is '+sys.argv[2]+'\n')
	if self.action == 'addSample':self.addSample()
	elif self.action == 'addFastq':self.addFastq()
	elif self.action == 'report':self.report()
	elif self.action == 'createScripts':self.createScripts()
	elif self.action == 'submitScripts':self.submitScripts()
	elif self.action == 'makeGraphics':self.makeGraphics()
	else:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a Valid action on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

    def addFastq(self):

	import sys
	import time
	
	try: sample = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a sample name on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	try:
	    f1 = sys.argv[4]
	    f2 = sys.argv[5]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a pair of fastq files on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	AnalysisPipe.database.addFastqs(sample,f1,f2)

    def addSample(self, ):
	import sys
	import time
	AnalysisPipe.database.addSample(sys.argv[3])

    def openLogfileConnection(self,):
        """ open a connection to the logfile, creates a logfile if none is present """
        
        #
        # Imports
        #
        import sys
        import time
        import os
        
        #
        # for logmessages
        #        
        tmpLogMessages = []
        
        #
        # check if logfile present open connection or create
        #
        AnalysisPipe.logfile = self.path + '/logfile.txt'
        if os.path.isfile(AnalysisPipe.logfile):
            if 'initiateAnalysis' in sys.argv:
                sys.stderr.write('#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# The logfile already exists please use another path to initiate the analysis.\n')
                sys.exit(1)
            else:
                AnalysisPipe.logfile = open(AnalysisPipe.logfile,'a',1)
                AnalysisPipe.logfile.write('----------------\n#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Connection to logfile '+AnalysisPipe.logfile.name+' opened.\n')
                return 0
        else:
            tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Creating the logfile "'+AnalysisPipe.logfile+'".'
            tmpLogMessages.append(tmpLogMessage)
            tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Looking for folder '+self.path+'...'
            tmpLogMessages.append(tmpLogMessage)
	    if not os.path.isdir(self.path):
		tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Path '+self.path+' not found creating...'
		tmpLogMessages.append(tmpLogMessage)
		os.mkdir(self.path)
            AnalysisPipe.logfile = open(AnalysisPipe.logfile,'w',1)
        
	AnalysisPipe.logfile.write('\n'.join(tmpLogMessages)+'\n')
	
        return tmpLogMessages

    def report(self, ):

	import time
	import operator
	import os
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Creating a report located at '+self.path+'/report.htm'+' \n')
    
	samples = AnalysisPipe.database.getSamples()
	samplesbyName = {}
	samplesbyId = {}
	for sample in samples:
	    samplesbyId[sample.id]=sample
	    samplesbyName[sample.name]=sample
	    sample.getStats()
	
	reportFile = open(self.path+'/report.htm','w')
	reportFile.write('<html>')
	reportFile.write("""<head><style>
	    body {
		background-color:white
		font-family : Arial,"Myriad Web",Verdana,Helvetica,sans-serif;
	    }
	    h1   {color:black}
	    p    {color:green}
	    
	    table, th, td {
		border: 1px solid black;
		border-collapse: collapse;
		
		font-size : 12;
	    }
	    th {
		text-align: center;
		background-color:darkgray;
		color:white;
		border: 1px solid black;
	    }
	    table {border-spacing: 1px;}
	    th,td {padding: 5px;}
	    td {text-align: center;}
	</style></head>""")

	reportFile.write('<body>')
	reportFile.write('<h1>Analysis Report '+self.path+'</h1>\n')

	reportFile.write('<h2>List of samples:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>Sample Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>Read Pair Count</th>')
	reportFile.write('<th>Pairs After Trimming</th>')
	reportFile.write('<th>Overall Mapping Rate<br>(bowtie)</th>')
	reportFile.write('<th>Overall Mapping Rate<br>(flagstat)</th>')
	reportFile.write('<th>Duplication Rate<br>(preFilter)</th>')
	reportFile.write('<th>Unmapped or<br>Low Mapping Quality</th>')
	reportFile.write('<th>Left After<br>All Filtering</th>')
	reportFile.write('</tr>')
	for sampleId, sample in sorted(samplesbyId.iteritems(), key=operator.itemgetter(0)):
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(sampleId)+'</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    reportFile.write('<td>'+thousandString(str(sample.readCount))+'</td>')
	    try:            reportFile.write('<td>'+thousandString(str(int(sample.stats['removeEmptyReads']['sum']['pairsOut'])))+' ('+str(percentage(sample.stats['removeEmptyReads']['sum']['pairsOut'],sample.readCount))+'%)</td>')
	    except KeyError:reportFile.write('<td>NA (NA%)</td>')
	    try:
		tmpCount  = sum([sample.stats['bowtie2']['sum'][tmp] for tmp in ['singleSingleMap','singleMultiMap']])
		tmpCount += sum([sample.stats['bowtie2']['sum'][tmp] for tmp in ['discordantPairs','properPairsMultiMap','properPairs']])*2
		reportFile.write('<td>'+str(percentage(tmpCount,sample.stats['bowtie2']['sum']['totalReads']*2))+'%</td>')
		assert sample.stats['bowtie2']['sum']['totalReads'] == sample.stats['removeEmptyReads']['sum']['pairsOut'], 'Error: pair counts do not match'
	    except KeyError:reportFile.write('<td>NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['fixedBamFlagstat']['mapped']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['fixedBamFlagstat']['duplicates']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['qualFilteredBamFlagstat']['totalReads']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['noDuplicatesBamFlagstat']['mapped']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')

	reportFile.write('<h2>List of fastqs:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>File Id</th>')
	reportFile.write('<th># Read Pairs</th> ')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>File Name (r1)</th>')
	reportFile.write('<th>FastQC r1</th>')
	reportFile.write('<th>FastQC r2</th>')
	reportFile.write('</tr>')
	for filePairId,readCount,fastq1,fastq2,sampleId in sorted(AnalysisPipe.database.getFastqs(), key=operator.itemgetter(0)):
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(filePairId)+'</td>')
	    reportFile.write('<td>'+thousandString(str(readCount))+'</td>')
	    reportFile.write('<td>'+samplesbyId[int(sampleId)].name+'</td>')
	    reportFile.write('<td>'+fastq1+'</td>')
	    if os.path.exists(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r1.allTrimmed_fastqc.html'): reportFile.write('<td><a href="'+os.path.relpath(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r1.allTrimmed_fastqc.html','/'.join(reportFile.name.split('/')[:-1]))+'">here</a></td>')
	    else:reportFile.write('<td>NA</td>')
	    if os.path.exists(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r2.allTrimmed_fastqc.html'): reportFile.write('<td><a href="'+os.path.relpath(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r2.allTrimmed_fastqc.html','/'.join(reportFile.name.split('/')[:-1]))+'">here</a></td>')
	    else:reportFile.write('<td>NA</td>')
	    reportFile.write('</tr>')
	reportFile.write('</table>')

	reportFile.write('<h2>Trimming Details:</h2>')
	reportFile.write('<h3>Samples:</h3>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>Sample Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>Original Bases</th>')
	reportFile.write('<th>% rubicon adapter</th>')
	reportFile.write('<th>% malbac adapter</th>')
	reportFile.write('<th>% illumina Adapters</th>')
	reportFile.write('<th>% quality trimmed</th>')
	reportFile.write('</tr>')
	for sampleId, sample in sorted(samplesbyId.iteritems(), key=operator.itemgetter(0)):
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(sampleId)+'</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    try: reportFile.write('<td>'+thousandString(str(int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'</td>')
	    except (KeyError, TypeError) as e: reportFile.write('<td>NA</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['rubiconWgaTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['malbacWgaTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['illuminaAndNexteraTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['qualityTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')

	reportFile.write('<h3>Files (r1 then r2):</h3>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>File Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th colspan="2">Original Bases</th>')
	reportFile.write('<th colspan="2">% rubicon adapter</th>')
	reportFile.write('<th colspan="2">% malbac adapter</th>')
	reportFile.write('<th colspan="2">% illumina Adapters</th>')
	reportFile.write('<th colspan="2">% quality trimmed</th>')
	reportFile.write('</tr>')
	for filePairId,readCount,fastq1,fastq2,sampleId in sorted(AnalysisPipe.database.getFastqs(), key=operator.itemgetter(0)):
	    sample = samplesbyId[int(sampleId)]
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(filePairId)+'</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    for read in ['r1','r2']:
		try: reportFile.write('<td>'+thousandString(str(int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'</td>')
		except (KeyError, TypeError) as e:reportFile.write('<td>NA</td>')
	    for read in ['r1','r2']:
		try: reportFile.write('<td>'+str(percentage(int(sample.stats['rubiconWgaTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    for read in ['r1','r2']:
		try: reportFile.write('<td>'+str(percentage(int(sample.stats['malbacWgaTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    for read in ['r1','r2']:
		try: reportFile.write('<td>'+str(percentage(int(sample.stats['illuminaAndNexteraTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    for read in ['r1','r2']:
		try: reportFile.write('<td>'+str(percentage(int(sample.stats['qualityTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		except (KeyError, TypeError) as e:reportFile.write('<td>NA%</td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')

	reportFile.write('</body></html>\n')
	
	return 0

    def createScripts(self, ):

	import time
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Creating sbatch scripts:\n')

	for sample in AnalysisPipe.database.getSamples():
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# sample: '+sample.name+' ... \n')
	    try: sample.getFastqs().next()
	    except StopIteration:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		continue
	    sample.trimFastqs()
	    sample.mapFastqs()
	    sample.mergeMapped()
	    sample.filterAndFixMerged()
	    sample.realignerTargetCreator()
	    sample.reAlignAndReCalibrate()
	    sample.haplotypeCalling()
	    sample.qcSteps()

    def submitScripts(self,sampleNameOrId=None):

	import time

	allSampleDependency = []
	for sample in AnalysisPipe.database.getSamples():

	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitting sbatches for sample: '+sample.name+' ... \n')
	    try: sample.getFastqs().next()
	    except StopIteration:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		continue

	    dependency = []
	    for filePairId,readCount,fastq1,fastq2,sampleId in sample.getFastqs():

		fileName = sample.scriptPath+'/trimming.'+sample.name+'.'+str(filePairId)+'.sh'
		jobid = submitSbatch(fileName)
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted trimming of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

		fileName = sample.scriptPath+'/mapping.'+sample.name+'.'+str(filePairId)+'.sh'
		jobid = submitSbatch(fileName,dependency=[jobid])
		dependency.append(jobid)
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted mapping of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/mergeMapped.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=dependency)
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted mering of mapped data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/FilterAndFix.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted fixing the of merged data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/realignerTargetCreator.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted realignerTargetCreator of data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/reAlignAndReCalibrate.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted reAlignAndReCalibrate of data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/haplotypeCalling.'+sample.name+'.sh'
	    hapJobid = submitSbatch(fileName,dependency=[jobid])
	    allSampleDependency.append(hapJobid)
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted haplotypecalling for '+sample.name+' with job id '+str(jobid)+' \n')
	    
	    fileName = sample.scriptPath+'/qcSteps.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted qc steps for '+sample.name+' with job id '+str(jobid)+' \n')

    def makeISizePlot(self):

	import pysam
	import numpy as np
	import matplotlib.pyplot as plt
	import operator
	import sys
	import os

	colors = ['b','b:','r','r:','g','g:','c','c:','m','m:','y','y:','k','k:']+[i+'--' for i in 'rgbcmyk']+[i+'-.' for i in 'rgbcmyk'] # ['-' | '--' | '-.' | ':' | 'None' | ' ' | '']
	isizes = {}

	for sample in AnalysisPipe.database.getSamples():
	    bamfile = pysam.Samfile(sample.dataPath+'/'+sample.name+'.noDuplicates.bam', "rb")
	    for read in bamfile.fetch():
		if read.tlen >= 1:
		    try:isizes[sample.name].append(int(read.tlen))
		    except KeyError:isizes[sample.name]= [read.tlen]

	fig, axes = plt.subplots(1, sharex=True)
	plots = []
	incrementer=0

	for sample in AnalysisPipe.database.getSamples():
	    #n, bins, patches = axes.hist(isizes[sample.name], 50, normed=1, histtype='step',label=sample.name)
	    counter = {}
	    total = 0

	    for isize in isizes[sample.name]:
		try:counter[isize] += 1
		except KeyError: counter[isize] = 1
		total+=1

	    y = [percentage(value,total) for key, value in sorted(counter.iteritems(), key=operator.itemgetter(0))]
	    #y = [value for key, value in sorted(counter.iteritems(), key=operator.itemgetter(0))]
	    x = sorted(counter.keys())
	    plots.append(axes.plot(x, y,colors[incrementer],label=sample.name))
	    incrementer+=1
	    axes.set_xlim([0,1000])

	handles, labels = axes.get_legend_handles_labels()
	hl = sorted(zip(handles, labels), key=operator.itemgetter(1))
	handles2, labels2 = zip(*hl)
	axes.legend(handles2, labels2,loc=0,fontsize='small')
	axes.set_xlabel('InsertSize (final filtered data)')
	axes.set_ylabel('Frequency')
	plt.savefig(AnalysisPipe.path+'/graphics/insertSizes.pdf',dpi=50,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/insertSizes.png',dpi=50,bbox_inches='tight')

    def makeGraphics(self,):

	import pysam
	import numpy as np
	import matplotlib.pyplot as plt
	import operator
	import sys
	import os
	import time
	import multiprocessing

	try: os.mkdir(AnalysisPipe.path+'/graphics')
	except OSError: pass

	perBaseCoverages = {}
	depthPerPosition = {}
	referenceBaseCount = {}
	sampleCount = sum([1 for sample in AnalysisPipe.database.getSamples()])

	poolOfProcesses = multiprocessing.Pool(min([multiprocessing.cpu_count(),sampleCount]),maxtasksperchild=1)
	parallelResults = poolOfProcesses.imap_unordered(graphForeachSample,AnalysisPipe.database.getSamples(),chunksize=1)

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Loading indata ...\n')
	for sample in AnalysisPipe.database.getSamples():
	    sys.stderr.write('sample '+sample.name+' ...\n')
	    sampleCount += 1

	    perBaseCoverages[sample.name] = {}
	    depthPerPosition[sample.name] = {}
	    referenceBaseCount[sample.name] = 0

	    bedtoolsFile = open(sample.dataPath+'/'+sample.name+'.bedtools.coverage.bed')
	    for line in bedtoolsFile:

		referenceBaseCount[sample.name] += 1
		line = line.rstrip().split('\t')

		try:            perBaseCoverages[sample.name][int(line[-1])] += 1
		except KeyError:perBaseCoverages[sample.name][int(line[-1])] = 1

		try:
		    #depthPerPosition[sample.name][line[0]][int(line[1])+int(line[5])-1] = int(line[-1])
		    depthPerPosition[sample.name][line[0]]['refPos'].append(int(line[1])+int(line[5])-1)
		    depthPerPosition[sample.name][line[0]]['rd'].append(int(line[-1])) 
		except KeyError:
		    #depthPerPosition[sample.name][line[0]] = {int(line[1])+int(line[5])-1:int(line[-1])}
		    depthPerPosition[sample.name][line[0]] = {'refPos':[int(line[1])+int(line[5])-1],'rd':[int(line[-1])]}
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# All data loaded.\n')

	for value in referenceBaseCount.values(): assert value == referenceBaseCount.values()[0]
	colors = ['b','b:','r','r:','g','g:','c','c:','m','m:','y','y:','k','k:']+[i+'--' for i in 'rgbcmyk']+[i+'-.' for i in 'rgbcmyk'] # ['-' | '--' | '-.' | ':' | 'None' | ' ' | '']

	# lorenz curve and gini index
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Generating lorentzcurve graph...\n')
	fig, axes = plt.subplots(1, sharex=True)
	plots = []
	a = [i/10.0 for i in range(0,11,1)]
	plots.append(axes.plot(a, a,'b-o',label='EvenDist, 0.00%'))
	incrementer = 0
	for sample in AnalysisPipe.database.getSamples():
	    pre_x = [count for rd, count in sorted(perBaseCoverages[sample.name].iteritems(), key=operator.itemgetter(0))]
	    pre_y = [rd*count for rd, count in sorted(perBaseCoverages[sample.name].iteritems(), key=operator.itemgetter(0))]
	    pre_x2 = [sum(pre_x[:i]) for i in range(len(pre_x))]
	    pre_y2 = [sum(pre_y[:i]) for i in range(len(pre_y))]
	    sequencedBases = sum(pre_y)
	    x = [0]+[float(value)/referenceBaseCount[sample.name] for value in pre_x2]
	    y = [0]+[float(value)/sequencedBases  for value in pre_y2]
	    B = sum( [((y[i]+y[i+1])/2.0)*(x[i+1]-x[i]) for i in range(len(x)-1)] )
	    A = 0.5 - B
	    G = A / (A + B)
	    assert A + B == 0.5
	    assert G == 2*A
	    assert G == 1 - 2*B
	    giniApprox = round(100*G,2)
	    plots.append(axes.plot(x, y,colors[incrementer],label=sample.name+', '+str(giniApprox)+'%'))
	    incrementer+=1
	handles, labels = axes.get_legend_handles_labels()
	hl = sorted(zip(handles, labels), key=operator.itemgetter(1))
	handles2, labels2 = zip(*hl)
	axes.legend(handles2, labels2,loc=0,fontsize='small')
	axes.set_xlabel('% of Targeted Bases')
	axes.set_ylabel('% of Sequenced Bases')
	plt.savefig(AnalysisPipe.path+'/graphics/lorentzCurve.pdf',dpi=50,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/lorentzCurve.png',dpi=50,bbox_inches='tight')

	# make coverage at rd plot
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Generating coverage at rd graph...\n')
	fig, axes = plt.subplots(1, sharex=True)
	plots = []
	incrementer=0
	for sample in AnalysisPipe.database.getSamples():
	    sampleMaxCoverage = max( perBaseCoverages[sample.name].keys() )
	    sampleMinCoverage = min( perBaseCoverages[sample.name].keys() )
	    sampleAverageCoverage = sum( [rd*count for rd,count in perBaseCoverages[sample.name].iteritems()] ) / sum( perBaseCoverages[sample.name].values() )
	    print sample.name,sampleMinCoverage,sampleMaxCoverage,sampleAverageCoverage
	    x = sorted(perBaseCoverages[sample.name].keys())
	    y = [count for rd,count in sorted(perBaseCoverages[sample.name].iteritems(),key=operator.itemgetter(0))]
	    yCum = [percentage(sum(y[i:]),referenceBaseCount[sample.name]) for i in range(len(y))]
	    plots.append(axes.plot(x, yCum, colors[incrementer],label=sample.name))
	    axes.set_xlim([0,50])
	    incrementer+=1
	handles, labels = axes.get_legend_handles_labels()
	hl = sorted(zip(handles, labels), key=operator.itemgetter(1))
	handles2, labels2 = zip(*hl)
	axes.legend(handles2, labels2,loc=0,fontsize='small')
	axes.set_xlabel('Coverage')
	axes.set_ylabel('% of targeted bases')
	plt.savefig(AnalysisPipe.path+'/graphics/exomecoverage.pdf',dpi=50,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/exomecoverage.png',dpi=50,bbox_inches='tight')

	# make rd over reference graph
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Generating rd over reference graph...\n')
	outputEvery = 100
	slidingWindowSize = outputEvery*10
	for chrom in depthPerPosition[sample.name].keys():

	    fig, axes = plt.subplots(sampleCount, sharex=True)
	    fig.set_size_inches(30,sampleCount*3)
	    axes[0].set_title("Chromosome "+chrom+" coverage over concatenated targets")

	    tmpCounter = 0
	    for sample in AnalysisPipe.database.getSamples():

		referencePositions = depthPerPosition[sample.name][chrom]['refPos']
		readDepths = depthPerPosition[sample.name][chrom]['rd']
		assert len(readDepths) == len(referencePositions)
		windowAverage = [sum(readDepths[i:i+slidingWindowSize])/float(slidingWindowSize) for i in range(len(readDepths))]
		concatednatedRefPos = range(len(referencePositions))

		axes[tmpCounter].plot(concatednatedRefPos, windowAverage, lw=1,color="green")
		axes[tmpCounter].fill_between(concatednatedRefPos,0,windowAverage, color="green",alpha=0.5)
		axes[tmpCounter].set_xlabel(sample.name)
		tmpCounter+=1

	    for i in range(sampleCount):axes[i].set_ylim(0,30);axes[i].set_xlim(min(concatednatedRefPos),max(concatednatedRefPos))
	    plt.savefig(AnalysisPipe.path+'/graphics/readDepth.'+chrom+'.pdf',dpi=50,bbox_inches="tight")
	    plt.savefig(AnalysisPipe.path+'/graphics/readDepth.'+chrom+'.png',dpi=50,bbox_inches="tight")

	#	for i in range(len(infiles)):
	#	    infile = open(infiles[i])
	#	    for line in infile:
	#	        line=line.rstrip().split("\t");
	#	        val[i].append(int(line[2]))
	#	        if i ==0:
	#	           #chrom.append(int(line[0]));
	#	           pos.append(int(line[1]));
	#
	#	# skip real genomic coordinates
	#	pos = range(len(pos))
	#
	#	tmp = {i:[] for i in range(len(infiles))}
	#	tmp["pos"] = []
	#	outputEvery = 100
	#	slidingWindow = outputEvery*10
	#	for i in range(len(pos)):
	#	    if i%outputEvery==0:
	#	        tmp["pos"].append(pos[i])
	#	        try: for i2 in range(len(infiles)): tmp[i2].append(sum(val[i2][i:i+slidingWindow])/float(slidingWindow))
	#	        except IndexError: for i2 in range(len(infiles)): tmp[i2].append(0)
	#
	#	for i in val: val[i] = tmp[i]
	#
	#	pos = tmp["pos"]

	# make isize plot
	self.makeISizePlot()

	return 0

class Database(object):
    
    def __init__(self, dbPath):
	self.path = dbPath

    def getConnection(self,):
	#
	# Import useful stuff
	#
	import sqlite3
	import sys

	#
	# Create database and set
	#
	try: self.conn = sqlite3.connect(self.path)
	except sqlite3.OperationalError:
	    print 'ERROR: Trouble with the database, plase check your commandline.'
	    sys.exit()
	self.c = self.conn.cursor()

    def commitAndClose(self,):
	#
	# commit changes and close connection
	#
	self.conn.commit()
	self.conn.close()

    def create(self,):
	""" creates the database holding all information used in the analysis """
	
	self.getConnection()
	
	#
	# Create tables
	#
	self.c.execute('''CREATE TABLE runs (startTime,command,commandLine,finishedSuccessfully,masterPid)''')
	self.c.execute('''CREATE TABLE fastqs (filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId,PRIMARY KEY (filePairId))''');
	self.c.execute('''CREATE TABLE settings (variableName,defaultValue,value,setTime,PRIMARY KEY (variableName))''')
	self.c.execute('''CREATE TABLE results (resultName,defaultValue,value,setTime,PRIMARY KEY (resultName))''')
	self.c.execute('''CREATE TABLE samples (sampleId,sampleName,PRIMARY KEY (sampleId))''')
	
	self.commitAndClose()

    def addToRunsTable(self, startTime, command, commandLine, finishedSuccessfully, masterPid):
	
	self.getConnection()
	
	#
	# check if pid already in database
	#
	t = (masterPid,)
	data = self.c.execute('SELECT masterPid, startTime FROM runs WHERE masterPid=?',t).fetchall()        
	if data:
	    for tmp1,tmp2 in data:

	#
	# if pid and startTime matches update the "finishedSuccessfully" entry
	#
		if tmp1 == masterPid and tmp2 == startTime:
		    values = (startTime, command, commandLine, finishedSuccessfully, masterPid)
		    self.c.execute('UPDATE runs SET finishedSuccessfully=? WHERE masterPid=? AND startTime=?', (finishedSuccessfully,masterPid,startTime))
	
	#
	# if not in the database add a new row
	#
	else:
	    values = (startTime, command, commandLine, finishedSuccessfully, masterPid)
	    self.c.execute('INSERT INTO runs VALUES (?,?,?,?,?)', values)
	
	self.commitAndClose()
	
	return 0

    def addSample(self, newSampleName):
	
	#
	# Imports
	#
	import sys
	import time
	
	#
	# open connection to database
	#
	self.getConnection()
	
	sampleNames = []
	sampleIds = []
	
	#
	# check if any of the fastqs already in database
	#
	data = self.c.execute('SELECT sampleId,sampleName FROM samples').fetchall()
	if data:
	    for (sampleId,sampleName) in data:
		#sampleName = sampleName[0]
		sampleNames.append(sampleName)
		sampleIds.append(sampleId)
	    if newSampleName in sampleNames:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# SampleName must be uniq, there is already a sample with name '+newSampleName+' , exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	
	if sampleIds:  sampleId = max(sampleIds)+1
	else:          sampleId = 0 
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Adding sample '+newSampleName+' to database with id '+str(sampleId)+'.\n')
	values = (sampleId,newSampleName)
	self.c.execute('INSERT INTO samples VALUES (?,?)', values)
	
	sample = Sample(sampleName=newSampleName, sampleId=sampleId)
	sample.createDirs()
	
	self.commitAndClose()
	
	return 0

    def getSamples(self):
	#
	# Imports
	#
	import sys
	import time
	
	#
	# open connection to database
	#
	self.getConnection()
	
	samples = []
	
	data = self.c.execute('SELECT sampleId,sampleName FROM samples').fetchall()
	if data:
	    for (sampleId,sampleName) in data: samples.append( Sample(sampleName=sampleName,sampleId=int(sampleId)) )
	
	self.commitAndClose()
	
	return samples

    def addFastqs(self, sampleNameOrId, fastq1, fastq2):

	#
	# Imports
	#
	import sys
	import os
	import time
	
	fastq1 = os.path.abspath(fastq1)
	fastq2 = os.path.abspath(fastq2)
	
	samples = AnalysisPipe.database.getSamples()
	samplesbyName = {}
	samplesbyId = {}
	for sample in samples:
	    samplesbyId[sample.id]=sample
	    samplesbyName[sample.name]=sample
	sampleName = None
	sampleId = None
	try:
	    if int(sampleNameOrId) in [int(value.id)  for value in samplesbyName.values()]:sampleId = sampleNameOrId; sampleName = samplesbyId[int(sampleId)].name
	except ValueError: pass
	if   sampleNameOrId  in samplesbyName.keys():sampleName = sampleNameOrId; sampleId = samplesbyName[sampleName].id
	else:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# SampleName (or id) must be registered in the database, there is no sample with name or id '+str(sampleNameOrId)+' , exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	#
	# open connection to database
	#
	self.getConnection()
	
	filePairId = None
	filePairIds = []
	
	#
	# check if any of the fastqs already in database
	#
	data = self.c.execute('SELECT filePairId,fastq1,fastq2 FROM fastqs').fetchall()
	if data:
	    for filePair in data:
		filePairId = int(filePair[0])
		filePairIds.append(filePairId)
		for fastq in [fastq1, fastq2]:
		    if fastq in filePair:
			message = 'ERROR: '+fastq+' already in the database.\nExiting after error.'
			print message
			AnalysisPipe.logfile.write(message+'\n')
			sys.exit(1)
	#
	# if not in the database add a new row
	#
	AnalysisPipe.logfile.write('Getting readcount for file'+fastq1+' ... \n')
	readCount = bufcount(fastq1)/4 #one read is four lines
	AnalysisPipe.logfile.write('...done. The file has '+str(readCount)+' reads.\n')
	addedToReadsTable = False#SEAseqPipeLine.startTimeStr
	minReadLength = 'NA'

	if filePairIds: filePairId = max(filePairIds)+1
	else: filePairId = 0
	values = (filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId)
	self.c.execute('INSERT INTO fastqs VALUES (?,?,?,?,?,?,?)', values)
	
	self.commitAndClose()
	
	return 0

    def getFastqs(self,):
	#
	# Imports
	#
	import sys
	
	#
	# open connection to database
	#
	self.getConnection()
		
	#
	# get att data in fastqs table
	#
	filePairs = self.c.execute('SELECT filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId FROM fastqs').fetchall()
	
	self.commitAndClose()
	
	#return [[readCount,fastq1,fastq2] if (not addedToReadsTable) else None for filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength in filePairs]
	return [[filePairId,readCount,fastq1,fastq2,sampleId] for filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId in filePairs]

    def getRuns(self, runTypes):

	self.getConnection()

	runsInfo = []
	data = self.c.execute('SELECT * FROM runs').fetchall()
	for startTime, command, commandLine, finishedSuccessfully, masterPid in data:
	    if command in runTypes: runsInfo.append([startTime, command, commandLine, finishedSuccessfully, masterPid])

	self.commitAndClose()

	return runsInfo

class Sample(object):

    def __init__(self, sampleName=None,sampleId=None):
	self.name = sampleName
	self.id = int(sampleId)
	self.path = AnalysisPipe.path+'/samples/'+self.name
	self.scriptPath = AnalysisPipe.path+'/samples/'+self.name+'/script'
	self.dataPath   = AnalysisPipe.path+'/samples/'+self.name+'/data'
	self.logPath    = AnalysisPipe.path+'/samples/'+self.name+'/logs'
	self.fastqcPath = AnalysisPipe.path+'/samples/'+self.name+'/fastQC'
	self.dependencies = {}

    @property
    def readCount(self, ):
	tmpCounter = 0
	for filePairId,readCount,fastq1,fastq2,sampleId in AnalysisPipe.database.getFastqs():
	    if int(sampleId) == self.id:tmpCounter+= readCount
	return tmpCounter

    def getFastqs(self):
	self.fastqIds = []
	for filePairId,readCount,fastq1,fastq2,sampleId in AnalysisPipe.database.getFastqs():
	    if int(sampleId) == self.id:
		self.fastqIds.append(filePairId)
		yield [filePairId,readCount,fastq1,fastq2,sampleId]

    def createDirs(self):
        import os
        try: os.makedirs(self.path)
        except OSError:pass
        try: os.makedirs(self.scriptPath)
        except OSError:pass
        try: os.makedirs(self.dataPath)
        except OSError:pass
        try: os.makedirs(self.fastqcPath)
        except OSError:pass
        try: os.makedirs(self.logPath)
        except OSError:pass

    def trimFastqs(self):
	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():

	    import sys
	    import time
	    try: project = sys.argv[3]
	    except IndexError:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	    #
	    # sbatch header
	    #
	    output = ''
	    output += '#! /bin/bash -l'+'\n'
	    output += '#SBATCH -A '+project+'\n'
	    output += '#SBATCH -n 2 -p core'+'\n'
	    output += '#SBATCH -t 72:00:00'+'\n'
	    output += '#SBATCH -J trim.'+self.name+'.'+str(filePairId)+'\n'
	    output += '#SBATCH -e '+self.logPath+'/stderr.trimming.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    output += '#SBATCH -o '+self.logPath+'/stdout.trimming.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    
	    try:
		output += '#SBATCH --mail-type=All'+'\n'
		output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	    except IndexError: pass
	    
	    #
	    # define variebles and go to path
	    #
	    output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	    output += 'cd '+self.path+'\n'
	    output += 'echo "-----"'+'\n'
	    
	    output += 'module load bioinfo-tools FastQC #cutadapt/1.5.0'+'\n'
	    output += 'workon py2.7\n\n'

	    #
	    # WGA adapter trimming
	    #
	    output += ''+AnalysisPipe.scriptPath+'/wgaAdapterTrimmer.py -i '+fastq1+' > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq 2> '+self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += ''+AnalysisPipe.scriptPath+'/wgaAdapterTrimmer.py -i '+fastq2+' > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq 2> '+self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'
	    
	    output += '\n'
	    output += 'cutadapt -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq  2> '+self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += 'cutadapt -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq  2> '+self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'
	    output += '\n'
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq\n'
	    
	    #
	    # illumina  adapter trimming
	    #
	    adaptersToTrim = '-a CTGTCTCTTATACACATCTGACGCTGCCGACGA -a CTGTCTCTTATACACATCTCCGAGCCCACGAGAC -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -a AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT'
	    output += 'cutadapt -n 3 '+adaptersToTrim+' '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq 2> '+self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += 'cutadapt -n 3 '+adaptersToTrim+' '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq 2> '+self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'

	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq \n'
	    output += 'wait\n'
	    
	    #
	    # quality trimmming
	    #
	    output += ''+AnalysisPipe.scriptPath+'/TrimBWAstyle.pl -q 20 '+self.dataPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq 2> '+self.logPath+'/qualityTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += ''+AnalysisPipe.scriptPath+'/TrimBWAstyle.pl -q 20 '+self.dataPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq 2> '+self.logPath+'/qualityTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'
	    
	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq\n'
	    output += 'wait\n'
	    
	    #
	    # remove empty or "N" only sequences
	    #
	    output += 'python '+AnalysisPipe.scriptPath+'/removeEmptyReads.py '
	    output += self.dataPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.singletts.fq '
	    output += '>&2 2> '+self.logPath+'/removeEmptyReads.'+str(filePairId)+'.log.txt\n'
	    
	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq\n'
	    output += 'wait\n'
	    
	    #
	    # compress files
	    #
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq &\n'
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq  &\n'
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.singletts.fq &\n'
	    output += 'wait\n'
	    
	    #
	    # FASTQC
	    #
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq.gz &\n'
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq.gz &\n'
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.singletts.fq.gz &\n'
	    output += 'wait\n'
	    output += 'mv -v '+self.dataPath+'/*fastqc* '+self.fastqcPath+'/\n'
	    
	    #
	    # Final output and write script to file
	    #
            output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
	    output += 'echo'+'\n'
	    output += 'wait'+'\n'
	    output += 'echo "$(date) AllDone"'+'\n'
	    output += 'echo "$(date) AllDone" >&2'+'\n'
	    with open(self.scriptPath+'/trimming.'+self.name+'.'+str(filePairId)+'.sh','w') as outfile: outfile.write(output)

    def mapFastqs(self):

	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():

	    import sys
	    import time
	    try: project = sys.argv[3]
	    except IndexError:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	    #
	    # sbatch header
	    #
	    output = '#! /bin/bash -l'+'\n'
	    output += '#SBATCH -A '+project+'\n'
	    output += '#SBATCH -n 16 -p node'+'\n'
	    output += '#SBATCH -t 5:00:00'+'\n'
	    output += '#SBATCH -J map.'+self.name+'.'+str(filePairId)+'\n'
	    output += '#SBATCH -e '+self.logPath+'/stderr.mapping.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    output += '#SBATCH -o '+self.logPath+'/stdout.mapping.'+self.name+'.'+str(filePairId)+'.txt'+'\n'

	    try:
		output += '#SBATCH --mail-type=All'+'\n'
		output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	    except IndexError: pass

	    #
	    # define variebles and go to path
	    #
	    output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	    output += 'cd '+self.path+'\n'
	    output += 'echo'+'\n'

	    #
	    # Bowtie2 mapping
	    #output += 'module load bioinfo-tools bwa/0.7.8\n'
	    #output += 'bwa mem -t 16 /sw/data/uppnex/reference/Homo_sapiens/GRCh37/program_files/bwa/concat.fa '+self.r1files[0]+' '+self.r2files[0]+' > '+self.sam+'\n'
	    #output += 'bowtie2 -1 '+self.r1files[0]+' -2 '+self.r2files[0]+' --very-sensitive-local -p16 -x '+self.reference+' > '+self.sam+'\n'
	    output += 'bowtie2 --maxins 2000 -p16 '
	    output += '-1 '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq.gz '
	    output += '-2 '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq.gz '
	    output += '-x '+AnalysisPipe.bowtie2Reference+' '
	    output += '> '+self.dataPath+'/'+str(filePairId)+'.sam '
	    output += '2> '+self.logPath+'/stderr.bowtie2.'+str(filePairId)+'.txt \n'
	    output += 'echo -e "mapping Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq.gz'+'\n'
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq.gz'+'\n'
	    

	    #
	    # Final output and write script to file
	    #
	    output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
	    output += 'wait'+'\n'
	    output += 'echo "$(date) AllDone"'+'\n'
	    output += 'echo "$(date) AllDone" >&2'+'\n'
	    with open(self.scriptPath+'/mapping.'+self.name+'.'+str(filePairId)+'.sh','w') as outfile: outfile.write(output)

    def mergeMapped(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	#
	# sbatch header
	#
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 5:00:00'+'\n'
	output += '#SBATCH -J merge.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.merge.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.merge.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass

	#
	# define variebles and go to path
	#
	output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	output += 'cd '+self.path+'\n'
	output += 'echo'+'\n'

	#
	# merge
	#
	inputFiles = ' INPUT='+' INPUT='.join([self.dataPath+'/'+str(filePairId)+'.sam' for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs()])
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/MergeSamFiles.jar '+inputFiles+' OUTPUT='+self.dataPath+'/'+self.name+'.merged.sam '
	output += '1>&2  2>  '+self.logPath+'/stderr.merging.'+self.name+'.txt \n'
	output += 'echo -e "mapping Done. $(date) Running on: $(hostname)" 1>&2'+'\n'

	#
	# Final output and write script to file
	#
	output += 'wait'+'\n'
	output += 'echo "$(date) AllDone"'+'\n'
	output += 'echo "$(date) AllDone" >&2'+'\n'
	with open(self.scriptPath+'/mergeMapped.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def filterAndFixMerged(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 5:00:00'+'\n'
	output += '#SBATCH -J fnf.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.filterAndFix.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.filterAndFix.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass

        #
        # define variebles and go to path
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'cd '+self.path+'\n'
        output += 'echo'+'\n'
        
        #
        # convert to bam file
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/SamFormatConverter.jar MAX_RECORDS_IN_RAM=2500000 '
	output += 'INPUT='+ self.dataPath+'/'+self.name+'.merged.sam '
	output += 'OUTPUT='+self.dataPath+'/'+self.name+'.merged.bam '
	output += '1>&2  2> '+self.logPath+'/stderr.sam2bam.'+self.name+'.txt \n'
        output += 'echo -e "sam2bam Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.merged.sam\n'

        #
        # sort the bam file
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/SortSam.jar MAX_RECORDS_IN_RAM=2500000 SORT_ORDER=coordinate '
	output += 'INPUT='+ self.dataPath+'/'+self.name+'.merged.bam '
        output += 'OUTPUT='+self.dataPath+'/'+self.name+'.sorted.bam '
	output += 'CREATE_INDEX=true 1>&2  2> '
	output += self.logPath+'/stderr.sortBam.'+self.name+'.txt \n'
        output += 'echo -e "bam2sort Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.merged.bam\n'

        #
        # mark duplicates
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/MarkDuplicates.jar MAX_RECORDS_IN_RAM=2500000 VALIDATION_STRINGENCY=LENIENT '
	output += 'INPUT='+ self.dataPath+'/'+self.name+'.sorted.bam '
	output += 'OUTPUT='+self.dataPath+'/'+self.name+'.marked.bam '
	output += 'METRICS_FILE='+self.logPath+'/markDuplicatesMetrix.'+self.name+'.txt '
	output += '1>&2  2> '+self.logPath+'/stderr.markDuplicates.'+self.name+'.txt \n'
        output += 'echo -e "mark Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.sorted.bam\n'

        #
        # fix missing information
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/AddOrReplaceReadGroups.jar '
        output += 'MAX_RECORDS_IN_RAM=2500000 '
        output += 'INPUT='+ self.dataPath+'/'+self.name+'.marked.bam '
        output += 'OUTPUT='+self.dataPath+'/'+self.name+'.fixed.bam '
        output += 'CREATE_INDEX=true RGID='+self.name+' RGLB='+self.name+' RGPL=ILLUMINA RGSM='+self.name+' RGCN="NA" RGPU="NA"'+'  '
	output += '1>&2  2> '+self.logPath+'/stderr.addAndReplaceReadGroups.'+self.name+'.txt \n'
        output += 'echo "addorreplace Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.marked.bam\n'

        #
        # samtools flagstat
        #
        output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.fixed.bam'+' > '+self.logPath+'/fixedBamFlagstat.'+self.name+'.txt \n'
        output += 'echo "flagstat Done. $(date) Running on: $(hostname)" 1>&2'+'\n'

        #
        # Final output and write script to file
        #
        output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
        output += 'wait'+'\n'
        output += 'echo "$(date) AllDone"'+'\n'
        output += 'echo "$(date) AllDone" >&2'+'\n'
	with open(self.scriptPath+'/FilterAndFix.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def realignerTargetCreator(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #

	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 16 -p node'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J realTC.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.realTC.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.realTC.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass

        #
        # define variebles and go to path
        #
        output += 'echo "$(date) Running on: $(hostname)"\n'

        #
        # Find targets for indel realignment
        #
        output += 'echo -e "-> RealignerTargetCreator <-"\n'
        output += 'java -Xmx72g -jar '+AnalysisPipe.gatkLocation+' -T RealignerTargetCreator '
	output += '-nt 16 '
	output += '-I '+self.dataPath+'/'+self.name+'.fixed.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-o '+self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/Mills_and_1000G_gold_standard.indels.b37.vcf'
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/1000G_phase1.indels.b37.vcf '
	output += '1>&2 2> '+self.logPath+'/stderr.RealignerTargetCreator.'+self.name+'.txt;'
        output += '\n'
        
        with open(self.scriptPath+'/realignerTargetCreator.'+self.name+'.sh','w') as outfile: outfile.write(output)
    
    def reAlignAndReCalibrate(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J reAlign.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.reAlign.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.reAlign.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'

        #
        # Realign reads around indels
        #
        output += 'echo -e "-> IndelRealigner <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T IndelRealigner '
	output += '-I '+self.dataPath+'/'+self.name+'.fixed.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-targetIntervals '+self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
        output += ' -o '+self.dataPath+'/'+self.name+'.reAligned.bam'+' '
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/Mills_and_1000G_gold_standard.indels.b37.vcf'
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/1000G_phase1.indels.b37.vcf  '
	output += '1>&2 2> '+self.logPath+'/stderr.indelRealigner.'+self.name+'.txt;'+'\n'
        output += '\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'echo "$(date) Running on: $(hostname)"\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.fixed.bam'+'\n'
        
        #
        # Quality recalibration
        #
        output += 'echo -e "-> BaseRecalibrator <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T BaseRecalibrator '
	output += '-I '+self.dataPath+'/'+self.name+'.reAligned.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-o '+self.dataPath+'/'+self.name+'.BQSR.grp'+' '
        output += ' -knownSites '+AnalysisPipe.gatkBundleLocation+'/dbsnp_138.b37.vcf '
	output += '1>&2 2> '+self.logPath+'/stderr.baseRecalibrator.'+self.name+'.txt;'+'\n'

        output += '\n'
        output += 'echo -e "-> PrintReads <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T PrintReads '
	output += '-I '+self.dataPath+'/'+self.name+'.reAligned.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-BQSR '+self.dataPath+'/'+self.name+'.BQSR.grp'+' '
	output += '-o '+self.dataPath+'/'+self.name+'.reCalibrated.bam'+' '
	output += '1>&2 2> '+self.logPath+'/stderr.printreads.txt ;\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.reAligned.bam'+'\n'
        output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.reCalibrated.bam > '+self.logPath+'/reCalibratedBamFlagstat.'+self.name+'.txt \n'

	output += 'samtools view -b -F 4 '   +self.dataPath+'/'+self.name+'.reCalibrated.bam > '+self.dataPath+'/'+self.name+'.unmapRemoved.bam  2> '+self.logPath+'/stderr.samtoolsView.removeUnmap.'+self.name+'.txt \n'
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.dataPath+'/'+self.name+'.unmapRemoved.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex1.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.unmapRemoved.bam > '+self.logPath+'/unmapRemovedBamFlagstat.'+self.name+'.txt \n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.reCalibrated.bam\n'

	output += 'samtools view -b -q 20 '  +self.dataPath+'/'+self.name+'.unmapRemoved.bam > '+self.dataPath+'/'+self.name+'.qualFiltered.bam  2> '+self.logPath+'/stderr.samtoolsView.qualFilter.'+self.name+'.txt \n'
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.dataPath+'/'+self.name+'.qualFiltered.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex2.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.qualFiltered.bam > '+self.logPath+'/qualFilteredBamFlagstat.'+self.name+'.txt \n'
	output += 'rm -v '+self.dataPath+'/'+self.name+'.unmapRemoved.bam\n'

	output += 'samtools view -b -F 1024 '+self.dataPath+'/'+self.name+'.qualFiltered.bam > '+self.dataPath+'/'+self.name+'.noDuplicates.bam  2> '+self.logPath+'/stderr.samtoolsView.removeDups.'+self.name+'.txt \n'
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.dataPath+'/'+self.name+'.noDuplicates.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex3.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.noDuplicates.bam > '+self.logPath+'/noDuplicatesBamFlagstat.'+self.name+'.txt \n'
	output += 'rm -v '+self.dataPath+'/'+self.name+'.qualFiltered.bam\n'

        #
        # Final output and write script to file
        #
        output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'wait\n'
        output += 'echo "$(date) AllDone"\n'
        with open(self.scriptPath+'/reAlignAndReCalibrate.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def haplotypeCalling(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J hapCal.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.haplotypeCalling.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.haplotypeCalling.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'

        output += 'echo "HC" '+'\n'
        
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' '
        output += '-T HaplotypeCaller '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
        output += '-I '+self.dataPath+'/'+self.name+'.noDuplicates.bam '
        output += '--genotyping_mode DISCOVERY '
        output += '-stand_emit_conf 10 '
        output += '-stand_call_conf 30 '
        output += '-L '+AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem.bed '
        output += '--dbsnp '+AnalysisPipe.gatkBundleLocation+'/dbsnp_138.b37.vcf '
        output += '--annotation AlleleBalance --annotation AlleleBalanceBySample --annotation BaseCounts --annotation BaseQualityRankSumTest '
        output += '--annotation ChromosomeCounts --annotation ClippingRankSumTest --annotation Coverage --annotation DepthPerAlleleBySample '
        output += '--annotation DepthPerSampleHC --annotation FisherStrand --annotation GCContent --annotation HaplotypeScore --annotation HardyWeinberg '
        output += '--annotation HomopolymerRun --annotation InbreedingCoeff --annotation LikelihoodRankSumTest --annotation LowMQ '
        output += '--annotation MVLikelihoodRatio --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation MappingQualityZeroBySample '
        output += '--annotation NBaseCount --annotation QualByDepth --annotation RMSMappingQuality --annotation ReadPosRankSumTest --annotation SampleList '
        output += '--annotation SnpEff --annotation SpanningDeletions --annotation StrandBiasBySample --annotation TandemRepeatAnnotator '
        output += '--annotation TransmissionDisequilibriumTest --annotation VariantType '#--annotation StrandOddsRatio 
        output += '--emitRefConfidence GVCF --variant_index_type LINEAR --variant_index_parameter 128000 '
        output += '-o '+self.dataPath+'/'+self.name+'.gvcf '
	output += '1>&2 2> '+self.logPath+'/stderr.haplotypeCallerGatk.'+self.name+'.txt &'+'\n'
        output += 'wait'+'\n'

        #
        # Final output and write script to file
        #
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'wait\n'
        output += 'echo "$(date) AllDone"\n'
        with open(self.scriptPath+'/haplotypeCalling.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def qcSteps(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J qcSteps.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.qcSteps.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.qcSteps.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[4]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'

        #
        # GATK callable Loci
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo -e "-> CallableLoci <-"'+'\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T CallableLoci '
	output +='-I '+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output +='-summary '+self.dataPath+'/'+self.name+'.callableLociSummary.txt '
	output +='-o '+self.dataPath+'/'+self.name+'.callableLoci.bed '
	output +='-R '+AnalysisPipe.bowtie2Reference+' '+'\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'
        output += 'echo'+'\n'
        output += 'echo "-----"'+'\n'

        #
        # qacompute
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo -e "-> Pauls qacompute <-"'+'\n'
        output += '/proj/b2010052/scripts/qaCompute -d -q 10 '
	output += '-m '+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output += self.dataPath+'/'+self.name+'.qacompute.out '
	output += '> '+self.logPath+'/'+self.name+'.qacompute.stdout.txt '
	output += '2> '+self.logPath+'/'+self.name+'.qacompute.stderr.txt '+'\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'

        #
        # picard HS metrics
        #
        output += 'java -Xmx3g -jar '+AnalysisPipe.picardLocation+'/CalculateHsMetrics.jar '
	output += 'BAIT_INTERVALS='  +AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem '
	output += 'TARGET_INTERVALS='+AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem '
	output += 'INPUT='+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output += 'OUTPUT='+self.dataPath+'/'+self.name+'.hs_metrics.summary.txt '
	output += 'PER_TARGET_COVERAGE='+self.dataPath+'/'+self.name+'.hs_metrics.perTargetCoverage.txt '
	output += 'REFERENCE_SEQUENCE='+AnalysisPipe.bowtie2Reference+'  '
	output += '1>&2 2> '+self.logPath+'/'+self.name+'.stderr.caluclateHsmetrics.txt \n'


	#
	# make files for coverage checks
	#
	output += "bedtools coverage -abam "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" -b "+AnalysisPipe.referencePath+"/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem.bed -d > "+self.dataPath+'/'+self.name+'.bedtools.coverage.bed\n'
	output += "awk '{print $7}' "+self.dataPath+'/'+self.name+'.bedtools.coverage.bed'+" | sort -n | uniq -c | awk '{print $1\"\\t\"$2}'> "+self.dataPath+'/'+self.name+".coverageDistribution.tsv\n"
	output += "awk '{print $1 \"\\t\" $2+$6-1 \"\\t\" $7}' "+self.dataPath+'/'+self.name+'.bedtools.coverage.bed'+" > "+self.dataPath+'/'+self.name+".depthPerPosition.tsv\n"

	
        #
        # Final output and write script to file
        #
        output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
	output += 'echo'+'\n'
        output += 'wait'+'\n'
        output += 'echo "$(date) AllDone"'+'\n'
        output += 'echo "$(date) AllDone" >&2'+'\n'
        with open(self.scriptPath+'/qcSteps.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def getStats(self):
	
	import re
	
	stats = {}
	stats['rubiconWgaTrimming'] = {}
	stats['malbacWgaTrimming']  = {}
	stats['illuminaAndNexteraTrimming']  = {}
	stats['qualityTrimming']  = {}
	stats['removeEmptyReads']  = {}
	stats['bowtie2'] = {}
	
	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
	    stats['rubiconWgaTrimming'][filePairId] = {'r1':None,'r2':None}
	    stats['malbacWgaTrimming'][filePairId]  = {'r1':None,'r2':None}
	    stats['illuminaAndNexteraTrimming'][filePairId]  = {'r1':None,'r2':None}
	    stats['qualityTrimming'][filePairId]    = {'r1':None,'r2':None}
	    
	    for read in ['r1','r2']:
		stats['rubiconWgaTrimming'][filePairId][read]         = extractData(infile=self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.'+read+'.log.txt',        pattern="Running wgaAdapterTrimmer.py\nProcessed a total of\t(?P<totalReads>\d+)\treads. \(.+\)\nProcessed a total of\t(?P<totalBases>\d+)\tbases \(.+\).\ntrimmed a total of\t(?P<trimmedBases>\d+)\tbases in the start of reads \(.+\).\nwgaAdapterTrimmer.py done exiting ...\n?")
		stats['malbacWgaTrimming'][filePairId][read]          = extractData(infile=self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.'+read+'.log.txt',         pattern="cutadapt version .+\nCommand line parameters: -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC .+\nMaximum error rate\: .+\%\n\s+No. of adapters\: 2\n\s+Processed reads\:\s+(?P<totalReads>\d+)\n\s+Processed bases\:\s+(?P<totalBases>\d+) bp \(.+ Mbp\)\n\s+Trimmed reads\:\s+(?P<trimmedReads>\d+) \(.+\%\)\n\s+Trimmed bases\:\s+(?P<trimmedBases>\d+) bp \(.+ Mbp\) \(.+\% of total\)\n\s+Too short reads\:\s+.+ \(.+\% of processed reads\)\n\s+Too long reads\:\s+.+ \(.+\% of processed reads\)\n\s+Total time\:\s+.+ s\n\s+Time per read\:\s+.+ ms")
		stats['illuminaAndNexteraTrimming'][filePairId][read] = extractData(infile=self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.'+read+'.log.txt',pattern="cutadapt version .+\nCommand line parameters: -n .+\nMaximum error rate\: .+\%\n\s+No. of adapters\: 4\n\s+Processed reads\:\s+(?P<totalReads>\d+)\n\s+Processed bases\:\s+(?P<totalBases>\d+) bp \(.+ Mbp\)\n\s+Trimmed reads\:\s+(?P<trimmedReads>\d+) \(.+\%\)\n\s+Trimmed bases\:\s+(?P<trimmedBases>\d+) bp \(.+ Mbp\) \(.+\% of total\)\n\s+Too short reads\:\s+.+ \(.+\% of processed reads\)\n\s+Too long reads\:\s+.+ \(.+\% of processed reads\)\n\s+Total time\:\s+.+ s\n\s+Time per read\:\s+.+ ms")
		stats['qualityTrimming'][filePairId][read]            = extractData(infile=self.logPath+'/qualityTrimming.'+str(filePairId)+'.'+read+'.log.txt',           pattern='(?P<totalBasess>\d+)\tbases\n(?P<trimmedBases>\d+)\ttrimmed')

	    stats['removeEmptyReads'][filePairId] = extractData(infile=self.logPath+'/removeEmptyReads.'+str(filePairId)+'.log.txt',pattern="""Running removeEmptyReads.py:\nHeader one is empty exiting.\n(?P<totalReads>\d+) read pairs processed.\n(?P<pairsOut>\d+) read pairs to outfiles .+.\n(?P<singlets>\d+) single reads to outfile .+.\nremoveEmptyReads Exiting.""")
	    stats['bowtie2'][filePairId]          = extractData(infile=self.logPath+'/stderr.bowtie2.'+str(filePairId)+'.txt',      pattern="""(?P<totalReads>\d+) reads; of these:\n\s+(?P<pairedReads>\d+) \(\d+.\d+\%\) were paired; of these:\n\s+(?P<notPropMapedPair>\d+) \(\d+.\d+\%\) aligned concordantly 0 times\n\s+(?P<properPairs>\d+) \(\d+.\d+\%\) aligned concordantly exactly 1 time\n\s+(?P<properPairsMultiMap>\d+) \(\d+.\d+\%\) aligned concordantly >1 times\n\s+----\n\s+(?P<notPropMapedPair2>\d+) pairs aligned concordantly 0 times; of these:\n\s+(?P<discordantPairs>\d+) \(\d+.\d+\%\) aligned discordantly 1 time\n\s+----\n\s+(?P<unMappedPair>\d+) pairs aligned 0 times concordantly or discordantly; of these:\n\s+(?P<possibleSingletons>\d+) mates make up the pairs; of these:\n\s+(?P<unMappedReads>\d+) \(\d+.\d+\%\) aligned 0 times\n\s+(?P<singleSingleMap>\d+) \(\d+.\d+\%\) aligned exactly 1 time\n\s+(?P<singleMultiMap>\d+) \(\d+.\d+\%\) aligned >1 times\n(?P<overallAlignmentRate>\d+.\d+)\% overall alignment rate""")

	stats['merging'] = extractData(infile=self.logPath+'/stderr.merging.'+self.name+'.txt',pattern="Finished reading inputs.+\n.+picard.sam.MergeSamFiles done. Elapsed time",checkType='program')
	pattern = 'LIBRARY\tUNPAIRED_READS_EXAMINED\tREAD_PAIRS_EXAMINED\tUNMAPPED_READS\tUNPAIRED_READ_DUPLICATES\tREAD_PAIR_DUPLICATES\tREAD_PAIR_OPTICAL_DUPLICATES\tPERCENT_DUPLICATION\tESTIMATED_LIBRARY_SIZE\n(?P<LIBRARY>.+)\t(?P<UNPAIRED_READS_EXAMINED>\d+)\t(?P<READ_PAIRS_EXAMINED>\d+)\t(?P<UNMAPPED_READS>\d+)\t(?P<UNPAIRED_READ_DUPLICATES>\d+)\t(?P<READ_PAIR_DUPLICATES>\d+)\t(?P<READ_PAIR_OPTICAL_DUPLICATES>\d+)\t(?P<PERCENT_DUPLICATION>\d+\,\d+)\t(?P<ESTIMATED_LIBRARY_SIZE>\d+)'
	oldpattern="""LIBRARY\tUNPAIRED_READS_EXAMINED\tREAD_PAIRS_EXAMINED\tUNMAPPED_READS\tUNPAIRED_READ_DUPLICATES\tREAD_PAIR_DUPLICATES\tREAD_PAIR_OPTICAL_DUPLICATES\tPERCENT_DUPLICATION\tESTIMATED_LIBRARY_SIZE\n(?P<Library>.+)\s+(?P<unPairedReads>\d+)\s+(?P<totalReads>\d+)\s+(?P<unMapped>\d+)\s+(?P<unPairedDups>\d+)\s+(?P<pairDups>\d+)\s+(?P<opticalDups>\d+)\s+(?P<percentageDuplication>\d+\,\d+)\s+(?P<estLibSize>\d+)"""
	stats['markDuplicatesMetrix'] = extractData(infile=self.logPath+'/markDuplicatesMetrix.'+self.name+'.txt',pattern=pattern)
	stats['fixedBamFlagstat']        = extractData(infile=self.logPath+'/fixedBamFlagstat.'+self.name+'.txt',       pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['reCalibratedBamFlagstat'] = extractData(infile=self.logPath+'/reCalibratedBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['unmapRemovedBamFlagstat'] = extractData(infile=self.logPath+'/unmapRemovedBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['qualFilteredBamFlagstat'] = extractData(infile=self.logPath+'/qualFilteredBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['noDuplicatesBamFlagstat'] = extractData(infile=self.logPath+'/noDuplicatesBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	
	self.logPath+'/'+self.name+'.qacompute.stdout.txt'
	self.logPath+'/'+self.name+'.qacompute.stderr.txt'
	self.dataPath+'/'+self.name+'.qacompute.out '
	#self.logPath+'/'+self.name+'.stderr.caluclateHsmetrics.txt'
	pattern = 'READ_GROUP\n(?P<BAIT_SET>.+)\t(?P<GENOME_SIZE>\d+)\t(?P<BAIT_TERRITORY>\d+)\t(?P<TARGET_TERRITORY>\d+)\t(?P<BAIT_DESIGN_EFFICIENCY>\d+(\,\d+)?)\t(?P<TOTAL_READS>\d+)\t(?P<PF_READS>\d+)\t(?P<PF_UNIQUE_READS>\d+)\t(?P<PCT_PF_READS>\d+(\,\d+)?)\t(?P<PCT_PF_UQ_READS>\d+(\,\d+)?)\t(?P<PF_UQ_READS_ALIGNED>\d+)\t(?P<PCT_PF_UQ_READS_ALIGNED>\d+(\,\d+)?)\t(?P<PF_UQ_BASES_ALIGNED>\d+)\t(?P<ON_BAIT_BASES>\d+)\t(?P<NEAR_BAIT_BASES>\d+)\t(?P<OFF_BAIT_BASES>\d+)\t(?P<ON_TARGET_BASES>\d+)\t(?P<PCT_SELECTED_BASES>\d+(\,\d+)?)\t(?P<PCT_OFF_BAIT>\d+(\,\d+)?)\t(?P<ON_BAIT_VS_SELECTED>\d+(\,\d+)?)\t(?P<MEAN_BAIT_COVERAGE>\d+(\,\d+)?)\t(?P<MEAN_TARGET_COVERAGE>\d+(\,\d+)?)\t(?P<PCT_USABLE_BASES_ON_BAIT>\d+(\,\d+)?)\t(?P<PCT_USABLE_BASES_ON_TARGET>\d+(\,\d+)?)\t(?P<FOLD_ENRICHMENT>\d+(\,\d+)?)\t(?P<ZERO_CVG_TARGETS_PCT>\d+(\,\d+)?)\t(?P<FOLD_80_BASE_PENALTY>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_2X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_10X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_20X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_30X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_40X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_50X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_100X>\d+(\,\d+)?)\t(?P<HS_LIBRARY_SIZE>(\s?)|(\d+(\,\d+)?))\t(?P<HS_PENALTY_10X>\d+(\,\d+)?)\t(?P<HS_PENALTY_20X>\d+(\,\d+)?)\t(?P<HS_PENALTY_30X>\d+(\,\d+)?)\t(?P<HS_PENALTY_40X>\d+(\,\d+)?)\t(?P<HS_PENALTY_50X>\d+(\,\d+)?)\t(?P<HS_PENALTY_100X>\d+(\,\d+)?)'
	stats['hs_metrics.summary'] = extractData(infile=self.dataPath+'/'+self.name+'.hs_metrics.summary.txt',pattern=pattern)

	### OUTFILES TO ADD CHECHS FOR LATER:
	#self.logPath+'/stderr.trimming.'+self.name+'.'+str(filePairId)+'.txt'
	#self.logPath+'/stdout.trimming.'+self.name+'.'+str(filePairId)+'.txt'
	#self.fastqcPath+'/\n'
	#self.logPath+'/stderr.mapping.'+self.name+'.'+str(filePairId)+'.txt'
	#self.logPath+'/stdout.mapping.'+self.name+'.'+str(filePairId)+'.txt'
	#self.logPath+'/stderr.merge.'+self.name+'.txt'
	#self.logPath+'/stdout.merge.'+self.name+'.txt'
	#self.logPath+'/stderr.filterAndFix.'+self.name+'.txt'
	#self.logPath+'/stdout.filterAndFix.'+self.name+'.txt'
	#self.logPath+'/stderr.sam2bam.'+self.name+'.txt'
	#self.logPath+'/stderr.sortBam.'+self.name+'.txt'
	#self.logPath+'/stderr.markDuplicates.'+self.name+'.txt'
	#self.logPath+'/stderr.addAndReplaceReadGroups.'+self.name+'.txt'
	#self.logPath+'/stderr.realTC.'+self.name+'.txt'
	#self.logPath+'/stdout.realTC.'+self.name+'.txt'
	#self.logPath+'/stderr.RealignerTargetCreator.'+self.name+'.txt;'
	#self.logPath+'/stderr.reAlign.'+self.name+'.txt'
	#self.logPath+'/stdout.reAlign.'+self.name+'.txt'
	#self.logPath+'/stderr.indelRealigner.'+self.name+'.txt;'+''
	#self.logPath+'/stderr.baseRecalibrator.'+self.name+'.txt;'+''
	#self.logPath+'/stderr.printreads.txt
	#self.logPath+'/stderr.samtoolsView.removeUnmap.'+self.name+'.txt'
	#self.logPath+'/stderr.samtoolsView.qualFilter.'+self.name+'.txt'
	#self.logPath+'/stderr.samtoolsView.removeDups.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex1.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex2.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex3.'+self.name+'.txt'
	#self.logPath+'/stderr.haplotypeCallerGatk.'+self.name+'.txt'
	#self.logPath+'/stderr.haplotypeCalling.'+self.name+'.txt'
	#self.logPath+'/stdout.haplotypeCalling.'+self.name+'.txt'
	#self.dataPath+'/'+self.name+'.gvcf '
	#self.logPath+'/stderr.qcSteps.'+self.name+'.txt'
	#self.logPath+'/stdout.qcSteps.'+self.name+'.txt'
	#self.dataPath+'/'+self.name+'.hs_metrics.perTargetCoverage.txt '
	#self.dataPath+'/'+str(filePairId)+'.sam'
	#self.dataPath+'/'+self.name+'.merged.sam'
	#self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
	#self.dataPath+'/'+self.name+'.BQSR.grp'+' '
	#self.dataPath+'/'+self.name+'.reCalibrated.bam\n'
	#self.dataPath+'/'+self.name+'.unmapRemoved.bam\n'
	#self.dataPath+'/'+self.name+'.qualFiltered.bam\n'
	#self.dataPath+'/'+self.name+'.noDuplicates.bam '
	#self.dataPath+'/'+self.name+'.callableLociSummary.txt '
	#self.dataPath+'/'+self.name+'.callableLoci.bed '

	# make sums
	for program in ['illuminaAndNexteraTrimming','malbacWgaTrimming','qualityTrimming','rubiconWgaTrimming']:
	    try:
		sums = {variable:0 for variable in stats[program][self.getFastqs().next()[0]]['r1'].keys()}
		for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
		    for read in ['r1','r2']:
			for variable, value in stats[program][filePairId][read].iteritems(): sums[variable]+=float(value)
		stats[program]['sum']= sums
	    except AttributeError: pass
	for program in ['removeEmptyReads','bowtie2']:
	    try:
		sums = {variable:0 for variable in stats[program][self.getFastqs().next()[0]].keys()}
		for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
		    for variable, value in stats[program][filePairId].iteritems(): sums[variable]+=float(value)
		stats[program]['sum']= sums
	    except AttributeError: pass
	    
	self.stats = stats

#	 debug output
	print "\n######## "+self.name+" ######## "
	for key,value in stats.iteritems():
	    print '    ',key
	    try:
		for key2,value2 in value.iteritems():
		    assert type(value2) == dict
		    print '        ',key2,value2
	    except:print '        ',value

if __name__ == "__main__": main()


# Should add
# program checks
# Lorentz curve
# exome coverage
# exome coverage @ RD
# insert sizes
# HS metrics summary

# Possible picard tols to utilize
# BamIndexStats
# CollectMultipleMetrics
#    CollectAlignmentSummaryMetrics
#    CollectInsertSizeMetrics
# CollectGcBiasMetrics
# EstimateLibraryComplexity
# CollectWgsMetrics or CalculateHsMetrics
# GenotypeConcordance
