#!/usr/bin/env python

helpMessage = """
############ fnuttglugg ################
This is a pipe for DNA sequence analysis.
You can use it to look at fat cells and other stuff.

use:
fnuttglugg <analysisPath> addSample <sampleName> [refType eg. Pat/Don/gDNA/True]
fnuttglugg <analysisPath> addFastq <sampleName> <r1> <r2>
fnuttglugg <analysisPath> createScripts <uppmaxProjectNumber> <"wgs"|"exome"> <emailAdress>
fnuttglugg <analysisPath> submitScripts
...more to come ...
fnuttglugg <analysisPath> report
fnuttglugg <analysisPath> makeGraphics
fnuttglugg <analysisPath> reSubmit <uppmaxAccount> <"wgs"|"exome"> [<emailAdress>] 11111111 <sampleId>
fnuttglugg <analysisPath> copyReport <destination>
fnuttglugg <analysisPath> variationCheck
fnuttglugg <analysisPath> changeSetting variableName=value [listsettings]
"""

def main():
    
    import sys
    
    for term in ['h','help','-h','--help']:
	if term in sys.argv:
	    print helpMessage
	    sys.exit()

    app = AnalysisPipe()
    app.run()

def myround(x, base=10): return int(base * round(float(x)/base))

def thousandString(string):
    if type(string) != str: string = str(int(round(string,0)))
    outstring = ''
    for i in range(len(string)):
	outstring += string[-(i+1)]
	if (i+1)%3 == 0: outstring += ' '
    return outstring[::-1]

def percentage(count,total):
    if str in [type(count),type(total)]: return 'NA'
    if 'NA' in [total,count]: return 'NA'
    if float(total) <=0.0: return 'NA'
    #return round(float(count) / float(total),4)
    return round(100* float(count) / float(total),2)

def extractData(infile=None,pattern=None,checkType='data'):
    import re
    if not pattern: return 'NoPatternGiven'
    if not infile: return 'NoInfileGiven'
    try:
	with open(infile) as infile:
	    data = infile.read()
	    p = re.compile(pattern)
	    m = p.search(data)
	    if m:
		if   checkType=='data':   return m.groupdict()
		elif checkType=='program':return 'FinishedOK'
	    else: return 'NoMatchFound'
    except IOError, e:
	assert e.errno == 2;
	return 'NoFileFound'

def bufcount(filename): # funtion "stolen from the internet" and modified
	""" returns the number of lines in a file
	"""
	import gzip
	if filename.split('.')[-1] in ['gz','gzip']: f = gzip.open(filename)
	else: f = open(filename)
	lines = 0
	buf_size = 1024 * 1024 * 10 # ten megabyte
	read_f = f.read # loop optimization
	
	buf = read_f(buf_size)
	while buf:
		lines += buf.count('\n')
		buf = read_f(buf_size)
		f.close
	return lines

def submitSbatch(filename,dependency=None):
    import subprocess
    import sys
    if dependency and (None not in dependency): command = ['sbatch','--dependency=afterok:'+':'.join(dependency),filename]
    else:          command = ['sbatch',filename]
    sbatch = subprocess.Popen( command, stdout=subprocess.PIPE, stderr=subprocess.PIPE )
    sbatch_out, errdata = sbatch.communicate()
    if sbatch.returncode != 0:
	print 'sbatch view Error code', sbatch.returncode, errdata
	print sbatch_out
	print filename
	sys.exit()
    jobid = sbatch_out.split('\n')[0].split(' ')[3]
    return jobid

def getIsizeData(sample):
    import time
    import operator
    import shutil
    import os
    import sys
    import pysam
    import gzip    

    try: uppmax_temp = os.environ["SNIC_TMP"]
    except:
	uppmax_temp = None
	print 'Not on uppmax no temporary directory'

    #isizes = []
    isizes = {}
    
    if not os.path.exists(sample.dataPath+'/'+sample.name+'.iSizes.pylist.gz'):
	if not os.path.exists(sample.dataPath+'/'+sample.name+'.noDuplicates.bam'):
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping iSize stats for sample '+sample.name+' the infile has not been created yet...\n')
	    return sample, isizes
    
	if uppmax_temp:
	    try:os.mkdir(uppmax_temp+'/fnuttglugg_TMP')
	    except OSError:pass
	    if not os.path.exists(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'):
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Copying '+sample.dataPath+'/'+sample.name+'.noDuplicates.bam'+' to temp location for faster reading from disk, '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'+' \n')
		shutil.copy(sample.dataPath+'/'+sample.name+'.noDuplicates.bam',uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam')
	    else:
		print 'WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'+' skipping copy!!'
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'+' skipping copy!!\n')
	    bamfileName  = uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'
	else:bamfileName = sample.dataPath+'/'+sample.name+'.noDuplicates.bam'
	if uppmax_temp:
	    try:os.mkdir(uppmax_temp+'/fnuttglugg_TMP')
	    except OSError:pass
	    if not os.path.exists(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'):
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Copying '+sample.dataPath+'/'+sample.name+'.noDuplicates.bai'+' to temp location for faster reading from disk, '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'+' \n')
		try: shutil.copy(sample.dataPath+'/'+sample.name+'.noDuplicates.bai',uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai')
		except IOError as e: pass
	    else:
		print 'WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'+' skipping copy!!'
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'+' skipping copy!!\n')
    
	# do work here
	try: bamfile = pysam.Samfile(bamfileName, "rb")
	except IOError:
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping insert size plot for sample '+sample.name+' the infile has not been created yet...\n')
	    return sample, isizes
	except ValueError:
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping insert size plot for sample '+sample.name+' the infile is not finished for processing...\n')
	    return sample, isizes
    
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading insert sizes for sample '+sample.name+'...\n')
	try:
	    for read in bamfile.fetch():
		#if read.tlen >= 1: isizes.append(int(read.tlen))
		if read.tlen >= 1:
		    try: isizes[int(read.tlen)] +=1
		    except KeyError:isizes[int(read.tlen)] =1
	except ValueError as e:
	    if e == 'fetch called on bamfile without index':
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping insert size plot for sample '+sample.name+' the bam index is not present...\n')
		sys.stderr.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping insert size plot for sample '+sample.name+' the bam index is not present...\n')
		return sample, isizes
	#isizes.sort()
	isizesFile = gzip.open(sample.dataPath+'/'+sample.name+'.iSizes.pylist.gz','wb',9)
	isizesFile.write(str(isizes))
	isizesFile.close()
    else:
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading isizes from iSizes.pylist-file '+sample.name+' ...\n')
	isizes = eval(gzip.open(sample.dataPath+'/'+sample.name+'.iSizes.pylist.gz','rb').read())

    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# joining to main, '+sample.name+' ...\n')
    return sample, isizes

def getGiniData(sample):

    import time
    import operator
    import shutil
    import os
    import sys

    try: uppmax_temp = os.environ["SNIC_TMP"]
    except:
	uppmax_temp = None
	print 'Not on uppmax no temporary directory'

    #LOADING
    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading data for sample '+sample.name+' ...\n')

    def bedtoolsversion2(uppmax_temp):
    
	import re
	
	perBaseCoverages = {}
	perBaseCoverages[sample.name] = {0:1}
	depthPerPosition = {}
	depthPerPosition[sample.name] = {}
	referenceBaseCount = {}
	referenceBaseCount[sample.name] = 1
	lastChrom = None
	referenceBaseCountChrom = {}
	counter = 0

	if (AnalysisPipe.settings.mode=='wgs' and not os.path.exists(sample.dataPath+'/'+sample.name+'.bedtools.genomecov.nonPhysical.histogram.gz')) or (AnalysisPipe.settings.mode=='exome' and not os.path.exists(sample.dataPath+'/'+sample.name+'.bedtools.exomecoverage.nonPhysical.histogram.gz')):
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping coverage stats for sample '+sample.name+' the infile has not been created yet...\n')
	    return depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount

	import gzip
	if AnalysisPipe.settings.mode == 'exome': bedtoolsFile = gzip.open(sample.dataPath+'/'+sample.name+'.bedtools.exomecoverage.nonPhysical.histogram.gz')
	elif AnalysisPipe.settings.mode == 'wgs': bedtoolsFile = gzip.open(sample.dataPath+'/'+sample.name+'.bedtools.genomecov.nonPhysical.histogram.gz')

	if not bedtoolsFile: return depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount

	for line in bedtoolsFile:
	    
	    line = line.rstrip().split('\t')
	    
	    if AnalysisPipe.settings.mode == 'wgs' and not re.match('genome',line[0]): continue
	    if AnalysisPipe.settings.mode == 'exome' and not re.match('all',line[0]): continue
	    
	    
	    #genome	0	3097784897	3101804739	0.998704
	    referenceBaseCount[sample.name] = int(line[3])
	    perBaseCoverages[sample.name][int(line[1])] = int(line[2])
	bedtoolsFile.close()

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Data in subprocess memeory, '+sample.name+' ...\n')

	return depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount

    depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount = bedtoolsversion2(uppmax_temp)

    #GINI targeted
    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Lorentz/Gini processing, '+sample.name+' ...\n')
    pre_x = [count for rd, count in sorted(perBaseCoverages[sample.name].iteritems(), key=operator.itemgetter(0))]
    pre_y = [rd*count for rd, count in sorted(perBaseCoverages[sample.name].iteritems(), key=operator.itemgetter(0))]
    pre_x2 = [sum(pre_x[:i]) for i in range(len(pre_x))]
    pre_y2 = [sum(pre_y[:i]) for i in range(len(pre_y))]
    sequencedBases = sum(pre_y)
    try:
	gini_x = [0]+[float(value)/referenceBaseCount[sample.name] for value in pre_x2]
	gini_y = [0]+[float(value)/sequencedBases  for value in pre_y2]
    except ZeroDivisionError:
	gini_x = [0]
	gini_y = [0]

    #GINI covered
    pre_x = [count for rd, count in sorted(perBaseCoverages[sample.name].iteritems(), key=operator.itemgetter(0))][1:] # do not include positions with coverage 0
    pre_y = [rd*count for rd, count in sorted(perBaseCoverages[sample.name].iteritems(), key=operator.itemgetter(0))][1:] # do not include positions with coverage 0
    pre_x2 = [sum(pre_x[:i]) for i in range(len(pre_x))]
    pre_y2 = [sum(pre_y[:i]) for i in range(len(pre_y))]
    sequencedBases = sum(pre_y)
    coveredBases = sum([count for rd, count in sorted(perBaseCoverages[sample.name].iteritems(), key=operator.itemgetter(0))][1:])
    print '#',sample.name+':','covered',thousandString(str(coveredBases)),'| targeted',thousandString(str(referenceBaseCount[sample.name])),'| Percentege coverage of target ('+AnalysisPipe.settings.mode+')',percentage(coveredBases,referenceBaseCount[sample.name]),'%'
    with open(sample.dataPath+'/targetCoveragestat.txt','w') as outputfile: outputfile.write('# '+sample.name+': covered '+thousandString(str(coveredBases))+' | targeted '+thousandString(str(referenceBaseCount[sample.name]))+' | Percentege coverage of target ('+AnalysisPipe.settings.mode+') '+str(percentage(coveredBases,referenceBaseCount[sample.name]))+' %\n')
    try:
	gini_x_cov = [0]+[float(value)/coveredBases for value in pre_x2]
	gini_y_cov = [0]+[float(value)/sequencedBases  for value in pre_y2]
    except ZeroDivisionError:
	gini_x_cov = [0]
	gini_y_cov = [0]

    # COVERAGE AT RD
    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Coverage@RD processing, '+sample.name+' ...\n')
    sample.MaxCoverage = max( perBaseCoverages[sample.name].keys() )
    sample.MinCoverage = min( perBaseCoverages[sample.name].keys() )
    sample.AverageCoverage = sum( [rd*count for rd,count in perBaseCoverages[sample.name].iteritems()] ) / sum( perBaseCoverages[sample.name].values() )
    #print sample.name,sampleMinCoverage,sampleMaxCoverage,sampleAverageCoverage
    cAtRd_x = sorted(perBaseCoverages[sample.name].keys())
    y = [count for rd,count in sorted(perBaseCoverages[sample.name].iteritems(),key=operator.itemgetter(0))]
    cAtRd_y = [percentage(sum(y[i:]),referenceBaseCount[sample.name]) for i in range(len(y))] # cumulative y

    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# joining to main, '+sample.name+' ...\n')
    return sample, gini_x, gini_y,gini_x_cov, gini_y_cov, cAtRd_x, cAtRd_y, referenceBaseCount[sample.name]

class PassThrough():
    
    def __init__(self,name):
	self.things = []
	self.name = name

    def add(self, thing):
	self.things.append(thing)
	if thing == '--STOP-ITER--': raise ValueError

    def end(self, ): self.things.append('--STOP-ITER--')

    def __iter__(self,): return self

    def next(self,):
	import time

	if len(self.things) == 0: return '--continue--'

	thing = self.things[0]
	if len(self.things) > 1: self.things = self.things[1:]
	else: self.things = []
	if thing == '--STOP-ITER--': raise StopIteration
	return thing

def bedPerChromGenerator(bedfile):
    
    import gzip
    try:
        if bedfile.split('.')[-1] in ['gz','gzip']:bedfile = gzip.open(bedfile)
        else:bedfile = open(bedfile)
    except IOError: yield None

    lastChrom = None
    chormPassThrough = None
    for line in bedfile:
	chrom = line.split('\t')[0]
	if chrom != lastChrom and chrom!= None:
	    if chormPassThrough: chormPassThrough.end()
	    chormPassThrough = PassThrough(chrom)
	    if chormPassThrough: yield chormPassThrough
	if chormPassThrough: chormPassThrough.add(line)
	lastChrom = chrom

def parallelMakeRDOverChromPlots(sample): #this function need to call some other function and nice commenting to make sense, it works though is currently a mess.
    
    import sys
    import gzip
    import itertools
    import time
    import pysam
    import numpy as np
    import matplotlib.pyplot as plt
    import operator
    import os
    import multiprocessing
    import glob
    
    #
    # set colors for plots
    #
    patcolor = '#1975FF'
    doncolor = '#FF3399'
    mixcolor = '#CC9900'
    othercolor = "#e70000"

    #
    # get chromosome sizes
    #
    if not os.path.exists(sample.dataPath+'/'+sample.name+'.noDuplicates.bam'):
	AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Cannot fetch chromsizes from bam for sample '+sample.name+' the infile has not been created yet...\n')
	return sample
    bamfileName = sample.dataPath+'/'+sample.name+'.noDuplicates.bam'
    try: bamfile = pysam.Samfile(bamfileName, "rb")
    except IOError:
	AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Cannot fetch chromsizes from bam for sample '+sample.name+' the infile has not been created yet...\n')
	return sample
    except ValueError:
	AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Cannot fetch chromsizes from bam for sample '+sample.name+' the infile is not finished for processing...\n')
	return sample
    lengthOfChromByName = {}
    for chrom in bamfile.header['SQ']: lengthOfChromByName[chrom['SN']] = chrom['LN']
    
    #
    # make directory for plots
    #
    try: os.makedirs(sample.plotsPath)
    except OSError:pass

    #
    # Decide what infile to use
    #
    if AnalysisPipe.settings.mode == 'wgs':    bedfile = sample.dataPath+'/'+sample.name+'.bedtools.genomecov.nonPhysical.bedgraph.gz'
    elif AnalysisPipe.settings.mode == 'exome':
	bedfile = sample.dataPath+'/'+sample.name+'.bedtools.coverage.nonPhysical.bed.gz'
	#bedfile = sample.dataPath+'/'+sample.name+'.bedtools.genomecov.nonPhysical.bedgraph.gz'
    postitionTranslationDict = {chrom:{0:0}  for chrom in lengthOfChromByName}
    try: # open befile
        if bedfile.split('.')[-1] in ['gz','gzip']:bedfile = gzip.open(bedfile)
        else:bedfile = open(bedfile)
    except IOError: return sample

    #
    # set window size and initiate other "variables"
    #
    if AnalysisPipe.settings.mode=='wgs':slidingWindowSize = 10000
    if AnalysisPipe.settings.mode=='exome':slidingWindowSize = 10000
    lastChromosome = None
    referencePositions = []
    readDepths = []
    tmp=0
    bedFileRowNumber = 0
    
    #
    # Get positions of SNPs for identification and ADOrate estimation
    #
    if os.path.exists(sample.dataPath+'/identificationVariantsList.tsv'):
	#snps = {chrom:{'Patient':[],'Donor':[],'mix':[],'other':[]} for chrom in lengthOfChromByName}
	snps = {chrom:[] for chrom in lengthOfChromByName}
	with open(sample.dataPath+'/identificationVariantsList.tsv') as infile:
	    for line in infile:
		chrom,pos,rsid,classification,gt,gq,dp,ad = line.rstrip().split('\t')
		#snps[chrom][classification].append(int(pos))
		snps[chrom].append([int(pos),classification])
    else: snps = None
    potentialInFiles = list(glob.iglob( sample.dataPath+'/adoVariantsList.ref=*.tsv' ))
    eraseAdo = False
    if len(potentialInFiles) >= 1:
	#ado = {chrom:{'dropout':[],'correct':[]} for chrom in lengthOfChromByName}
	ado = {chrom:[] for chrom in lengthOfChromByName}
	if len(potentialInFiles) == 1: infile = potentialInFiles[0]
	if len(potentialInFiles) > 1:
	    data = open(sample.dataPath+'/sampleClassification.txt').read().split('\n')
	    refsample = data[1]
	    infile = sample.dataPath+'/adoVariantsList.ref='+refsample+'.tsv'
	    if data[0] == 'mix' or data[0] =='Unknown':  infile = potentialInFiles[0]; eraseAdo = True
	with open(infile) as infile:
	    for line in infile:
		chrom,pos,rsid,classification,gt,gq,dp,ad = line.rstrip().split('\t')
		if classification == 'NoData': continue
		try: ado[chrom].append([int(pos),classification])
		except KeyError: pass
    else: ado = None
    if eraseAdo:  ado = None

    #
    # Start parsiing throug bedfile
    #
    for line in bedfile:
	bedFileRowNumber += 1

	if line.split('\t')[0][0] == 'G': continue
	currentChromosome = line.split('\t')[0]

	if currentChromosome != lastChromosome:
	    #
	    # NEW CHROMOSOME initiate new variables make plots and summaries
	    #
	    if AnalysisPipe.settings.mode=='wgs':outputEvery = lengthOfChromByName[currentChromosome]/1000
	    if AnalysisPipe.settings.mode=='exome':outputEvery = lengthOfChromByName[currentChromosome]/(1000*100)#exome approx 1%
	    currentPosition = outputEvery
	    windowLower = currentPosition-slidingWindowSize/2
	    windowUpper = currentPosition+slidingWindowSize/2

	    # for all chroms that != None and have read depth data for the ref
	    if lastChromosome:
		if rdOverRef_x and rdOverRef_y:
		    
		    #
		    # Convert the position translation dictionary
		    #
		    invertedPostitionTranslationDict = {lastChromosome:{tmpRowNumber:tmpPosition for tmpPosition,tmpRowNumber in postitionTranslationDict[lastChromosome].iteritems()}}

		    sys.stderr.write(sample.name+' making plot chromosome '+lastChromosome+' ...\n')
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Generating rd over reference graph '+sample.name+'(pid='+str(os.getpid())+') chrom='+lastChromosome+'...\n')

		    #
		    # make boxplot
		    #
		    fig, ax = plt.subplots(1, sharex=True)
		    ax.set_title("Chromosome "+str(lastChromosome)+" RD boxplot")
		    #ax.plot(rdOverRef_x, rdOverRef_y, lw=1,color="green")
		    moreThanZero = [i for i in rdOverRef_y if i > 0]
		    plt.boxplot([rdOverRef_y,moreThanZero])
		    plt.savefig(sample.plotsPath+'/boxplotRD.'+sample.name+'.'+lastChromosome+'.pdf',dpi=50,bbox_inches="tight")
		    plt.savefig(sample.plotsPath+'/boxplotRD.'+sample.name+'.'+lastChromosome+'.png',dpi=50,bbox_inches="tight")
		    plt.close(fig)
		    
		    #
		    # plot the readdepth data
		    #
		    fig, axes = plt.subplots(1, sharex=True)
		    if type(axes) != list: axes = [axes]
		    fig.set_size_inches(30,3)
		    if AnalysisPipe.settings.mode=='wgs': axes[0].set_title("Chromosome "+str(lastChromosome)+" wgs read depth/coverage")
		    if AnalysisPipe.settings.mode=='exome': axes[0].set_title("Chromosome "+str(lastChromosome)+" concatenated exons read depth/coverage")
		    tmpCounter = 0
		    axes[tmpCounter].plot(rdOverRef_x, rdOverRef_y, lw=1,color="green")
		    axes[tmpCounter].fill_between(rdOverRef_x,0,rdOverRef_y, color="green",alpha=0.5)
		    axes[tmpCounter].set_xlabel(sample.name)

		    #
		    # Set X and Y scales
		    #
		    maxY = 0
		    maxY = max([maxY,max(rdOverRef_y)])
		    yscalemax = max([30,max([myround(maxY),2])])
		    yscalemax = max([30,np.percentile(moreThanZero, 75)*2])
		    if AnalysisPipe.settings.RDoverchromYscaleMax != 0: yscalemax = AnalysisPipe.settings.RDoverchromYscaleMax
		    axes[tmpCounter].set_ylim( 0, yscalemax);
		    if AnalysisPipe.settings.mode == 'wgs':   axes[tmpCounter].set_xlim( min(rdOverRef_x), lengthOfChromByName[lastChromosome])#max(rdOverRef_x) )
		    if AnalysisPipe.settings.mode == 'exome': axes[tmpCounter].set_xlim( min(rdOverRef_x), max(rdOverRef_x) )
		    
		    #
		    # Plot the identification and ADO estimation SNP positions
		    #
		    #snps = {lastChromosome:[[10479794,'Patient'],[38226767,'Patient'],[38226768,'Patient'],[38226769,'Patient'],[38226770,'Donor'],[90179536,'Donor'],[154026876,'Donor'],[186389971,'Patient'],[236140570,'Patient']]}
		    if snps:
			lastClass = None
			chunkPositions = []
			for pos,classification in snps[lastChromosome]:
			    if classification != lastClass:
				if chunkPositions and lastClass:
				    if AnalysisPipe.settings.mode=='exome':
					tmpChunkPositions = chunkPositions
					chunkPositions = []
					for tmpPos in tmpChunkPositions:
					    try: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos])
					    except KeyError:
						if tmpPos+1 in postitionTranslationDict[lastChromosome]: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos+1])
						elif tmpPos-1 in postitionTranslationDict[lastChromosome]: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos-1])
						else: AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# cannot fin position translation for '+sample.name+' chrom='+lastChromosome+' position='+thousandString(tmpPos)+'...\n')
				    #print sample.name, lastChromosome, lastClass, len(chunkPositions), [thousandString(i) for i in chunkPositions]
				    if   lastClass == 'Patient': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.95 for i in chunkPositions],'o-',color=patcolor)
				    elif lastClass == 'Donor':   axes[tmpCounter].plot(chunkPositions,[yscalemax*0.90 for i in chunkPositions],'o-',color=doncolor)
				    elif lastClass == 'mix':     axes[tmpCounter].plot(chunkPositions,[yscalemax*0.85 for i in chunkPositions],'o-',color=mixcolor)
				    elif lastClass == 'other':   axes[tmpCounter].plot(chunkPositions,[yscalemax*0.80 for i in chunkPositions],'o-',color=othercolor)
				chunkPositions = [pos]
				lastClass = classification
			    else:
				chunkPositions.append(pos)
			if chunkPositions and lastClass:
			    if AnalysisPipe.settings.mode=='exome':
				tmpChunkPositions = chunkPositions
				chunkPositions = []
				for tmpPos in tmpChunkPositions:
				    try: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos])
				    except KeyError:
					if tmpPos+1 in postitionTranslationDict[lastChromosome]: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos+1])
					elif tmpPos-1 in postitionTranslationDict[lastChromosome]: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos-1])
					else: AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# cannot fin position translation for '+sample.name+' chrom='+lastChromosome+' position='+thousandString(tmpPos)+'...\n')
			    if   lastClass == 'Patient': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.95 for i in chunkPositions],'o-',color=patcolor)
			    elif lastClass == 'Donor':   axes[tmpCounter].plot(chunkPositions,[yscalemax*0.90 for i in chunkPositions],'o-',color=doncolor)
			    elif lastClass == 'mix':     axes[tmpCounter].plot(chunkPositions,[yscalemax*0.85 for i in chunkPositions],'o-',color=mixcolor)
			    elif lastClass == 'other':   axes[tmpCounter].plot(chunkPositions,[yscalemax*0.80 for i in chunkPositions],'o-',color=othercolor)
			#if snps[lastChromosome]['Patient']: axes[tmpCounter].plot(snps[lastChromosome]['Patient'],[yscalemax*0.95 for i in snps[lastChromosome]['Patient']],'o',color=patcolor)
			#if snps[lastChromosome]['Donor']:   axes[tmpCounter].plot(snps[lastChromosome]['Donor'],  [yscalemax*0.90 for i in snps[lastChromosome]['Donor']],  'o',color=doncolor)
			#if snps[lastChromosome]['mix']:     axes[tmpCounter].plot(snps[lastChromosome]['mix'],    [yscalemax*0.85 for i in snps[lastChromosome]['mix']],    'o',color=mixcolor)
			#if snps[lastChromosome]['other']:   axes[tmpCounter].plot(snps[lastChromosome]['other'],  [yscalemax*0.80 for i in snps[lastChromosome]['other']],  'o',color=othercolor)
		    if ado:
			lastClass = None
			chunkPositions = []
			#print sample.name, ado[lastChromosome]
			for pos,classification in ado[lastChromosome]:
			    if classification != lastClass:
				#print sample.name, lastChromosome, lastClass, len(chunkPositions), [thousandString(i) for i in chunkPositions]
				if chunkPositions and lastClass:
				    #print sample.name, lastClass, chunkPositions,'BEFORE'
				    if AnalysisPipe.settings.mode=='exome':
					tmpChunkPositions = chunkPositions
					chunkPositions = []
					for tmpPos in tmpChunkPositions:
					    try: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos])
					    except KeyError:AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# cannot fin position translation for '+sample.name+' chrom='+lastChromosome+' position='+thousandString(tmpPos)+'...\n')
				    #print sample.name, lastClass, chunkPositions
				    if   lastClass == 'correct': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.75 for i in chunkPositions],'o-',color='yellow')
				    elif lastClass == 'dropout': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.70 for i in chunkPositions],'o-',color='orangered')
				chunkPositions = [pos]
				lastClass = classification
			    else:
				chunkPositions.append(pos)
			if chunkPositions and lastClass:
			    if AnalysisPipe.settings.mode=='exome':
				tmpChunkPositions = chunkPositions
				chunkPositions = []
				for tmpPos in tmpChunkPositions:
				    try: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos])
				    except KeyError:AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# cannot fin position translation for '+sample.name+' chrom='+lastChromosome+' position='+thousandString(tmpPos)+'...\n')
			    if   lastClass == 'correct': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.75 for i in chunkPositions],'o-',color='yellow')
			    elif lastClass == 'dropout': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.70 for i in chunkPositions],'o-',color='orangered')
#			if ado[lastChromosome]['correct']: axes[tmpCounter].plot(ado[lastChromosome]['correct'],[yscalemax*0.75 for i in ado[lastChromosome]['correct']],'o--',color='yellow')
#			if ado[lastChromosome]['dropout']: axes[tmpCounter].plot(ado[lastChromosome]['dropout'],[yscalemax*0.70 for i in ado[lastChromosome]['dropout']],'o--',color='orangered')
#		    tmpCounter+=1
		    
		    #
		    # Set X-ticks lables
		    #
		    if AnalysisPipe.settings.mode == 'wgs':
			a=axes[0].get_xticks().tolist()
			a = [thousandString(tmpPos) for tmpPos in a]
			axes[0].set_xticklabels(a)
		    if AnalysisPipe.settings.mode == 'exome':
			a=axes[0].get_xticks().tolist()
			lasta = [a[-1]]
			a = [thousandString(invertedPostitionTranslationDict[lastChromosome][int(tmpPos)]) for tmpPos in a[:-1]]
			axes[0].set_xticklabels(a)
		    
		    #
		    # Save the figure to png and pdf files
		    #
		    plt.savefig(sample.plotsPath+'/readDepth.'+sample.name+'.'+lastChromosome+'.pdf',dpi=50,bbox_inches="tight")
		    plt.savefig(sample.plotsPath+'/readDepth.'+sample.name+'.'+lastChromosome+'.png',dpi=50,bbox_inches="tight")
		    plt.close(fig)

	    sys.stderr.write(sample.name+' start loading data chromosome '+currentChromosome+' ...\n')

	    #
	    # Reset values for next chromosome
	    #
	    #print 'value reset after chrom',lastChromosome
	    currentPosition = outputEvery
	    windowLower = currentPosition-slidingWindowSize/2
	    windowUpper = currentPosition+slidingWindowSize/2
	    rdOverRef_y = []
	    rdOverRef_x = []
	    rowBuffer = []
	    lastChromosome = currentChromosome
	    if AnalysisPipe.settings.mode == 'wgs':
		lineChrom,linePosFrom,linePosTo,lineDepth = line.rstrip().split('\t')
		rowBuffer.append([lineChrom,0,int(linePosFrom),0])
	    if AnalysisPipe.settings.mode == 'exome':
		bedFileRowNumber=1
		lineChrom,fetureStart,featureEnd,strand,featureName,feturePos,lineDepth = line.rstrip().split('\t')
		linePosFrom = bedFileRowNumber
		linePosTo = bedFileRowNumber+1
		#if bedFileRowNumber in [1587054, 159127, 1621117]: print bedFileRowNumber,'=',line.rstrip(),'==>',int(fetureStart)+int(feturePos)-1
		#if int(fetureStart)+int(feturePos)-1 in [26604522, 26753942, 27238150]: print '#################################WTF', bedFileRowNumber,'=',line.rstrip(),'==>',int(fetureStart)+int(feturePos)-1
		#if int(fetureStart)+int(feturePos)-1 in postitionTranslationDict[currentChromosome]: print 'OverWrite============>',int(fetureStart)+int(feturePos)-1,postitionTranslationDict[currentChromosome][int(fetureStart)+int(feturePos)-1],bedFileRowNumber
		postitionTranslationDict[currentChromosome][int(fetureStart)+int(feturePos)-1] = bedFileRowNumber
	    #if tmp == 1:break
	    tmp +=1

	#
	# read bedfile line and add to buffer
	#
	if AnalysisPipe.settings.mode == 'wgs':
	    lineChrom,linePosFrom,linePosTo,lineDepth = line.rstrip().split('\t')
	if AnalysisPipe.settings.mode == 'exome':
	    lineChrom,fetureStart,featureEnd,strand,featureName,feturePos,lineDepth = line.rstrip().split('\t')
	    linePosFrom = bedFileRowNumber
	    linePosTo = bedFileRowNumber+1
	    #if bedFileRowNumber in [1587054, 159127, 1621117]: print 'ROW',bedFileRowNumber,'=',line.rstrip(),'==>',int(fetureStart)+int(feturePos)-1
	    #if int(fetureStart)+int(feturePos)-1 in [26604522, 26753942, 27238150]: print 'POS', bedFileRowNumber,'=',line.rstrip(),'==>',int(fetureStart)+int(feturePos)-1
	    #if int(fetureStart)+int(feturePos)-1 in postitionTranslationDict[currentChromosome]: print 'OverWrite============>',bedFileRowNumber,'=',line.rstrip(),'==>',int(fetureStart)+int(feturePos)-1,' already in @',postitionTranslationDict[currentChromosome][int(fetureStart)+int(feturePos)-1]
	    postitionTranslationDict[currentChromosome][int(fetureStart)+int(feturePos)-1] = bedFileRowNumber
	rowBuffer.append([lineChrom,int(linePosFrom),int(linePosTo),int(lineDepth)])

	#
	# when the buffer is large enough; calculate window average, remove from buffer start and move to next window
	#
	if windowUpper <= linePosTo and len(rowBuffer)>=10:

	    while windowUpper <= rowBuffer[-2][2]:
		depthWindow = []
		tmpBuffer = []

		for row in rowBuffer:

		    chrom,posFrom,posTo,depth = row

		    if posTo < windowLower: continue

		    elif posTo >= windowLower:

			tmpBuffer.append(row)
			
			if   posTo <  windowUpper:
			    for i in xrange(max(windowLower,posFrom),posTo): depthWindow.append(depth)

			elif posTo >= windowUpper and posFrom < windowUpper:
			    for i in xrange(max(windowLower,posFrom),windowUpper): depthWindow.append(depth)

			else: pass

		    else: print 'Not possible!!'

		rowBuffer = tmpBuffer

		rdOverRef_x.append( currentPosition )
		rdOverRef_y.append( float(sum(depthWindow) / float(len(depthWindow)) ))
		currentPosition += outputEvery
		windowLower = currentPosition-slidingWindowSize/2
		windowUpper = currentPosition+slidingWindowSize/2
	else: pass
##################################################################################
## TO NOT LOOSE LAST CHROMOSOME VERY UGLY THO SHOULD WORK NEED TO BE FIXED LATER #
    currentChromosome = 'mockCHR'
    if currentChromosome != lastChromosome:
	    #
	    # NEW CHROMOSOME initiate new variables make plots and summaries
	    #
	    #if AnalysisPipe.settings.mode=='wgs':outputEvery = lengthOfChromByName[currentChromosome]/1000
	    #if AnalysisPipe.settings.mode=='exome':outputEvery = lengthOfChromByName[currentChromosome]/(1000*100)#exome approx 1%
	    #currentPosition = outputEvery
	    #windowLower = currentPosition-slidingWindowSize/2
	    #windowUpper = currentPosition+slidingWindowSize/2

	    # for all chroms that != None and have read depth data for the ref
	    if lastChromosome:
		if rdOverRef_x and rdOverRef_y:
		    
		    #
		    # Convert the position translation dictionary
		    #
		    invertedPostitionTranslationDict = {lastChromosome:{tmpRowNumber:tmpPosition for tmpPosition,tmpRowNumber in postitionTranslationDict[lastChromosome].iteritems()}}

		    sys.stderr.write(sample.name+' making plot chromosome '+lastChromosome+' ...\n')
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Generating rd over reference graph '+sample.name+'(pid='+str(os.getpid())+') chrom='+lastChromosome+'...\n')

		    #
		    # make boxplot
		    #
		    fig, ax = plt.subplots(1, sharex=True)
		    ax.set_title("Chromosome "+str(lastChromosome)+" RD boxplot")
		    #ax.plot(rdOverRef_x, rdOverRef_y, lw=1,color="green")
		    moreThanZero = [i for i in rdOverRef_y if i > 0]
		    plt.boxplot([rdOverRef_y,moreThanZero])
		    plt.savefig(sample.plotsPath+'/boxplotRD.'+sample.name+'.'+lastChromosome+'.pdf',dpi=50,bbox_inches="tight")
		    plt.savefig(sample.plotsPath+'/boxplotRD.'+sample.name+'.'+lastChromosome+'.png',dpi=50,bbox_inches="tight")
		    plt.close(fig)
		    
		    #
		    # plot the readdepth data
		    #
		    fig, axes = plt.subplots(1, sharex=True)
		    if type(axes) != list: axes = [axes]
		    fig.set_size_inches(30,3)
		    if AnalysisPipe.settings.mode=='wgs': axes[0].set_title("Chromosome "+str(lastChromosome)+" wgs read depth/coverage")
		    if AnalysisPipe.settings.mode=='exome': axes[0].set_title("Chromosome "+str(lastChromosome)+" concatenated exons read depth/coverage")
		    tmpCounter = 0
		    axes[tmpCounter].plot(rdOverRef_x, rdOverRef_y, lw=1,color="green")
		    axes[tmpCounter].fill_between(rdOverRef_x,0,rdOverRef_y, color="green",alpha=0.5)
		    axes[tmpCounter].set_xlabel(sample.name)

		    #
		    # Set X and Y scales
		    #
		    maxY = 0
		    maxY = max([maxY,max(rdOverRef_y)])
		    yscalemax = max([30,max([myround(maxY),2])])
		    yscalemax = max([30,np.percentile(moreThanZero, 75)*2])
		    axes[tmpCounter].set_ylim( 0, yscalemax);
		    if AnalysisPipe.settings.mode == 'wgs':   axes[tmpCounter].set_xlim( min(rdOverRef_x), lengthOfChromByName[lastChromosome])#max(rdOverRef_x) )
		    if AnalysisPipe.settings.mode == 'exome': axes[tmpCounter].set_xlim( min(rdOverRef_x), max(rdOverRef_x) )
		    
		    #
		    # Plot the identification and ADO estimation SNP positions
		    #
		    #snps = {lastChromosome:[[10479794,'Patient'],[38226767,'Patient'],[38226768,'Patient'],[38226769,'Patient'],[38226770,'Donor'],[90179536,'Donor'],[154026876,'Donor'],[186389971,'Patient'],[236140570,'Patient']]}
		    if snps:
			lastClass = None
			chunkPositions = []
			for pos,classification in snps[lastChromosome]:
			    if classification != lastClass:
				if chunkPositions and lastClass:
				    if AnalysisPipe.settings.mode=='exome':
					tmpChunkPositions = chunkPositions
					chunkPositions = []
					for tmpPos in tmpChunkPositions:
					    try: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos])
					    except KeyError:
						if tmpPos+1 in postitionTranslationDict[lastChromosome]: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos+1])
						elif tmpPos-1 in postitionTranslationDict[lastChromosome]: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos-1])
						else: AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# cannot fin position translation for '+sample.name+' chrom='+lastChromosome+' position='+thousandString(tmpPos)+'...\n')
				    #print sample.name, lastChromosome, lastClass, len(chunkPositions), [thousandString(i) for i in chunkPositions]
				    if   lastClass == 'Patient': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.95 for i in chunkPositions],'o-',color=patcolor)
				    elif lastClass == 'Donor':   axes[tmpCounter].plot(chunkPositions,[yscalemax*0.90 for i in chunkPositions],'o-',color=doncolor)
				    elif lastClass == 'mix':     axes[tmpCounter].plot(chunkPositions,[yscalemax*0.85 for i in chunkPositions],'o-',color=mixcolor)
				    elif lastClass == 'other':   axes[tmpCounter].plot(chunkPositions,[yscalemax*0.80 for i in chunkPositions],'o-',color=othercolor)
				chunkPositions = [pos]
				lastClass = classification
			    else:
				chunkPositions.append(pos)
			if chunkPositions and lastClass:
			    if AnalysisPipe.settings.mode=='exome':
				tmpChunkPositions = chunkPositions
				chunkPositions = []
				for tmpPos in tmpChunkPositions:
				    try: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos])
				    except KeyError:
					if tmpPos+1 in postitionTranslationDict[lastChromosome]: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos+1])
					elif tmpPos-1 in postitionTranslationDict[lastChromosome]: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos-1])
					else: AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# cannot fin position translation for '+sample.name+' chrom='+lastChromosome+' position='+thousandString(tmpPos)+'...\n')
			    if   lastClass == 'Patient': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.95 for i in chunkPositions],'o-',color=patcolor)
			    elif lastClass == 'Donor':   axes[tmpCounter].plot(chunkPositions,[yscalemax*0.90 for i in chunkPositions],'o-',color=doncolor)
			    elif lastClass == 'mix':     axes[tmpCounter].plot(chunkPositions,[yscalemax*0.85 for i in chunkPositions],'o-',color=mixcolor)
			    elif lastClass == 'other':   axes[tmpCounter].plot(chunkPositions,[yscalemax*0.80 for i in chunkPositions],'o-',color=othercolor)
			#if snps[lastChromosome]['Patient']: axes[tmpCounter].plot(snps[lastChromosome]['Patient'],[yscalemax*0.95 for i in snps[lastChromosome]['Patient']],'o',color=patcolor)
			#if snps[lastChromosome]['Donor']:   axes[tmpCounter].plot(snps[lastChromosome]['Donor'],  [yscalemax*0.90 for i in snps[lastChromosome]['Donor']],  'o',color=doncolor)
			#if snps[lastChromosome]['mix']:     axes[tmpCounter].plot(snps[lastChromosome]['mix'],    [yscalemax*0.85 for i in snps[lastChromosome]['mix']],    'o',color=mixcolor)
			#if snps[lastChromosome]['other']:   axes[tmpCounter].plot(snps[lastChromosome]['other'],  [yscalemax*0.80 for i in snps[lastChromosome]['other']],  'o',color=othercolor)
		    if ado:
			lastClass = None
			chunkPositions = []
			#print sample.name, ado[lastChromosome]
			for pos,classification in ado[lastChromosome]:
			    if classification != lastClass:
				#print sample.name, lastChromosome, lastClass, len(chunkPositions), [thousandString(i) for i in chunkPositions]
				if chunkPositions and lastClass:
				    #print sample.name, lastClass, chunkPositions,'BEFORE'
				    if AnalysisPipe.settings.mode=='exome':
					tmpChunkPositions = chunkPositions
					chunkPositions = []
					for tmpPos in tmpChunkPositions:
					    try: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos])
					    except KeyError:AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# cannot fin position translation for '+sample.name+' chrom='+lastChromosome+' position='+thousandString(tmpPos)+'...\n')
				    #print sample.name, lastClass, chunkPositions
				    if   lastClass == 'correct': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.75 for i in chunkPositions],'o-',color='yellow')
				    elif lastClass == 'dropout': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.70 for i in chunkPositions],'o-',color='orangered')
				chunkPositions = [pos]
				lastClass = classification
			    else:
				chunkPositions.append(pos)
			if chunkPositions and lastClass:
			    if AnalysisPipe.settings.mode=='exome':
				tmpChunkPositions = chunkPositions
				chunkPositions = []
				for tmpPos in tmpChunkPositions:
				    try: chunkPositions.append(postitionTranslationDict[lastChromosome][tmpPos])
				    except KeyError:AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# cannot fin position translation for '+sample.name+' chrom='+lastChromosome+' position='+thousandString(tmpPos)+'...\n')
			    if   lastClass == 'correct': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.75 for i in chunkPositions],'o-',color='yellow')
			    elif lastClass == 'dropout': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.70 for i in chunkPositions],'o-',color='orangered')
#			if ado[lastChromosome]['correct']: axes[tmpCounter].plot(ado[lastChromosome]['correct'],[yscalemax*0.75 for i in ado[lastChromosome]['correct']],'o--',color='yellow')
#			if ado[lastChromosome]['dropout']: axes[tmpCounter].plot(ado[lastChromosome]['dropout'],[yscalemax*0.70 for i in ado[lastChromosome]['dropout']],'o--',color='orangered')
#		    tmpCounter+=1
		    
		    #
		    # Set X-ticks lables
		    #
		    if AnalysisPipe.settings.mode == 'wgs':
			a=axes[0].get_xticks().tolist()
			a = [thousandString(tmpPos) for tmpPos in a]
			axes[0].set_xticklabels(a)
		    if AnalysisPipe.settings.mode == 'exome':
			a=axes[0].get_xticks().tolist()
			lasta = [a[-1]]
			a = [thousandString(invertedPostitionTranslationDict[lastChromosome][int(tmpPos)]) for tmpPos in a[:-1]]
			axes[0].set_xticklabels(a)
		    
		    #
		    # Save the figure to png and pdf files
		    #
		    plt.savefig(sample.plotsPath+'/readDepth.'+sample.name+'.'+lastChromosome+'.pdf',dpi=50,bbox_inches="tight")
		    plt.savefig(sample.plotsPath+'/readDepth.'+sample.name+'.'+lastChromosome+'.png',dpi=50,bbox_inches="tight")
		    plt.close(fig)

	    sys.stderr.write(sample.name+' start loading data chromosome '+currentChromosome+' ...\n')

	    #
	    # Reset values for next chromosome
	    #
	    #print 'value reset after chrom',lastChromosome
	#    currentPosition = outputEvery
	#    windowLower = currentPosition-slidingWindowSize/2
	#    windowUpper = currentPosition+slidingWindowSize/2
	#    rdOverRef_y = []
	#    rdOverRef_x = []
	#    rowBuffer = []
	#    lastChromosome = currentChromosome
	#    if AnalysisPipe.settings.mode == 'wgs':
	#	lineChrom,linePosFrom,linePosTo,lineDepth = line.rstrip().split('\t')
	#	rowBuffer.append([lineChrom,0,int(linePosFrom),0])
	#    if AnalysisPipe.settings.mode == 'exome':
	#	bedFileRowNumber=1
	#	lineChrom,fetureStart,featureEnd,strand,featureName,feturePos,lineDepth = line.rstrip().split('\t')
	#	linePosFrom = bedFileRowNumber
	#	linePosTo = bedFileRowNumber+1
	#	#if bedFileRowNumber in [1587054, 159127, 1621117]: print bedFileRowNumber,'=',line.rstrip(),'==>',int(fetureStart)+int(feturePos)-1
	#	#if int(fetureStart)+int(feturePos)-1 in [26604522, 26753942, 27238150]: print '#################################WTF', bedFileRowNumber,'=',line.rstrip(),'==>',int(fetureStart)+int(feturePos)-1
	#	#if int(fetureStart)+int(feturePos)-1 in postitionTranslationDict[currentChromosome]: print 'OverWrite============>',int(fetureStart)+int(feturePos)-1,postitionTranslationDict[currentChromosome][int(fetureStart)+int(feturePos)-1],bedFileRowNumber
	#	postitionTranslationDict[currentChromosome][int(fetureStart)+int(feturePos)-1] = bedFileRowNumber
	#    #if tmp == 1:break
	    tmp +=1
##################################################################################

    #AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# joining to main, '+sample.name+' ...\n')    
    return sample

def graphForeachSample(sample):

    import time
    import operator
    import shutil
    import os
    import sys
    
    try: uppmax_temp = os.environ["SNIC_TMP"]
    except:
	uppmax_temp = None
	print 'Not on uppmax no temporary directory'

    #LOADING
    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading data for sample '+sample.name+' ...\n')

    
    def bedtoolsversion(uppmax_temp):
    
	perBaseCoverages = {}
	depthPerPosition = {}
	referenceBaseCount = {}
	perBaseCoverages[sample.name] = {}
	depthPerPosition[sample.name] = {}
	referenceBaseCount[sample.name] = 0

	if not os.path.exists(sample.dataPath+'/'+sample.name+'.bedtools.coverage.bed'):
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping coverage stats for sample '+sample.name+' the infile has not been created yet...\n')
	    return False
    
	if uppmax_temp:
	    try:os.mkdir(uppmax_temp+'/fnuttglugg_TMP')
	    except OSError:pass
	    if not os.path.exists(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed'):
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Copying '+sample.dataPath+'/'+sample.name+'.bedtools.coverage.bed to temp location for faster reading from disk, '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed \n')
		shutil.copy(sample.dataPath+'/'+sample.name+'.bedtools.coverage.bed',uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed')
	    else:
		print 'WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed skipping copy!!'
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed skipping copy!!\n')
	    bedtoolsFile  = open(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed')
	else:bedtoolsFile = open(sample.dataPath+'/'+sample.name+'.bedtools.coverage.bed')
    
	lastChrom = None
	referenceBaseCountChrom = {}
	counter = 0
	for line in bedtoolsFile:
    
	    line = line.rstrip().split('\t')
	    referenceBaseCount[sample.name] += 1
	    try: referenceBaseCountChrom[line[0]] += 1
	    except KeyError:referenceBaseCountChrom[line[0]] = 1
    
	    try:            perBaseCoverages[sample.name][int(line[-1])] += 1
	    except KeyError:perBaseCoverages[sample.name][int(line[-1])] = 1
    
	    #print line[0], int(line[1]) + int(line[5])-1, int(line[-1])
	    try:
		depthPerPosition[sample.name][line[0]]['refPos'].append(int(line[1])+int(line[5])-1)
		depthPerPosition[sample.name][line[0]]['rd'].append(int(line[-1])) 
	    except KeyError:
		depthPerPosition[sample.name][line[0]] = {'refPos':[int(line[1])+int(line[5])-1],'rd':[int(line[-1])]}
    
	    if lastChrom != line[0]:
		sys.stderr.write(sample.name+' now starting to read chrom '+line[0]+' ...\n')
		lastChrom = line[0]
	    counter += 1
	    #if counter == 1e6: break
    
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Data in subprocess memeory, '+sample.name+' ...\n')
	if uppmax_temp: os.remove(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed')
	return depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount

    def bedtoolsversion2(uppmax_temp):
    
	import re
	
	perBaseCoverages = {}
	perBaseCoverages[sample.name] = {0:1}
	depthPerPosition = {}
	depthPerPosition[sample.name] = {}
	referenceBaseCount = {}
	referenceBaseCount[sample.name] = 1
	lastChrom = None
	referenceBaseCountChrom = {}
	counter = 0

	if not os.path.exists(sample.dataPath+'/'+sample.name+'.bedtools.genomecov.nonPhysical.histogram'):
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping coverage stats for sample '+sample.name+' the infile has not been created yet...\n')
	    return depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount

	bedtoolsFile = open(sample.dataPath+'/'+sample.name+'.bedtools.genomecov.nonPhysical.histogram')
	bedtoolsFile2 = open(sample.dataPath+'/'+sample.name+'.bedtools.genomecov.nonPhysical.bedgraph')

	if not bedtoolsFile or not bedtoolsFile2: return depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount

	for line in bedtoolsFile:
	    
	    line = line.rstrip().split('\t')
	    
	    if not re.match('genome',line[0]):
		continue
	    
	    #genome	0	3097784897	3101804739	0.998704
	    referenceBaseCount[sample.name] = int(line[3])
	    perBaseCoverages[sample.name][int(line[1])] = int(line[2])
	bedtoolsFile.close()

	#for line in bedtoolsFile2:
	#    line = line.rstrip().split('\t')
	    #1       0       534314  0
	    #1       534314  534335  1
	    #1       534335  567070  0
	    #1       567070  567153  1
	    #1       567153  794130  0
	    #1       794130  794131  1
	    #1       794131  794132  2
	    #1       794132  794133  5
	    #1       794133  794148  6

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Data in subprocess memeory, '+sample.name+' ...\n')

	return depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount

    def samtoolsversion(uppmax_temp):

	import pysam

	if not os.path.exists(sample.dataPath+'/'+sample.name+'.noDuplicates.bam'):
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping iSize stats for sample '+sample.name+' the infile has not been created yet...\n')
	    return sample, isizes
    
	if uppmax_temp:
	    try:os.mkdir(uppmax_temp+'/fnuttglugg_TMP')
	    except OSError:pass
	    if not os.path.exists(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'):
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Copying '+sample.dataPath+'/'+sample.name+'.noDuplicates.bam'+' to temp location for faster reading from disk, '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'+' \n')
		shutil.copy(sample.dataPath+'/'+sample.name+'.noDuplicates.bam',uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam')
	    else:
		print 'WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'+' skipping copy!!'
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'+' skipping copy!!\n')
	    bamfileName  = uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'
	else:bamfileName = sample.dataPath+'/'+sample.name+'.noDuplicates.bam'
	if uppmax_temp:
	    try:os.mkdir(uppmax_temp+'/fnuttglugg_TMP')
	    except OSError:pass
	    if not os.path.exists(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'):
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Copying '+sample.dataPath+'/'+sample.name+'.noDuplicates.bai'+' to temp location for faster reading from disk, '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'+' \n')
		try: shutil.copy(sample.dataPath+'/'+sample.name+'.noDuplicates.bai',uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai')
		except IOError as e: pass
	    else:
		print 'WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'+' skipping copy!!'
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'+' skipping copy!!\n')
    
	# now start do work here	
	# open file
	try: bamfile = pysam.Samfile(bamfileName, "rb")
	except IOError:
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping coverage plot for sample '+sample.name+' the infile has not been created yet...\n')
	    return sample, isizes
	except ValueError:
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping coverage plot for sample '+sample.name+' the infile is not finished for processing...\n')
	    return sample, isizes

	perBaseCoverages = {}
	depthPerPosition = {}
	referenceBaseCount = {}
	perBaseCoverages[sample.name] = {0:0}
	depthPerPosition[sample.name] = {}
	referenceBaseCount[sample.name] = 0
	lastChrom = None
	referenceBaseCountChrom = {}
	counter = 0
	lastPos = 0

	lengthOfChromByName = {}
	for chrom in bamfile.header['SQ']: lengthOfChromByName[chrom['SN']] = chrom['LN']

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading coverage data (wgs) for sample '+sample.name+'...\n')
	try:
	    for column in bamfile.pileup():

		chromName = bamfile.getrname(column.tid)

		if lastPos != column.pos-1:
		    for i in range(lastPos,column.pos):
			referenceBaseCount[sample.name] += 1
			try: referenceBaseCountChrom[chromName] += 1
			except KeyError:referenceBaseCountChrom[chromName] = 1
			perBaseCoverages[sample.name][0] += 1
			try:
			    depthPerPosition[sample.name][chromName]['refPos'].append(i)
			    depthPerPosition[sample.name][chromName]['rd'].append(0) 
			except KeyError:
			    depthPerPosition[sample.name][chromName] = {'refPos':[i],'rd':[0]}

		referenceBaseCount[sample.name] += 1
		try: referenceBaseCountChrom[chromName] += 1
		except KeyError:referenceBaseCountChrom[chromName] = 1

		try:            perBaseCoverages[sample.name][int(column.n)] += 1
		except KeyError:perBaseCoverages[sample.name][int(column.n)] = 1

		try:
		    depthPerPosition[sample.name][chromName]['refPos'].append(int(column.pos))
		    depthPerPosition[sample.name][chromName]['rd'].append(int(column.n)) 
		except KeyError:
		    depthPerPosition[sample.name][chromName] = {'refPos':[int(column.pos)],'rd':[int(column.n)]}

		if lastChrom != chromName:
		    if lastPos != lengthOfChromByName[chromName]: #count up to end here
			for i in range(lastPos,lengthOfChromByName[chromName]):
			    referenceBaseCount[sample.name] += 1
			    try: referenceBaseCountChrom[chromName] += 1
			    except KeyError:referenceBaseCountChrom[chromName] = 1
			    perBaseCoverages[sample.name][0] += 1
			    try:
				depthPerPosition[sample.name][chromName]['refPos'].append(i)
				depthPerPosition[sample.name][chromName]['rd'].append(0) 
			    except KeyError:
				depthPerPosition[sample.name][chromName] = {'refPos':[i],'rd':[0]}
		    sys.stderr.write(sample.name+' now starting to read chrom '+str(chromName)+' ...\n')
		    lastChrom = chromName
		counter += 1
		lastPos = column.pos
		#if counter == 1e3: break

	except ValueError as e:
	    if e == 'fetch called on bamfile without index':
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping coverage plot for sample '+sample.name+' the bam index is not present...\n')
		sys.stderr.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping coverage plot for sample '+sample.name+' the bam index is not present...\n')
		return sample, isizes
    
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Data in subprocess memeory, '+sample.name+' ...\n')
	return depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount

#    if AnalysisPipe.settings.mode == 'exome':
#	depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount = bedtoolsversion(uppmax_temp)
#	print perBaseCoverages,referenceBaseCount
#	perBaseCoverages = {}
#	perBaseCoverages[sample.name] = {}
#	with open(sample.dataPath+'/'+sample.name+'.bedtools.exomecoverage.histogram') as infile: data = infile.read()
#	lines = data.split('\n')
#	for line in lines:
#	    if line[:3] == 'all':
#		line=line.split('\t')
#		perBaseCoverages[sample.name][line[1]] = line[2]
#		referenceBaseCount[sample.name] = line[3]
#	print perBaseCoverages,referenceBaseCount
#    elif AnalysisPipe.settings.mode == 'wgs': depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount = bedtoolsversion(uppmax_temp)
    depthPerPosition,perBaseCoverages,referenceBaseCountChrom,referenceBaseCount = bedtoolsversion(uppmax_temp)

    # RD Over reference
    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# RD over ref processing, '+sample.name+' ...\n')
    outputEvery = 100
    slidingWindowSize = outputEvery*100
    chromDist = {}
    for chrom in depthPerPosition[sample.name].keys():
	referencePositions = depthPerPosition[sample.name][chrom]['refPos']
	readDepths = depthPerPosition[sample.name][chrom]['rd']
	assert len(readDepths) == len(referencePositions)
	#print referencePositions[:150]
	#print readDepths[:150]
	#windowAverage = [sum(readDepths[i:i+slidingWindowSize])/float(slidingWindowSize) for i in range(len(readDepths))]
	#rdOverRef_x = concatednatedRefPos
	#rdOverRef_y = windowAverage
	rdOverRef_y = []
	rdOverRef_x = []
	for i in range(len(referencePositions)):
	    if i%outputEvery == 0:
		#rdOverRef_x.append( referencePositions[i] )
		rdOverRef_x.append( i )
		rdOverRef_y.append( float(sum(readDepths[i:i+slidingWindowSize])) / float(slidingWindowSize) )
	#rdOverRef_x = range(len(rdOverRef_x))

	chromDist[chrom] = {'x':rdOverRef_x, 'y':rdOverRef_y}

    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# joining to main, '+sample.name+' ...\n')
    return sample, chromDist

def reportParallel(sample):
    sample.getStats()
    return sample

def sorted_nicely( l ): # funtion "stolen from the internet"
    import re
    """ Sort the given iterable in the way that humans expect.""" 
    convert = lambda text: int(text) if text.isdigit() else text 
    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] 
    return sorted(l, key = alphanum_key)

class AnalysisPipe(object):

    def __init__(self,):
	
	import sys
	import os
	import time
	
	AnalysisPipe.masterPid = os.getpid()

	try:
	    path = sys.argv[1]
	    path = os.path.abspath(path)
	    if path[-1] == '/': path = path[:-1]
	    AnalysisPipe.path = path
	    AnalysisPipe.programPath = os.path.abspath(sys.argv[0])
	    if os.path.islink(AnalysisPipe.programPath ): AnalysisPipe.programPath = os.readlink(AnalysisPipe.programPath)
	    AnalysisPipe.scriptPath    = '/'+'/'.join(AnalysisPipe.programPath.split('/')[:-1])
	    AnalysisPipe.referencePath = '/'+'/'.join(AnalysisPipe.programPath.split('/')[:-2])+'/references'
	except IndexError:
	    sys.stderr.write('#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Please supply a path on your commandline (currently: "'+' '.join(sys.argv)+'")\n')
	    sys.exit(1)

	AnalysisPipe.bowtie2Reference = '~/singleFatCellExomeAnalysis/references/GATKbundle/human_g1k_v37.fasta'
	AnalysisPipe.picardLocation = '~/bin/picard-tools-1.114'
	AnalysisPipe.gatkLocation = '~/singleFatCellExomeAnalysis/bin/GenomeAnalysisTK-3.1-1/GenomeAnalysisTK.jar'
	AnalysisPipe.gatkBundleLocation = '~/singleFatCellExomeAnalysis/references/GATKbundle/'
	AnalysisPipe.picardLocation = '/sw/apps/bioinfo/picard/1.114/milou/'
	AnalysisPipe.gatkLocation = '/sw/apps/bioinfo/GATK/3.1.1/GenomeAnalysisTK.jar'

	self.openLogfileConnection()
	
	return None

    def getDataBase(self,):

	import os
	import time

	AnalysisPipe.database = Database(self.path+'/data.db')
	if not os.path.exists(AnalysisPipe.database.path): AnalysisPipe.database.create()
	
	AnalysisPipe.settings = Settings()
	AnalysisPipe.settings.setDefaults()
	AnalysisPipe.settings.loadFromDb()

    def copyReport(self,):

	import shutil
	import os
	import sys
	import re

	try: destination = sys.argv[-1]
	except IndexError: pass

	if os.path.exists(destination):
	    if re.match('[Yy]([eE][sS])?',raw_input('The destination folder '+destination+' already excists do you want to delete it?\n')): shutil.rmtree(destination)

	try: os.mkdir(destination)
	except OSError: pass

	shutil.copy(self.path+'/report.htm',destination+'/report.htm')
	if os.path.exists(AnalysisPipe.path+'/graphics'): shutil.copytree(AnalysisPipe.path+'/graphics', destination+'/graphics')
	
	samplesById={sample.id:sample for sample in AnalysisPipe.database.getSamples()}
	
	os.mkdir(destination+'/samples')
	doneIds = []
	for filePairId,readCount,fastq1,fastq2,sampleId in AnalysisPipe.database.getFastqs():
	    try:
		if sampleId not in doneIds and sampleId in samplesById:
		    sys.stderr.write('creating '+destination+'/samples/'+samplesById[int(sampleId)].name+'\n')
		    os.mkdir(destination+'/samples/'+samplesById[int(sampleId)].name)
		    if os.path.exists(samplesById[int(sampleId)].fastqcPath): shutil.copytree(samplesById[int(sampleId)].fastqcPath, destination+'/samples/'+samplesById[int(sampleId)].name+'/fastQC')
		    if os.path.exists(samplesById[int(sampleId)].plotsPath ): shutil.copytree(samplesById[int(sampleId)].plotsPath, destination+'/samples/'+samplesById[int(sampleId)].name+'/plots')
		doneIds.append(sampleId)
	    except OSError: pass

    def run(self, ):
	
	import time
	import sys
	import socket
	
	try: self.action = sys.argv[2]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Please supply an action on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	
	self.getDataBase()
	
	import os
	
	AnalysisPipe.database.addToRunsTable(time.time(),self.action,' '.join(sys.argv),False,os.getpid())
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Running '+' '.join(sys.argv)+'\n')
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Hostname '+socket.gethostname()+'\n')

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Action is '+sys.argv[2]+'\n')
	if self.action == 'addSample':self.addSample()
	elif self.action == 'addFastq':self.addFastq()
	elif self.action == 'report':self.report()
	elif self.action == 'createScripts':self.createScripts()
	elif self.action == 'submitScripts':self.submitScripts()
	elif self.action == 'makeGraphics':self.makeGraphics()
	elif self.action == 'reSubmit':self.reSubmit()
	elif self.action == 'copyReport':self.copyReport()
	elif self.action == 'variationCheck':self.variationCheck()
	elif self.action == 'changeSetting':self.changeSetting()
	else:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Please supply a Valid action on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	    
	AnalysisPipe.settings.saveToDb()

    def reSubmit(self, remove=True):

	import sys
	import time
	import glob
	import os
	import shutil
	import re

	try: sampleIdOrName = sys.argv[-1]
	except IndexError:
	    AnalysisPipe.logfile.write('#ERROR#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Bad command no sample id or name found ... \n')
	    sys.stderr.write('#ERROR#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Bad command no sample id or name found ... \n')
	    sys.exit()

	try:
	    submissionArray = sys.argv[-2]
	    if not re.match('[01]{8}',submissionArray): submissionArray = None
	    else:
		submissionArray=[int(i) for i in submissionArray]
	except IndexError: pass

	try:
	    project = sys.argv[3]
	    AnalysisPipe.settings.setVariable('uppmaxProject',project)
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try:
	    wgsOrExome = sys.argv[4]
	    AnalysisPipe.settings.setVariable('mode',wgsOrExome)
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)


	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Creating sbatch scripts:\n')

	sampleFound = False
	for sample in AnalysisPipe.database.getSamples():
	    
	    if sampleIdOrName == sample.name or (sampleIdOrName.isdigit() and int(sampleIdOrName) == sample.id):

		sampleFound = True
		if submissionArray and remove:
		    if submissionArray[0]: pass
		    if submissionArray[1]: pass
		    if submissionArray[2]: pass
		    if submissionArray[3]: pass
		    if submissionArray[4]: pass
		    if submissionArray[5]: pass
		    if submissionArray[6]: pass
		    if submissionArray[7]: pass
		elif remove:
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Removing old sbatch scripts, data and logs ... \n')
		    files = list(glob.iglob( sample.scriptPath+'/*' ))
		    files+= list(glob.iglob( sample.dataPath  +'/*' ))
		    files+= list(glob.iglob( sample.logPath   +'/*' ))
		    files+= list(glob.iglob( sample.fastqcPath+'/*' ))
		    for filename in files:
			if os.path.isdir(filename): shutil.rmtree(filename);
			else: os.remove(filename)
			sys.stderr.write('removing '+filename+'\n')

		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# sample: '+sample.name+' ... \n')
		try: sample.getFastqs().next()
		except StopIteration:
		    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		    continue
		sample.trimFastqs()
		sample.mapFastqs()
		sample.mergeMapped()
		sample.filterAndFixMerged()
		sample.realignerTargetCreator()
		sample.reAlignAndReCalibrate()
		sample.haplotypeCalling()
		sample.qcSteps()
	
		allSampleDependency = []

		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitting sbatches for sample: '+sample.name+' ... \n')
		try: sample.getFastqs().next()
		except StopIteration:
		    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		    continue

		jobid = None

		dependency = []
		for filePairId,readCount,fastq1,fastq2,sampleId in sample.getFastqs():

		    if not submissionArray or submissionArray[0]:
			fileName = sample.scriptPath+'/trimming.'+sample.name+'.'+str(filePairId)+'.sh'
			jobid = submitSbatch(fileName)
			AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted trimming of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

		    if not submissionArray or submissionArray[1]:
			fileName = sample.scriptPath+'/mapping.'+sample.name+'.'+str(filePairId)+'.sh'
			jobid = submitSbatch(fileName,dependency=[jobid])
			dependency.append(jobid)
			AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted mapping of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

		
		if not submissionArray or submissionArray[2]:
		    fileName = sample.scriptPath+'/mergeMapped.'+sample.name+'.sh'
		    jobid = submitSbatch(fileName,dependency=dependency)
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted mering of mapped data for '+sample.name+' with job id '+str(jobid)+' \n')

		if not submissionArray or submissionArray[3]:
		    fileName = sample.scriptPath+'/FilterAndFix.'+sample.name+'.sh'
		    jobid = submitSbatch(fileName,dependency=[jobid])
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted fixing the of merged data for '+sample.name+' with job id '+str(jobid)+' \n')

		if not submissionArray or submissionArray[4]:
		    fileName = sample.scriptPath+'/realignerTargetCreator.'+sample.name+'.sh'
		    jobid = submitSbatch(fileName,dependency=[jobid])
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted realignerTargetCreator of data for '+sample.name+' with job id '+str(jobid)+' \n')

		if not submissionArray or submissionArray[5]:
		    fileName = sample.scriptPath+'/reAlignAndReCalibrate.'+sample.name+'.sh'
		    jobid = submitSbatch(fileName,dependency=[jobid])
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted reAlignAndReCalibrate of data for '+sample.name+' with job id '+str(jobid)+' \n')

		if not submissionArray or submissionArray[6]:
		    fileName = sample.scriptPath+'/haplotypeCalling.'+sample.name+'.sh'
		    hapJobid = submitSbatch(fileName,dependency=[jobid])
		    allSampleDependency.append(hapJobid)
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted haplotypecalling for '+sample.name+' with job id '+str(jobid)+' \n')

		if not submissionArray or submissionArray[7]:
		    fileName = sample.scriptPath+'/qcSteps.'+sample.name+'.sh'
		    jobid = submitSbatch(fileName,dependency=[jobid])
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted qc steps for '+sample.name+' with job id '+str(jobid)+' \n')

	if not sampleFound:
	    AnalysisPipe.logfile.write('#ERROR#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Sample '+str(sampleIdOrName)+' not found in database ... \n')
	    sys.stderr.write(          '#ERROR#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Sample '+str(sampleIdOrName)+' not found in database ... \n')
	    sys.exit()

    def addFastq(self,):

	import sys
	import time
	
	try: sample = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Please supply a sample name on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	try:
	    f1 = sys.argv[4]
	    f2 = sys.argv[5]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Please supply a pair of fastq files on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	AnalysisPipe.database.addFastqs(sample,f1,f2)

    def addSample(self, ):
	import sys
	import time
	try: AnalysisPipe.database.addSample(sys.argv[3],newSampleRefType=sys.argv[4])
	except IndexError: AnalysisPipe.database.addSample(sys.argv[3])

    def openLogfileConnection(self,):
        """ open a connection to the logfile, creates a logfile if none is present """
        
        #
        # Imports
        #
        import sys
        import time
        import os
        
        #
        # for logmessages
        #        
        tmpLogMessages = []
        
        #
        # check if logfile present open connection or create
        #
        AnalysisPipe.logfile = self.path + '/logfile.txt'
        if os.path.isfile(AnalysisPipe.logfile):
            if 'initiateAnalysis' in sys.argv:
                sys.stderr.write('#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# The logfile already exists please use another path to initiate the analysis.\n')
                sys.exit(1)
            else:
                AnalysisPipe.logfile = open(AnalysisPipe.logfile,'a',1)
                AnalysisPipe.logfile.write('----------------\n#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Connection to logfile '+AnalysisPipe.logfile.name+' opened.\n')
                return 0
        else:
            tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Creating the logfile "'+AnalysisPipe.logfile+'".'
            tmpLogMessages.append(tmpLogMessage)
            tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Looking for folder '+self.path+'...'
            tmpLogMessages.append(tmpLogMessage)
	    if not os.path.isdir(self.path):
		tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Path '+self.path+' not found creating...'
		tmpLogMessages.append(tmpLogMessage)
		os.mkdir(self.path)
            AnalysisPipe.logfile = open(AnalysisPipe.logfile,'w',1)
        
	AnalysisPipe.logfile.write('\n'.join(tmpLogMessages)+'\n')
	
        return tmpLogMessages

    def report(self, ):

	import time
	import operator
	import os
	import glob
	import multiprocessing
	import pysam
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Creating a report located at '+self.path+'/report.htm'+' \n')
    
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Getting stats for all samlpes ....'+' \n')
	#samples = AnalysisPipe.database.getSamples()
	samples = []
	sampleCount = sum(1 for sample in AnalysisPipe.database.getSamples())

	poolOfProcesses = multiprocessing.Pool(min([multiprocessing.cpu_count(),sampleCount]),maxtasksperchild=1)
	parallelResults = poolOfProcesses.imap(reportParallel,AnalysisPipe.database.getSamples(),chunksize=1)

	samplesbyName = {}
	samplesbyId = {}
	refSamplesFirst = []
	lengthOfChromByName = {}
	for sample in parallelResults:
	#for sample in samples:
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading stats for '+sample.name+' ....'+' \n')
	    samplesbyId[sample.id]=sample
	    samplesbyName[sample.name]=sample
	    samples.append(sample)
	    #sample.getStats()
	    if sample.refType: refSamplesFirst.append(sample)
	### get chromosome sizes
	    if not os.path.exists(sample.dataPath+'/'+sample.name+'.noDuplicates.bam'):
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Cannot fetch chromsizes from bam for sample '+sample.name+' the infile has not been created yet...\n')
	    bamfileName = sample.dataPath+'/'+sample.name+'.noDuplicates.bam'
	    try: bamfile = pysam.Samfile(bamfileName, "rb")
	    except IOError:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Cannot fetch chromsizes from bam for sample '+sample.name+' the infile has not been created yet...\n')
		continue
	    except ValueError:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Cannot fetch chromsizes from bam for sample '+sample.name+' the infile is not finished for processing...\n')
		continue
	    for chrom in bamfile.header['SQ']: lengthOfChromByName[chrom['SN']] = chrom['LN']
	### done
	for sampleId, sample in sorted(samplesbyId.iteritems(), key=operator.itemgetter(0)):
	    if sample not in refSamplesFirst: refSamplesFirst.append(sample)
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Sample stats loaded, creating output ....'+' \n')
	
	reportFile = open(self.path+'/report.htm','w',1)
	reportFile.write('<html>')
	reportFile.write("""<head><style>
	    body {
		background-color:white
		font-family : Arial,"Myriad Web",Verdana,Helvetica,sans-serif;
	    }
	    h1   {color:black}
	    p    {color:green}
	    
	    table, th, td {
		border: 1px solid black;
		border-collapse: collapse;
		
		font-size : 12;
	    }
	    th {
		text-align: center;
		background-color:darkgray;
		color:white;
		border: 1px solid black;
	    }
	    table {border-spacing: 1px;}
	    th,td {padding: 5px;}
	    td {text-align: center;}
	</style></head>""")

	reportFile.write('<body>')
	reportFile.write('<h1>Analysis Report '+self.path+'</h1>\n')

	reportFile.write('<h2>List of samples:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>Sample Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>Read Pair Count</th>')
	reportFile.write('<th>Pairs After Trimming</th>')
	reportFile.write('<th>Overall Mapping Rate<br>(bowtie)</th>')
	reportFile.write('<th>Overall Mapping Rate<br>(flagstat)</th>')
	reportFile.write('<th>Duplication Rate<br>(preFilter)</th>')
	reportFile.write('<th>Unmapped or<br>Low Mapping Quality</th>')
	reportFile.write('<th>Left After<br>All Filtering</th>')
	reportFile.write('<th>'+AnalysisPipe.settings.mode+'<br>Coverage</th>')
	reportFile.write('</tr>')
	for sample in refSamplesFirst:
	    reportFile.write('<tr>')
	    reportFile.write('<td>')
	    reportFile.write(str(sample.id))
	    reportFile.write('</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    reportFile.write('<td>'+thousandString(str(sample.readCount))+'</td>')
	    try:            reportFile.write('<td>'+thousandString(str(int(sample.stats['removeEmptyReads']['sum']['pairsOut'])))+' ('+str(percentage(sample.stats['removeEmptyReads']['sum']['pairsOut'],sample.readCount))+'%)</td>')
	    except KeyError:reportFile.write('<td><font color="red">NA (NA%)</font></td>')
	    try:
		tmpCount  = sum([sample.stats['bowtie2']['sum'][tmp] for tmp in ['singleSingleMap','singleMultiMap']])
		tmpCount += sum([sample.stats['bowtie2']['sum'][tmp] for tmp in ['discordantPairs','properPairsMultiMap','properPairs']])*2
		assert sample.stats['bowtie2']['sum']['totalReads'] == sample.stats['removeEmptyReads']['sum']['pairsOut'], 'Error: pair counts do not match'
		reportFile.write('<td>'+str(percentage(tmpCount,sample.stats['bowtie2']['sum']['totalReads']*2))+'%</td>')
	    except KeyError:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['fixedBamFlagstat']['mapped']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['fixedBamFlagstat']['duplicates']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['fixedBamFlagstat']['totalReads'])-int(sample.stats['qualFilteredBamFlagstat']['totalReads']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['noDuplicatesBamFlagstat']['mapped']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    if 'averageTartgetCoverage' in sample.stats and sample.stats['averageTartgetCoverage'] != 'NA':reportFile.write('<td>'+str(sample.stats['averageTartgetCoverage'])+'%</td>')
	    else: reportFile.write('<td><font color="red">NA%</font></td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')

	reportFile.write('<h2>List of fastqs:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>File Id</th>')
	reportFile.write('<th># Read Pairs</th> ')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>File Name (r1)</th>')
	reportFile.write('<th>FastQC r1</th>')
	reportFile.write('<th>FastQC r2</th>')
	reportFile.write('</tr>')
	for filePairId,readCount,fastq1,fastq2,sampleId in sorted(AnalysisPipe.database.getFastqs(), key=operator.itemgetter(0)):
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(filePairId)+'</td>')
	    reportFile.write('<td>'+thousandString(str(readCount))+'</td>')
	    try:               reportFile.write('<td>'+samplesbyId[int(sampleId)].name+'</td>')
	    except KeyError:   reportFile.write('<td>'+'Unknown'                      +'</td>')
	    reportFile.write('<td>'+fastq1+'</td>')
	    try:
		if os.path.exists(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r1.allTrimmed_fastqc.html'): reportFile.write('<td><a href="'+os.path.relpath(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r1.allTrimmed_fastqc.html','/'.join(reportFile.name.split('/')[:-1]))+'">here</a></td>')
		else:reportFile.write('<td><font color="red">NA</font></td>')
		if os.path.exists(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r2.allTrimmed_fastqc.html'): reportFile.write('<td><a href="'+os.path.relpath(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r2.allTrimmed_fastqc.html','/'.join(reportFile.name.split('/')[:-1]))+'">here</a></td>')
		else:reportFile.write('<td><font color="red">NA</font></td>')
	    except KeyError:
		reportFile.write('<td><font color="red">NA</font></td>')
		reportFile.write('<td><font color="red">NA</font></td>')
	    reportFile.write('</tr>')
	reportFile.write('</table>')

	reportFile.write('<h2>Trimming Details:</h2>')
	reportFile.write('<h3>Samples:</h3>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>Sample Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>Original Bases</th>')
	reportFile.write('<th>% rubicon adapter</th>')
	reportFile.write('<th>% malbac adapter</th>')
	reportFile.write('<th>% ampliOne adapter</th>')
	reportFile.write('<th>% illumina Adapters</th>')
	reportFile.write('<th>% quality trimmed</th>')
	reportFile.write('</tr>')
	for sample in refSamplesFirst:
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(sample.id)+'</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    try: reportFile.write('<td>'+thousandString(str(int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'</td>')
	    except (KeyError, TypeError) as e: reportFile.write('<td><font color="red">NA</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['rubiconWgaTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['malbacWgaTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['ampliOneTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['illuminaAndNexteraTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['qualityTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')

	reportFile.write('<h3>Files (r1 then r2):</h3>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>File Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th colspan="2">Original Bases</th>')
	reportFile.write('<th colspan="2">% rubicon adapter</th>')
	reportFile.write('<th colspan="2">% malbac adapter</th>')
	reportFile.write('<th colspan="2">% ampliOne adapter</th>')
	reportFile.write('<th colspan="2">% illumina Adapters</th>')
	reportFile.write('<th colspan="2">% quality trimmed</th>')
	reportFile.write('</tr>')
	for filePairId,readCount,fastq1,fastq2,sampleId in sorted(AnalysisPipe.database.getFastqs(), key=operator.itemgetter(0)):
	    try:
		sample = samplesbyId[int(sampleId)]
		reportFile.write('<tr>')
		reportFile.write('<td>'+str(filePairId)+'</td>')
		reportFile.write('<td>'+sample.name+'</td>')
		for read in ['r1','r2']:
		    try: reportFile.write('<td>'+thousandString(str(int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'</td>')
		    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA</font></td>')
		for read in ['r1','r2']:
		    try: reportFile.write('<td>'+str(percentage(int(sample.stats['rubiconWgaTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
		for read in ['r1','r2']:
		    try: reportFile.write('<td>'+str(percentage(int(sample.stats['malbacWgaTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
		for read in ['r1','r2']:
		    try: reportFile.write('<td>'+str(percentage(int(sample.stats['ampliOneTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
		for read in ['r1','r2']:
		    try: reportFile.write('<td>'+str(percentage(int(sample.stats['illuminaAndNexteraTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
		for read in ['r1','r2']:
		    try: reportFile.write('<td>'+str(percentage(int(sample.stats['qualityTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
		reportFile.write('<tr>')
	    except KeyError: pass
	reportFile.write('</table>')

	reportFile.write('<h2>Read Orientations:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>Sample Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>Left After<br>All Filtering</th>')
	reportFile.write('<th>Properly<br>Paired</th>')
	reportFile.write('<th>Fwd Rev</th>')
	reportFile.write('<th>Fwd Rev<br>full overlapp</th>')
	reportFile.write('<th>Fwd Fwd</th>')
	reportFile.write('<th>Rev Fwd</th>')
	reportFile.write('<th>Rev Rev</th>')
	reportFile.write('<th>r1 r2 on<br>diferent chrom</th>')
	reportFile.write('</tr>')
	for sample in refSamplesFirst:
	    reportFile.write('<tr>')
	    reportFile.write('<td>')
	    reportFile.write(str(sample.id))
	    reportFile.write('</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['noDuplicatesBamFlagstat']['mapped']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['orientations']['PP']), int(sum(sample.stats['orientations'].values()))))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['orientations']['FR']), int(sum(sample.stats['orientations'].values()))))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['orientations']['FRsp']), int(sum(sample.stats['orientations'].values()))))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['orientations']['FF']), int(sum(sample.stats['orientations'].values()))))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['orientations']['RF']), int(sum(sample.stats['orientations'].values()))))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['orientations']['RR']), int(sum(sample.stats['orientations'].values()))))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['orientations']['difChrom']), int(sum(sample.stats['orientations'].values()))))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')

	reportFile.write('<h2>HS-METRICS:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>Sample Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>On Target</th>')
	reportFile.write('<th>% RD=0 targets</th>')
	reportFile.write('<th>% bases @2X</th>')
	reportFile.write('<th>% bases @10X</th>')
	reportFile.write('<th>% bases @20X</th>')
	reportFile.write('<th>% bases @30X</th>')
	reportFile.write('<th>% bases @40X</th>')
	reportFile.write('<th>% bases @50X</th>')
	reportFile.write('<th>% bases @100X</th>')
	reportFile.write('<th>FOLD_ENRICHMENT</th>')
	reportFile.write('</tr>')
	for sample in refSamplesFirst:
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(sample.id)+'</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_SELECTED_BASES'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e: reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['ZERO_CVG_TARGETS_PCT'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_2X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_10X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_20X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_30X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_40X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_50X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_100X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</font></td>')
	    try: reportFile.write('<td>'+str(round(float(sample.stats['hs_metrics.summary']['FOLD_ENRICHMENT'].replace(',','.')),2))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA</font></td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')

	reportFile.write('<h2>Variants:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>Sample Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>refSample</th>')
	reportFile.write('<th>totalVars</th>')
	reportFile.write('<th>% Hetero</th>')
	reportFile.write('<th>% Dropout</th>')
	reportFile.write('<th>% Other</th>')
	if os.path.exists(AnalysisPipe.path+'/SNPidentificationTable.colored.txt'):
	    reportFile.write('<th>totalVars<br>Identification</th>')
	    reportFile.write('<th>% Patient</th>')
	    reportFile.write('<th>% Donor</th>')
	    reportFile.write('<th>% Mix</th>')
	    reportFile.write('<th>% Other</th>')
	    reportFile.write('<th>Classification</th>')
	    reportFile.write('<th>Low Confidence</th>')
	reportFile.write('</tr>')
	for sample in refSamplesFirst:
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(sample.id)+'</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    if sample.stats['refsample'] != 'NA':reportFile.write('<td>'+str(sample.stats['refsample'])+'</td>')
	    else: reportFile.write('<td><font color="red">NA%</font></td>')
	    if sample.stats['totalAdoVar'] != 'NA':reportFile.write('<td>'+str(sample.stats['totalAdoVar'])+'</td>')
	    else: reportFile.write('<td><font color="red">NA%</font></td>')
	    if sample.stats['correct%'] != 'NA':reportFile.write('<td>'+str(sample.stats['correct%'])+'%</td>')
	    else: reportFile.write('<td><font color="red">NA%</font></td>')
	    if sample.stats['dropout%'] != 'NA':reportFile.write('<td>'+str(sample.stats['dropout%'])+'%</td>')
	    else: reportFile.write('<td><font color="red">NA%</font></td>')
	    if sample.stats['other%'] != 'NA':reportFile.write('<td>'+str(sample.stats['other%'])+'%</td>')
	    else: reportFile.write('<td><font color="red">NA%</font></td>')
	    if os.path.exists(AnalysisPipe.path+'/SNPidentificationTable.colored.txt'):
		if sample.stats['totalIdVar'] != 'NA':reportFile.write('<td>'+str(sample.stats['totalIdVar'])+'</td>')
		else: reportFile.write('<td><font color="red">NA%</font></td>')
		if sample.stats['pat%'] != 'NA':reportFile.write('<td>'+str(sample.stats['pat%'])+'%</td>')
		else: reportFile.write('<td><font color="red">NA%</font></td>')
		if sample.stats['don%'] != 'NA':reportFile.write('<td>'+str(sample.stats['don%'])+'%</td>')
		else: reportFile.write('<td><font color="red">NA%</font></td>')
		if sample.stats['mix%'] != 'NA':reportFile.write('<td>'+str(sample.stats['mix%'])+'%</td>')
		else: reportFile.write('<td><font color="red">NA%</font></td>')
		if sample.stats['other%'] != 'NA':reportFile.write('<td>'+str(sample.stats['other%'])+'%</td>')
		else: reportFile.write('<td><font color="red">NA%</font></td>')
		if sample.stats['sampleIs'] != 'NA':reportFile.write('<td>'+str(sample.stats['sampleIs'])+'</td>')
		else: reportFile.write('<td><font color="red">NA%</font></td>')
		if sample.stats['sampleIsLowConf'] != 'NA':reportFile.write('<td>'+str(sample.stats['sampleIsLowConf'])+'</td>')
		else: reportFile.write('<td><font color="red">NA%</font></td>')

	    reportFile.write('<tr>')
	reportFile.write('</table>')

	if not list(glob.iglob(AnalysisPipe.path+'/graphics/heatmap*.png')):
	    reportFile.write('<font color="red">No identification heatmap graphs created yet.</font><br>')
	else:
	    reportFile.write('<h2>Heatmaps:</h2>')
	    for filename in list(glob.iglob(AnalysisPipe.path+'/graphics/heatmap*.png')):
		reportFile.write(''+filename.split('.png')[0].split('/')[-1]+':<br>')
		reportFile.write('<a href="'+os.path.relpath(filename.split('.png')[0]+'.pdf','/'.join(reportFile.name.split('/')[:-1]))+'">')
		reportFile.write('<img width="100%" src="'+os.path.relpath(filename.split('.png')[0]+'.png',  '/'.join(reportFile.name.split('/')[:-1]))+'">')
		reportFile.write('</a>')

	if not list(glob.iglob(AnalysisPipe.path+'/graphics/pieChart*.png')):
	    reportFile.write('<font color="red">No identification piecharts graphs created yet.</font><br>')
	else:
	    reportFile.write('<h2>Piecharts:</h2>')
	    for filename in list(glob.iglob(AnalysisPipe.path+'/graphics/pieChart*.png')):
		reportFile.write(''+filename.split('.png')[0].split('/')[-1]+':<br>')
		reportFile.write('<a href="'+os.path.relpath(filename.split('.png')[0]+'.pdf','/'.join(reportFile.name.split('/')[:-1]))+'">')
		reportFile.write('<img width="100%" src="'+os.path.relpath(filename.split('.png')[0]+'.png',  '/'.join(reportFile.name.split('/')[:-1]))+'">')
		reportFile.write('</a>')

	reportFile.write('<h2>InsertSize graphs:</h2>')
	if os.path.exists(AnalysisPipe.path+'/graphics/insertSizes.png'):
	    reportFile.write('<a href="'+os.path.relpath(AnalysisPipe.path+'/graphics/insertSizes.pdf','/'.join(reportFile.name.split('/')[:-1]))+'">')
	    reportFile.write('<img src="'+os.path.relpath(AnalysisPipe.path+'/graphics/insertSizes.png',  '/'.join(reportFile.name.split('/')[:-1]))+'">')
	    reportFile.write('</a>')
	else: reportFile.write('<font color="red">Insert size graph not created yet.</font><br>')
	reportFile.write('<h2>Coverage graphs:</h2>')
	if os.path.exists(AnalysisPipe.path+'/graphics/lorentzCurve.png'):
	    reportFile.write('<a href="'+os.path.relpath(AnalysisPipe.path+'/graphics/lorentzCurve.pdf','/'.join(reportFile.name.split('/')[:-1]))+'">')
	    reportFile.write('<img src="'+os.path.relpath(AnalysisPipe.path+'/graphics/lorentzCurve.png', '/'.join(reportFile.name.split('/')[:-1]))+'">')
	    reportFile.write('</a>')
	else: reportFile.write('<font color="red">Lorentz curve graph not created yet.</font><br>')
	if os.path.exists(AnalysisPipe.path+'/graphics/lorentzCurveCoveredOnly.png'):
	    reportFile.write('<a href="'+os.path.relpath(AnalysisPipe.path+'/graphics/lorentzCurveCoveredOnly.pdf','/'.join(reportFile.name.split('/')[:-1]))+'">')
	    reportFile.write('<img src="'+os.path.relpath(AnalysisPipe.path+'/graphics/lorentzCurveCoveredOnly.png', '/'.join(reportFile.name.split('/')[:-1]))+'">')
	    reportFile.write('</a>')
	else: reportFile.write('<font color="red">Lorentz curve graph for CoveredOnly not created yet.</font><br>')
	if os.path.exists(AnalysisPipe.path+'/graphics/exomecoverage.png'):
	    reportFile.write('<a href="'+os.path.relpath(AnalysisPipe.path+'/graphics/exomecoverage.pdf','/'.join(reportFile.name.split('/')[:-1]))+'">')
	    reportFile.write('<img src="'+os.path.relpath(AnalysisPipe.path+'/graphics/exomecoverage.png','/'.join(reportFile.name.split('/')[:-1]))+'">')
	    reportFile.write('</a>')
	else: reportFile.write('<font color="red">Exome coverage graph not created yet.</font><br>')
	reportFile.write('<br>')
	
	if not list(glob.iglob(AnalysisPipe.path+'/samples/*/plots/readDepth.*.png')):
	    reportFile.write('<font color="red">No read depth graphs created yet.</font><br>')
	else:
	    for chrom in sorted_nicely(lengthOfChromByName.keys()):
		if chrom[0] == 'G':continue
		reportFile.write('<br><br>Coverage over chromosome '+chrom+':<br>')
		for sample in refSamplesFirst:
		    if os.path.exists(sample.plotsPath+'/readDepth.'+sample.name+'.'+chrom+'.png'):
			reportFile.write('<a href="'+ os.path.relpath(sample.plotsPath+'/readDepth.'+sample.name+'.'+chrom+'.pdf','/'.join(reportFile.name.split('/')[:-1]))+'">')
			reportFile.write('<img src="'+os.path.relpath(sample.plotsPath+'/readDepth.'+sample.name+'.'+chrom+'.png', '/'.join(reportFile.name.split('/')[:-1]))+'">')
			reportFile.write('</a><br>')

	#for imgFileName in glob.iglob(AnalysisPipe.path+'/graphics/readDepth.*.png'):
	#    relPath = os.path.relpath(imgFileName, '/'.join(reportFile.name.split('/')[:-1]))
	#    reportFile.write('<br>Coverage over chromosome '+imgFileName.split('/readDepth.')[-1].split('.png')[0]+':<br>')
	#    reportFile.write('<img src="'+relPath+'">')

	reportFile.write('</body></html>\n')
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Full report now written to '+self.path+'/report.htm'+' \n')
	return 0

    def createScripts(self, ):

	import time
	import sys
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Creating sbatch scripts:\n')

	try:
	    project = sys.argv[3]
	    AnalysisPipe.settings.setVariable('uppmaxProject',project)
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try:
	    wgsOrExome = sys.argv[4]
	    AnalysisPipe.settings.setVariable('mode',wgsOrExome)
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	for sample in AnalysisPipe.database.getSamples():
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# sample: '+sample.name+' ... \n')
	    try: sample.getFastqs().next()
	    except StopIteration:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		continue
	    sample.trimFastqs()
	    sample.mapFastqs()
	    sample.mergeMapped()
	    sample.filterAndFixMerged()
	    sample.realignerTargetCreator()
	    sample.reAlignAndReCalibrate()
	    sample.haplotypeCalling()
	    sample.qcSteps()
	
	self.createPipeWideScripts()

    def submitScripts(self,sampleNameOrId=None):

	import time

	allSampleDependency = []
	for sample in AnalysisPipe.database.getSamples():

	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitting sbatches for sample: '+sample.name+' ... \n')
	    try: sample.getFastqs().next()
	    except StopIteration:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		continue

	    dependency = []
	    for filePairId,readCount,fastq1,fastq2,sampleId in sample.getFastqs():

		fileName = sample.scriptPath+'/trimming.'+sample.name+'.'+str(filePairId)+'.sh'
		jobid = submitSbatch(fileName)
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted trimming of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

		fileName = sample.scriptPath+'/mapping.'+sample.name+'.'+str(filePairId)+'.sh'
		jobid = submitSbatch(fileName,dependency=[jobid])
		dependency.append(jobid)
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted mapping of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/mergeMapped.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=dependency)
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted mering of mapped data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/FilterAndFix.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted fixing the of merged data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/realignerTargetCreator.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted realignerTargetCreator of data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/reAlignAndReCalibrate.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted reAlignAndReCalibrate of data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/haplotypeCalling.'+sample.name+'.sh'
	    hapJobid = submitSbatch(fileName,dependency=[jobid])
	    allSampleDependency.append(hapJobid)
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted haplotypecalling for '+sample.name+' with job id '+str(jobid)+' \n')
	    
	    fileName = sample.scriptPath+'/qcSteps.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Submitted qc steps for '+sample.name+' with job id '+str(jobid)+' \n')

    def makeISizePlot(self,):

	import pysam
	import numpy as np
	import matplotlib.pyplot as plt
	import operator
	import sys
	import os
	import time
	import multiprocessing

	htmlColors = ['AliceBlue','AntiqueWhite','Aqua','Aquamarine','Azure','Beige','Bisque','Black','BlanchedAlmond','Blue','BlueViolet','Brown','BurlyWood','CadetBlue','Chartreuse','Chocolate','Coral','CornflowerBlue','Cornsilk','Crimson','Cyan','DarkBlue','DarkCyan','DarkGoldenRod','DarkGray','DarkGreen','DarkKhaki','DarkMagenta','DarkOliveGreen','DarkOrange','DarkOrchid','DarkRed','DarkSalmon','DarkSeaGreen','DarkSlateBlue','DarkSlateGray','DarkTurquoise','DarkViolet','DeepPink','DeepSkyBlue','DimGray','DodgerBlue','FireBrick','FloralWhite','ForestGreen','Fuchsia','Gainsboro','GhostWhite','Gold','GoldenRod','Gray','Green','GreenYellow','HoneyDew','HotPink','IndianRed','Indigo','Ivory','Khaki','Lavender','LavenderBlush','LawnGreen','LemonChiffon','LightBlue','LightCoral','LightCyan','LightGoldenRodYellow','LightGray','LightGreen','LightPink','LightSalmon','LightSeaGreen','LightSkyBlue','LightSlateGray','LightSteelBlue','LightYellow','Lime','LimeGreen','Linen','Magenta','Maroon','MediumAquaMarine','MediumBlue','MediumOrchid','MediumPurple','MediumSeaGreen','MediumSlateBlue','MediumSpringGreen','MediumTurquoise','MediumVioletRed','MidnightBlue','MintCream','MistyRose','Moccasin','NavajoWhite','Navy','OldLace','Olive','OliveDrab','Orange','OrangeRed','Orchid','PaleGoldenRod','PaleGreen','PaleTurquoise','PaleVioletRed','PapayaWhip','PeachPuff','Peru','Pink','Plum','PowderBlue','Purple','Red','RosyBrown','RoyalBlue','SaddleBrown','Salmon','SandyBrown','SeaGreen','SeaShell','Sienna','Silver','SkyBlue','SlateBlue','SlateGray','Snow','SpringGreen','SteelBlue','Tan','Teal','Thistle','Tomato','Turquoise','Violet','Wheat','WhiteSmoke','Yellow','YellowGreen']
	linestyles = ['-',':','--','-.']
	colors = ['b','b:','r','r:','g','g:','c','c:','m','m:','y','y:','k','k:']+[i+'--' for i in 'rgbcmyk']+[i+'-.' for i in 'rgbcmyk'] # ['-' | '--' | '-.' | ':' | 'None' | ' ' | '']
	isizes = {}

	sampleCount = sum([1 for sample in AnalysisPipe.database.getSamples()])
	poolOfProcesses = multiprocessing.Pool(min([multiprocessing.cpu_count(),sampleCount]),maxtasksperchild=1)
	parallelResults = poolOfProcesses.imap_unordered(getIsizeData,AnalysisPipe.database.getSamples(),chunksize=1)
	
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading and processing indata ...\n')
	for output in parallelResults:
	    sample, sampleInsSizes = output
	    isizes[sample.name] = sampleInsSizes
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# All iSize data loaded and processed.\n')
	poolOfProcesses.close()
	poolOfProcesses.join()

	#for sample in AnalysisPipe.database.getSamples():
	#    try: bamfile = pysam.Samfile(sample.dataPath+'/'+sample.name+'.noDuplicates.bam', "rb")
	#    except IOError:
	#	AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping insert size plot for sample '+sample.name+' the infile has not been created yet...\n')
	#	continue
	#    except ValueError:
	#	AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping insert size plot for sample '+sample.name+' the infile is not finished for processing...\n')
	#	continue
	#    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading insert sizes for sample '+sample.name+'...\n')
	#    for read in bamfile.fetch():
	#	if read.tlen >= 1:
	#	    try:isizes[sample.name].append(int(read.tlen))
	#	    except KeyError:isizes[sample.name]= [read.tlen]

	if not isizes:
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping insert size plot no sample has data ...\n')
	    return 0

	fig, axes = plt.subplots(1, sharex=True)
	plots = []
	incrementer=0
	incrementer2=0
	incrementer3=0

	for sample in AnalysisPipe.database.getSamples():
	    #n, bins, patches = axes.hist(isizes[sample.name], 50, normed=1, histtype='step',label=sample.name)
	    counter = {}
	    total = 0

#	    for isize in isizes[sample.name]:
#		try:counter[isize] += 1
#		except KeyError: counter[isize] = 1
#		total+=1
	    counter = isizes[sample.name]
	    total = sum(counter.values())

	    y = [percentage(value,total) for key, value in sorted(counter.iteritems(), key=operator.itemgetter(0))]
	    #y = [value for key, value in sorted(counter.iteritems(), key=operator.itemgetter(0))]
	    x = sorted(counter.keys())
	    try: plots.append(axes.plot(x, y,colors[incrementer],label=sample.name))
	    except IndexError:
		plots.append(axes.plot(x, y,color=htmlColors[incrementer2],linestyle=linestyles[incrementer3],label=sample.name))
		if incrementer3 == 3:
		    incrementer2+=1
		    incrementer3 = 0
		else: incrementer3+=1
	    incrementer+=1
	    axes.set_xlim([0,1000])

	handles, labels = axes.get_legend_handles_labels()
	hl = sorted(zip(handles, labels), key=operator.itemgetter(1))
	handles2, labels2 = zip(*hl)
	axes.legend(handles2, labels2,loc=0,fontsize='small')
	axes.set_xlabel('InsertSize (final filtered data)')
	axes.set_ylabel('Frequency')
	plt.savefig(AnalysisPipe.path+'/graphics/insertSizes.pdf',dpi=50,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/insertSizes.png',dpi=50,bbox_inches='tight')

    def makeGiniAndCoverageatRDplot(self):
	import numpy as np
	import matplotlib.pyplot as plt
	import operator
	import sys
	import os
	import time
	import multiprocessing

	sampleCount = sum([1 for sample in AnalysisPipe.database.getSamples()])

	poolOfProcesses = multiprocessing.Pool(min([multiprocessing.cpu_count(),sampleCount]),maxtasksperchild=1)
	parallelResults = poolOfProcesses.imap_unordered(getGiniData,AnalysisPipe.database.getSamples(),chunksize=1)

	referenceBaseCount ={}
	plotData = {}
	returnedSampleNames = []
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading and processing indata ...\n')
	for output in parallelResults:
	    if not output: continue
	    sample, gini_x, gini_y,gini_x_cov, gini_y_cov, cAtRd_x, cAtRd_y, referenceBaseCount[sample.name] = output
	    returnedSampleNames.append(sample.name)
	    plotData[sample.name] = {'gini_x':gini_x, 'gini_y':gini_y,'gini_x_cov':gini_x_cov, 'gini_y_cov':gini_y_cov, 'cAtRd_x':cAtRd_x, 'cAtRd_y':cAtRd_y}
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# All coverage data loaded and processed.\n')
	
	samples = []
	for sample in AnalysisPipe.database.getSamples():
	    if sample.name in returnedSampleNames: samples.append(sample)
	if not samples:
	    #p.join()
	    return 0

	for value in referenceBaseCount.values(): assert value == max(referenceBaseCount.values()) or value == 1, str(value)+' == '+str(max(referenceBaseCount.values()))
	htmlColors = ['AliceBlue','AntiqueWhite','Aqua','Aquamarine','Azure','Beige','Bisque','Black','BlanchedAlmond','Blue','BlueViolet','Brown','BurlyWood','CadetBlue','Chartreuse','Chocolate','Coral','CornflowerBlue','Cornsilk','Crimson','Cyan','DarkBlue','DarkCyan','DarkGoldenRod','DarkGray','DarkGreen','DarkKhaki','DarkMagenta','DarkOliveGreen','DarkOrange','DarkOrchid','DarkRed','DarkSalmon','DarkSeaGreen','DarkSlateBlue','DarkSlateGray','DarkTurquoise','DarkViolet','DeepPink','DeepSkyBlue','DimGray','DodgerBlue','FireBrick','FloralWhite','ForestGreen','Fuchsia','Gainsboro','GhostWhite','Gold','GoldenRod','Gray','Green','GreenYellow','HoneyDew','HotPink','IndianRed','Indigo','Ivory','Khaki','Lavender','LavenderBlush','LawnGreen','LemonChiffon','LightBlue','LightCoral','LightCyan','LightGoldenRodYellow','LightGray','LightGreen','LightPink','LightSalmon','LightSeaGreen','LightSkyBlue','LightSlateGray','LightSteelBlue','LightYellow','Lime','LimeGreen','Linen','Magenta','Maroon','MediumAquaMarine','MediumBlue','MediumOrchid','MediumPurple','MediumSeaGreen','MediumSlateBlue','MediumSpringGreen','MediumTurquoise','MediumVioletRed','MidnightBlue','MintCream','MistyRose','Moccasin','NavajoWhite','Navy','OldLace','Olive','OliveDrab','Orange','OrangeRed','Orchid','PaleGoldenRod','PaleGreen','PaleTurquoise','PaleVioletRed','PapayaWhip','PeachPuff','Peru','Pink','Plum','PowderBlue','Purple','Red','RosyBrown','RoyalBlue','SaddleBrown','Salmon','SandyBrown','SeaGreen','SeaShell','Sienna','Silver','SkyBlue','SlateBlue','SlateGray','Snow','SpringGreen','SteelBlue','Tan','Teal','Thistle','Tomato','Turquoise','Violet','Wheat','WhiteSmoke','Yellow','YellowGreen']
	linestyles = ['-',':','--','-.']
	colors = ['b','b:','r','r:','g','g:','c','c:','m','m:','y','y:','k','k:']+[i+'--' for i in 'rgbcmyk']+[i+'-.' for i in 'rgbcmyk'] # ['-' | '--' | '-.' | ':' | 'None' | ' ' | '']

	# lorenz curve and gini index
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Generating lorentzcurve graph...\n')
	fig, axes = plt.subplots(1, sharex=True)
	plots = []
	a = [i/10.0 for i in range(0,11,1)]
	plots.append(axes.plot(a, a,'b-o',label='EvenDist, 0.00%'))
	incrementer=0
	incrementer2=0
	incrementer3=0
	for sample in samples:
	    x = plotData[sample.name]['gini_x']
	    y = plotData[sample.name]['gini_y']
	    B = sum( [((y[i]+y[i+1])/2.0)*(x[i+1]-x[i]) for i in range(len(x)-1)] )
	    A = 0.5 - B
	    G = A / (A + B)
	    assert A + B == 0.5
	    assert G == 2*A
	    assert G == 1 - 2*B
	    giniApprox = round(100*G,2)
	    try: plots.append(axes.plot(x, y,colors[incrementer],label=sample.name+', '+str(giniApprox)+'%'))
	    except IndexError:
		plots.append(axes.plot(x, y,color=htmlColors[incrementer2],linestyle=linestyles[incrementer3],label=sample.name+', '+str(giniApprox)+'%'))
		if incrementer3 == 3:
		    incrementer2+=1
		    incrementer3 = 0
		else: incrementer3+=1
	    incrementer+=1
	handles, labels = axes.get_legend_handles_labels()
	hl = sorted(zip(handles, labels), key=operator.itemgetter(1))
	handles2, labels2 = zip(*hl)
	axes.legend(handles2, labels2,loc=0,fontsize='small')
	axes.set_xlabel('% of Targeted Bases')
	axes.set_ylabel('% of Sequenced Bases')
	plt.savefig(AnalysisPipe.path+'/graphics/lorentzCurve.pdf',dpi=50,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/lorentzCurve.png',dpi=50,bbox_inches='tight')

	# lorenz curve and gini index covered only
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Generating lorentzcurve graph for only covered bases...\n')
	fig, axes = plt.subplots(1, sharex=True)
	plots = []
	a = [i/10.0 for i in range(0,11,1)]
	plots.append(axes.plot(a, a,'b-o',label='EvenDist, 0.00%'))
	incrementer=0
	incrementer2=0
	incrementer3=0
	for sample in samples:
	    x = plotData[sample.name]['gini_x_cov']
	    y = plotData[sample.name]['gini_y_cov']
	    B = sum( [((y[i]+y[i+1])/2.0)*(x[i+1]-x[i]) for i in range(len(x)-1)] )
	    A = 0.5 - B
	    G = A / (A + B)
	    assert A + B == 0.5
	    assert G == 2*A
	    assert G == 1 - 2*B
	    giniApprox = round(100*G,2)
	    try: plots.append(axes.plot(x, y,colors[incrementer],label=sample.name+', '+str(giniApprox)+'%'))
	    except IndexError:
		plots.append(axes.plot(x, y,color=htmlColors[incrementer2],linestyle=linestyles[incrementer3],label=sample.name+', '+str(giniApprox)+'%'))
		if incrementer3 == 3:
		    incrementer2+=1
		    incrementer3 = 0
		else: incrementer3+=1
	    incrementer+=1
	handles, labels = axes.get_legend_handles_labels()
	hl = sorted(zip(handles, labels), key=operator.itemgetter(1))
	handles2, labels2 = zip(*hl)
	axes.legend(handles2, labels2,loc=0,fontsize='small')
	axes.set_xlabel('% of Covered Bases')
	axes.set_ylabel('% of Sequenced Bases')
	plt.savefig(AnalysisPipe.path+'/graphics/lorentzCurveCoveredOnly.pdf',dpi=50,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/lorentzCurveCoveredOnly.png',dpi=50,bbox_inches='tight')

	# make coverage at rd plot
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Generating coverage at rd graph...\n')
	fig, axes = plt.subplots(1, sharex=True)
	plots = []
	incrementer=0
	incrementer2=0
	incrementer3=0
	for sample in samples:
	    try: plots.append(axes.plot(plotData[sample.name]['cAtRd_x'], plotData[sample.name]['cAtRd_y'], colors[incrementer],label=sample.name))
	    except IndexError:
		plots.append( axes.plot(plotData[sample.name]['cAtRd_x'], plotData[sample.name]['cAtRd_y'], color=htmlColors[incrementer2],linestyle=linestyles[incrementer3],label=sample.name) )
		if incrementer3 == 3:
		    incrementer2+=1
		    incrementer3 = 0
		else: incrementer3+=1
	    axes.set_xlim([1,50])
	    incrementer+=1
	handles, labels = axes.get_legend_handles_labels()
	hl = sorted(zip(handles, labels), key=operator.itemgetter(1))
	handles2, labels2 = zip(*hl)
	axes.legend(handles2, labels2,loc=0,fontsize='small')
	axes.set_xlabel('Read Depth')
	axes.set_ylabel('% of targeted bases')
	plt.savefig(AnalysisPipe.path+'/graphics/exomecoverage.pdf',dpi=50,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/exomecoverage.png',dpi=50,bbox_inches='tight')

	#p.join()
	
	return 0

    def makeRdOverChromGraph(self,):
	import numpy as np
	import matplotlib.pyplot as plt
	import operator
	import sys
	import os
	import time
	import multiprocessing

	sampleCount = sum([1 for sample in AnalysisPipe.database.getSamples()])

	poolOfProcesses = multiprocessing.Pool(min([multiprocessing.cpu_count(),sampleCount]),maxtasksperchild=1)
	parallelResults = poolOfProcesses.imap_unordered(parallelMakeRDOverChromPlots,AnalysisPipe.database.getSamples(),chunksize=1)

	referenceBaseCount ={}
	plotData = {}
	returnedSampleNames = []
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Creating per chromosome RD graphs ...\n')
	for output in parallelResults:
	    if not output: continue
	    sample = output
	    returnedSampleNames.append(sample.name)
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# All RD over chrom plots made.\n')
	poolOfProcesses.terminate()
	poolOfProcesses.join()

    def makeGraphics(self,):

	import pysam
	import numpy as np
	import matplotlib.pyplot as plt
	import operator
	import sys
	import os
	import time
	import multiprocessing

	try: os.mkdir(AnalysisPipe.path+'/graphics')
	except OSError: pass

	# make isize plot
	self.makeISizePlot()
	#p = multiprocessing.Process(target=self.makeISizePlot,args=())
	#p.start()

	self.makeGiniAndCoverageatRDplot()
	
	self.makeRdOverChromGraph()

	#sampleCount = sum([1 for sample in AnalysisPipe.database.getSamples()])
	#poolOfProcesses = multiprocessing.Pool(min([multiprocessing.cpu_count(),sampleCount]),maxtasksperchild=1)
	#parallelResults = poolOfProcesses.imap_unordered(graphForeachSample,AnalysisPipe.database.getSamples(),chunksize=1)
	#
	#rdOverRefDistYes = {}
	#rdOverRefDistXes = {}
	#returnedSampleNames = []
	#AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading and processing indata ...\n')
	#for output in parallelResults:
	#    if not output: continue
	#    sample, chromDist = output
	#    returnedSampleNames.append(sample.name)
	#    for chrom, values in chromDist.iteritems():
	#	try: rdOverRefDistYes[chrom][sample.name] = values['y']
	#	except KeyError:rdOverRefDistYes[chrom] = {sample.name:values['y']}
	#	if chrom in rdOverRefDistXes: assert rdOverRefDistXes[chrom] == values['x']
	#	else: rdOverRefDistXes[chrom] = values['x']
	#AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# All coverage data loaded and processed.\n')
	#
	#samples = []
	#for sample in AnalysisPipe.database.getSamples():
	#    if sample.name in returnedSampleNames: samples.append(sample)
	#if not samples:
	#    #p.join()
	#    return 0
	#
	## make rd over reference graph
	#AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Generating rd over reference graph...\n')
	#for chrom in rdOverRefDistXes.keys():
	#
	#    fig, axes = plt.subplots(sampleCount, sharex=True)
	#    fig.set_size_inches(30,sampleCount*3)
	#    axes[0].set_title("Chromosome "+str(chrom)+" coverage over concatenated targets")
	#
	#    tmpCounter = 0
	#    maxY = 0
	#    for sample in samples:
	#
	#	axes[tmpCounter].plot(rdOverRefDistXes[chrom], rdOverRefDistYes[chrom][sample.name], lw=1,color="green")
	#	axes[tmpCounter].fill_between(rdOverRefDistXes[chrom],0,rdOverRefDistYes[chrom][sample.name], color="green",alpha=0.5)
	#	axes[tmpCounter].set_xlabel(sample.name)
	#	tmpCounter+=1
	#	
	#	maxY = max([maxY,max(rdOverRefDistYes[chrom][sample.name])])
	#
	#    for i in range(sampleCount):
	#	axes[i].set_ylim( 0, min([30,max([myround(maxY),1])]) );
	#	axes[i].set_xlim( min(rdOverRefDistXes[chrom]), max(rdOverRefDistXes[chrom]) )
	#    plt.savefig(AnalysisPipe.path+'/graphics/readDepth.'+chrom+'.pdf',dpi=50,bbox_inches="tight")
	#    plt.savefig(AnalysisPipe.path+'/graphics/readDepth.'+chrom+'.png',dpi=50,bbox_inches="tight")

	#p.join()
	
	return 0

    def createPipeWideScripts(self,):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 16 -p node'+'\n'
	output += '#SBATCH -t 240:00:00'+'\n'
	output += '#SBATCH -J graphAndRep.'+self.path.split('/')[-1]+'\n'
	output += '#SBATCH -e '+self.path+'/stderr.graphAndRep.txt'+'\n'
	output += '#SBATCH -o '+self.path+'/stdout.graphAndRep.txt'+'\n'
	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'
	output += 'module load bioinfo-tools python/2.7 pysam  FastQC cutadapt/1.8.0 bowtie2 samtools picard/1.114 BEDTools/2.16.2  GATK/3.1.1'+'\n'
        output += 'workon py2.7\n'
        output += AnalysisPipe.programPath+' '+AnalysisPipe.path+' makeGraphics\n'
	output += AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
	output += 'wait'+'\n'

        #
        # Final output and write script to file
        #
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'wait\n'
        output += 'echo "$(date) AllDone"\n'
        with open(self.path+'/graphAndRep.sh','w') as outfile: outfile.write(output)

	# GenotypeGVCFs
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 16 -p node'+'\n'
	output += '#SBATCH -t 240:00:00'+'\n'
	output += '#SBATCH -J genotype.'+self.path.split('/')[-1]+'\n'
	output += '#SBATCH -e '+self.path+'/stderr.genotype.txt'+'\n'
	output += '#SBATCH -o '+self.path+'/stdout.genotype.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo "$(date) Running on: $(hostname)" >&2'+'\n'
        output += 'echo "HC" '+'\n'
        output += 'java -Xmx100g -jar '+AnalysisPipe.gatkLocation+' -nt 16 '
        output += '-T  GenotypeGVCFs '
        output += '-R '+AnalysisPipe.bowtie2Reference+' '
        output += '--variant '+('\\'+'\n'+' --variant ').join([sample.dataPath+'/'+sample.name+'.gvcf ' for sample in AnalysisPipe.database.getSamples()])+' '
        if wgsOrExome == 'exome': output += '-L '+AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem.bed '
	elif wgsOrExome == 'wgs': output += '-L '+AnalysisPipe.referencePath+'/wgs.bed '
        output += '--dbsnp '+AnalysisPipe.gatkBundleLocation+'/dbsnp_138.b37.vcf '
        output += '-A StrandBiasBySample -A FisherStrand -A MappingQualityZero -A MappingQualityZeroBySample -A QualByDepth '
        output += '-o '+self.path+'/raw_variants.vcf  1>&2 2> '+self.path+'/stderr.GenotypeGVCFs_GATKrun.txt &'+'\n'
        
        output += 'wait'+'\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'
        output += 'wait'+'\n'
        output += 'echo "$(date) AllDone"'+'\n'
        output += 'echo "$(date) AllDone" >&2'+'\n\n\n'
        with open(self.path+'/GenotypeGVCFs.sh','w') as outfile: outfile.write(output)
    
	# vqsr_indels
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 16 -p node'+'\n'
	output += '#SBATCH -t 240:00:00'+'\n'
	output += '#SBATCH -J vqsrIndel.'+self.path.split('/')[-1]+'\n'
	output += '#SBATCH -e '+self.path+'/stderr.vqsrIndel.txt'+'\n'
	output += '#SBATCH -o '+self.path+'/stdout.vqsrIndel.txt'+'\n'
	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass
	output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo "$(date) Running on: $(hostname)" >&2'+'\n'
        output += 'echo "VQSR" '+'\n'
        output += 'java -Xmx24g -jar '+AnalysisPipe.gatkLocation+' \\'+'\n'
        output += '   -T VariantRecalibrator \\'+'\n'
        output += '   -nt 16 \\'+'\n'
        output += '   -R '+AnalysisPipe.bowtie2Reference+' \\'+'\n'
        output += '   -input '+self.path+'/recalibrated_snps_raw_indels.vcf \\'+'\n'
        output += '   -recalFile '+self.path+'/indels.raw.recal \\'+'\n'
        output += '   -tranchesFile '+self.path+'/indels.raw.tranches \\'+'\n'
        output += '   -resource:mills,known=true,training=true,truth=true,prior=12.0 '+AnalysisPipe.gatkBundleLocation+'/Mills_and_1000G_gold_standard.indels.b37.vcf \\'+'\n'
        output += '   -an DP -an FS -an MQRankSum -an ReadPosRankSum \\'+'\n'
        output += '   -tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0 \\'+'\n'
        output += '   --maxGaussians 4 \\'+'\n'
        output += '   -rscriptFile '+self.path+'/recalibrate_INDEL_plots.R \\'+'\n'
        output += '   -mode INDEL'+'\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'
        output += 'wait'+'\n'
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo "---> apply recal <---"'+'\n'
        output += 'java -Xmx72g -jar '+AnalysisPipe.gatkLocation+' \\'+'\n'
        output += '   -nt 16 \\'+'\n'
        output += '   -T ApplyRecalibration \\'+'\n'
        output += '   -R '+AnalysisPipe.bowtie2Reference+' \\'+'\n'
        output += '   -input '+self.path+'/recalibrated_snps_raw_indels.vcf \\'+'\n'
        output += '   -tranchesFile '+self.path+'/indels.raw.tranches \\'+'\n'
        output += '   -recalFile '+self.path+'/indels.raw.recal \\'+'\n'
        output += '   -o '+self.path+'/indels.recalibrated.vcf \\'+'\n'
        output += '   --ts_filter_level 99.0 \\'+'\n'
        output += '   -mode INDEL'+'\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'
        output += 'wait'+'\n'
        output += 'echo $file;'+'\n'
        output += 'echo "$(date) AllDone"'+'\n'
        output += 'echo "$(date) AllDone" >&2'+'\n'
        with open(self.path+'/vqsrIndel.sh','w') as outfile: outfile.write(output)
    
    #def makeScript_vqsr_snps(self):
        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 16 -p node'+'\n'
	output += '#SBATCH -t 240:00:00'+'\n'
	output += '#SBATCH -J vqsrSnps.'+self.path.split('/')[-1]+'\n'
	output += '#SBATCH -e '+self.path+'/stderr.vqsrSnps.txt'+'\n'
	output += '#SBATCH -o '+self.path+'/stdout.vqsrSnps.txt'+'\n'
	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo "$(date) Running on: $(hostname)" >&2'+'\n'
        output += 'echo "---> Variant filtration <---" '+'\n'
        output += 'echo "---> Variant filtration <---" 1>&2'+'\n'
        output += 'java -Xmx60g -jar '+AnalysisPipe.gatkLocation+' -T VariantFiltration '
        output += '-R '+AnalysisPipe.bowtie2Reference+' '
        output += '--filterExpression  "QUAL < 50.0" --filterName "LowQual" '
        output += '--filterExpression "FS > 60" --filterName "FisherSB" '# for SNPs
        #output += '--filterExpression "FS > 200" --filterName "FIsherSB" '# for INDELs
        output += '--filterExpression "QD<1.0" --filterName "QualByDepth" '
        output += '--filterExpression "(MQ0 >= 4 && ((MQ0/(1.0 * DP)) > 0.1))" --filterName "FUBAR" '
        output += '--variant '+self.path+'/raw_variants.vcf '
        output += '--out '+self.path+'/filtered_variants.vcf '
        output += '\n'
        output += 'echo "Variant filtration done. $(date) Running on: $(hostname)"'+'\n'
        output += 'echo "---> VQSR <---" '+'\n'
        output += 'echo "---> VQSR <---" 1>&2'+'\n'
        output += 'java -Xmx60g -jar '+AnalysisPipe.gatkLocation+' -T VariantRecalibrator '
        output += '-R '+AnalysisPipe.bowtie2Reference+' -input '+self.path+'/filtered_variants.vcf'
        output += ' -resource:hapmap,known=false,training=true,truth=true,prior=15.0'+' '+AnalysisPipe.gatkBundleLocation+'/hapmap_3.3.b37.vcf'
        output += ' -resource:omni,known=false,training=true,truth=false,prior=12.0' +' '+AnalysisPipe.gatkBundleLocation+'/1000G_omni2.5.b37.vcf'
        output += ' -resource:dbsnp,known=true,training=false,truth=false,prior=6.0' +' '+AnalysisPipe.gatkBundleLocation+'/dbsnp_138.b37.vcf'
        output += ' -an QD -an MQRankSum -an ReadPosRankSum -an FS -an DP -mode SNP'
        output += ' -tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0'
        output += ' -minNumBad 1000'
        output += ' -recalFile '+self.path+'/recalibrate_SNP.recal '
        output += ' -tranchesFile '+self.path+'/recalibrate_SNP.tranches '
        output += ' -rscriptFile '+self.path+'/recalibrate_SNP_plots.R '
        output += '\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'
        output += 'wait'+'\n'
        output += 'echo "-----"'+'\n'
        output +='\n'
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output +='\n'
        output +='\n'
        output += 'echo "---> apply recal <---" '+'\n'
        output += 'echo "---> apply recal <---" 1>&2'+'\n'
        output +='\n'
        output += 'java -Xmx60g -jar '+AnalysisPipe.gatkLocation+' '
        output += '-T ApplyRecalibration -R '+AnalysisPipe.bowtie2Reference+' '
        output += ' -input '+self.path+'/filtered_variants.vcf '
        output += ' -recalFile '+self.path+'/recalibrate_SNP.recal '
        output += ' -tranchesFile '+self.path+'/recalibrate_SNP.tranches '
        output += ' -o '+self.path+'/recalibrated_snps_raw_indels.vcf '
        output += ' --ts_filter_level 99.0 -mode SNP'+'\n'
        output +='\n'
        output +='\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'
        output += 'wait'+'\n'
        output += 'echo "$(date) AllDone"'+'\n'
        output += 'echo "$(date) AllDone" >&2'+'\n'
        with open(self.path+'/vqsrSnps.sh','w') as outfile: outfile.write(output)
	return

    def readVcfToMem(self,):
	
	import os
	import sys
	import time
	
	self.variations = {}
	AnalysisPipe.nonDbSNPVarCount = 0

	self.vcfLineCount = 0
	if os.path.exists(self.path+'/indels.recalibrated.vcf'): infilename = self.path+'/indels.recalibrated.vcf'
	elif os.path.exists(self.path+'/recalibrated_snps_raw_indels.vcf'): infilename = self.path+'/recalibrated_snps_raw_indels.vcf'
	else:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# cannot find the vfc file has the variant calling and recalibration finished without errors?\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit()
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading vcf to memory.\n')
	lastChomosomeRead = None
	with open(infilename) as infile:
	    for line in infile:
		#if sum([len(self.variations[chrom]) for chrom in self.variations]) >= 1e4:
		#    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Warning running on a subset of variants debbugging purpose.\n')
		#    break

                if line[0] == "#": #THIS LINE IS HEADER
                    if line[1] == 'C': #THIS LINE IS HEADER WITH SAMPLE INFO
                        line = line.rstrip().split('\t')
                        sampleNames = [line[i] for i in range(9,len(line))]
                    continue

		self.vcfLineCount +=1

		variation = VcfEntry(line,sampleNames)

		if variation.passFilter == 'PASS' and not variation.isIndel:
		    try:             self.variations[variation.chrom].append(variation)
		    except KeyError: self.variations[variation.chrom] = [variation]
		    if variation.chrom != lastChomosomeRead: AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# started loading variations from '+str(variation.chrom)+'.\n')
		    lastChomosomeRead = variation.chrom
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# vcf file loaded.\n')

    def parseVcfToIdentify(self):

	import sys
	import operator
	import time

	patId = None
	donId = None
	samplesbyName = {}
	samplesNamesNoPD = []
	samplesbyId = {}
	refSamplesFirst = []
	samples = []
	for sample in AnalysisPipe.database.getSamples():
	    samplesbyId[sample.id]=sample
	    samplesbyName[sample.name]=sample
	    samples.append(sample)
	    if sample.refType: refSamplesFirst.append(sample)
	    if sample.refType == 'Patient': patId = sample.name
	    if sample.refType == 'Donor': donId = sample.name
	    if not sample.refType: samplesNamesNoPD.append(sample.name)
	#for sampleId, sample in sorted(samplesbyId.iteritems(), key=operator.itemgetter(0)):
	#    if sample not in refSamplesFirst: refSamplesFirst.append(sample)
	for sampleName in sorted_nicely(samplesNamesNoPD):
	    sample = samplesbyName[sampleName]
	    if sample not in refSamplesFirst: refSamplesFirst.append(sample)
	if not patId: patId = raw_input('cannot find the Patient sample please give a Sample name to use as Patient: ')
	if not donId: donId = raw_input('cannot find the Donor sample please give a Sample name to use as Donor: ')
	coloredTableOut = open(self.path+'/SNPidentificationTable.colored.txt','w')

	print 'pat='+patId, 'don='+donId
	
	if not patId or not donId:
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Patient and/or Donor id missing, skipping the identification of cell origin.\n')
	    return

	identifcationPercentages = {}
	for sample in samples: identifcationPercentages[sample.name] = {'totalVariationCount':0,'Patient':0,'Donor':0,'mix':0,'other':0}
	
	totalVariationCount = 0
	informativeVariationCount = 0

	heatmapLists = {'chromosome':[],'position':[],'SNP_ID':[]}
	heatmapLists.update({sample.name:[] for sample in refSamplesFirst})
	
	for sample in refSamplesFirst: sample.onDiskVariantsList = open(sample.dataPath+'/identificationVariantsList.tsv','w')
	for sample in refSamplesFirst: sample.onDiskVariantsSummary = open(sample.dataPath+'/identificationVariantsSummary.txt','w')
	for sample in refSamplesFirst: sample.onDiskclassification = open(sample.dataPath+'/sampleClassification.txt','w')

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Parsing variation per chromosome.\n')
	for chrom in sorted_nicely(self.variations.keys()):#,variations in sorted(self.variations.iteritems(), key=operator.itemgetter(0)):
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# now at '+str(chrom)+'.\n')
	    for variation in self.variations[ chrom ]:

		totalVariationCount += 1

		# patient and donor filters
		patDP = bool(variation.perSampleInfo[patId]['DP'] >= int(AnalysisPipe.settings.patientAndDonorDPcutoff))
		donDP = bool(variation.perSampleInfo[donId]['DP'] >=  int(AnalysisPipe.settings.patientAndDonorDPcutoff))
		patQual = bool(variation.perSampleInfo[patId]['GQ'] >=  int(AnalysisPipe.settings.patientAndDonorGQcutoff))
		donQual = bool(variation.perSampleInfo[donId]['GQ'] >=  int(AnalysisPipe.settings.patientAndDonorGQcutoff))
		patHom = bool(variation.perSampleInfo[patId]['GT'].split('/').count(variation.perSampleInfo[patId]['GT'].split('/')[0])==2)
		donHom = bool(variation.perSampleInfo[donId]['GT'].split('/').count(variation.perSampleInfo[donId]['GT'].split('/')[0])==2)
		same = bool(variation.perSampleInfo[donId]['GT']==variation.perSampleInfo[patId]['GT'])

		if patDP and donDP and donHom and patHom and patQual and donQual and not same:
		    informativeVariationCount += 1
		    identify = {variation.perSampleInfo[patId]['GT']:'\033[34m'+'Patient'+'\033[0m',
				variation.perSampleInfo[donId]['GT']:'\033[95m'+'Donor'+'\033[0m',
				'./.':'\033[90m'+'NoData'+'\033[0m',
				str(variation.perSampleInfo[patId]['GT'].split('/')[0])+'/'+variation.perSampleInfo[donId]['GT'].split('/')[0]:'\033[93m'+'mix'+'\033[0m',
				str(variation.perSampleInfo[donId]['GT'].split('/')[0])+'/'+variation.perSampleInfo[patId]['GT'].split('/')[0]:'\033[93m'+'mix'+'\033[0m'}
		    identifyNoColor = {variation.perSampleInfo[patId]['GT']:'Patient',
				       variation.perSampleInfo[donId]['GT']:'Donor',
				       './.':'NoData',
				       str(variation.perSampleInfo[patId]['GT'].split('/')[0])+'/'+variation.perSampleInfo[donId]['GT'].split('/')[0]:'mix',
				       str(variation.perSampleInfo[donId]['GT'].split('/')[0])+'/'+variation.perSampleInfo[patId]['GT'].split('/')[0]:'mix'}
		    test=''
		    pfSamples = 0
		    tmpHeatmapLists = {'chromosome':variation.chrom,'position':variation.pos,'SNP_ID':variation.id}
		    tmpHeatmapLists.update({sample.name:None for sample in refSamplesFirst})
		    for sample in refSamplesFirst:
			sampleDP = None
			sampleQual = None

			# per sample filters
			sampleDP   = bool(variation.perSampleInfo[sample.name]['DP'] >= int(AnalysisPipe.settings.sampleIdentificationDPcutoff))
			sampleQual = bool(variation.perSampleInfo[sample.name]['GQ'] >= int(AnalysisPipe.settings.sampleIdentificationGQcutoff))
			if sample.name in [patId, donId]:
			    sampleDP   = bool(variation.perSampleInfo[sample.name]['DP'] >= int(AnalysisPipe.settings.patientAndDonorDPcutoff))
			    sampleQual = bool(variation.perSampleInfo[sample.name]['GQ'] >= int(AnalysisPipe.settings.patientAndDonorGQcutoff))

			if sampleDP and sampleQual:
			    try:
				pfSamples+=1
				test+=sample.name+'='+identify[variation.perSampleInfo[sample.name]['GT']]+'\t'
				identifcationPercentages[sample.name]['totalVariationCount'] += 1
				identifcationPercentages[sample.name][identifyNoColor[variation.perSampleInfo[sample.name]['GT']]] += 1
				if   identifyNoColor[variation.perSampleInfo[sample.name]['GT']] == 'Patient':tmpHeatmapLists[sample.name] = 2
				elif identifyNoColor[variation.perSampleInfo[sample.name]['GT']] == 'Donor':  tmpHeatmapLists[sample.name] = 3
				elif identifyNoColor[variation.perSampleInfo[sample.name]['GT']] == 'mix':    tmpHeatmapLists[sample.name] = 1
			    except KeyError:
				test+=sample.name+'='+'\033[31m'+'other('+variation.perSampleInfo[sample.name]['GT']+')\033[0m\t'
				identifcationPercentages[sample.name]['other']
				tmpHeatmapLists[sample.name] = 4
				#test+=sample.name+'='+'otherGT('+variation.perSampleInfo[sample.name]['GT']+')\t'
			    try:             sample.onDiskVariantsList.write(str(variation.chrom)+'\t'+str(variation.pos)+'\t'+str(variation.id)+'\t'+identifyNoColor[variation.perSampleInfo[sample.name]['GT']]+'\t'+variation.perSampleInfo[sample.name]['GT']+'\t'+str(variation.perSampleInfo[sample.name]['GQ'])+'\t'+str(variation.perSampleInfo[sample.name]['DP'])+'\t'+str(variation.perSampleInfo[sample.name]['AD'])+'\n')
			    except KeyError: sample.onDiskVariantsList.write(str(variation.chrom)+'\t'+str(variation.pos)+'\t'+str(variation.id)+'\t'+'other'                                                    +'\t'+variation.perSampleInfo[sample.name]['GT']+'\t'+str(variation.perSampleInfo[sample.name]['GQ'])+'\t'+str(variation.perSampleInfo[sample.name]['DP'])+'\t'+str(variation.perSampleInfo[sample.name]['AD'])+'\n')
			else:
			    test+=sample.name+'='+'\033[90m'+'LowDPorGQ'+'\033[0m'+'\t'
			    tmpHeatmapLists[sample.name] = 0
		    if pfSamples >= 3:
			coloredTableOut.write( str(variation.chrom)+'\t'+str(variation.pos)+'\t'+str(variation.id)+'\t'+test+'\n')
			heatmapLists['chromosome'].append(variation.chrom)
			heatmapLists['position'].append(variation.pos)
			heatmapLists['SNP_ID'].append(variation.id)
			for sample in refSamplesFirst:
			    heatmapLists[sample.name].append(tmpHeatmapLists[sample.name])
	#coloredTableOut.write('\t\t\t')
	#for sample in refSamplesFirst:
	#    coloredTableOut.write(sampleName+'|'+
	#			  'pat%='+str(percentage(identifcationPercentages[sample.name]['Patient'],identifcationPercentages[sample.name]['totalVariationCount']))+'|'+
	#			  'don%='+str(percentage(identifcationPercentages[sample.name]['Donor'],identifcationPercentages[sample.name]['totalVariationCount']))+'|'+
	#			  'mix%='+str(percentage(identifcationPercentages[sample.name]['mix'],identifcationPercentages[sample.name]['totalVariationCount']))+'|'+
	#			  'other%='+str(percentage(identifcationPercentages[sample.name]['other'],identifcationPercentages[sample.name]['totalVariationCount']))+'|'+
	#			  'total='+str(identifcationPercentages[sample.name]['totalVariationCount'])+
	#			  '\t')
	coloredTableOut.write('\nSAMPLESUMS:')
	coloredTableOut.write('\n\t\t\t')
	for sample in refSamplesFirst:
	    sampleIs = 'Unknown'
	    sample.totalVariations = identifcationPercentages[sample.name]['totalVariationCount']
	    sample.patPercentage = percentage(identifcationPercentages[sample.name]['Patient'],sample.totalVariations)
	    sample.donPercentage = percentage(identifcationPercentages[sample.name]['Donor'],  sample.totalVariations)
	    sample.mixPercentage = percentage(identifcationPercentages[sample.name]['mix'],    sample.totalVariations)
	    sample.otherPercentage = percentage(identifcationPercentages[sample.name]['other'],    sample.totalVariations)
	    
	    if type(sample.patPercentage)==float and sample.patPercentage > 80: sampleIs = 'Patient'
	    if type(sample.donPercentage)==float and sample.donPercentage > 80: sampleIs = 'Donor'
	    if (type(sample.mixPercentage)==float and sample.mixPercentage > 80) or (type(sample.patPercentage)==float and type(sample.donPercentage)==float and type(sample.mixPercentage)==float and sample.patPercentage > 10 and sample.donPercentage > 10 and sample.mixPercentage > 10): sampleIs = 'mix'
	    sample.onDiskclassification.write(sampleIs+'\n')
	    if   sampleIs== 'Patient':sample.onDiskclassification.write(patId+'\n')
	    elif sampleIs == 'Donor':  sample.onDiskclassification.write(donId+'\n')
	    if sample.totalVariations <=5:
		sampleIs += ' LowConf'
		sample.onDiskclassification.write('LowConf'+'\n')
	    sample.classification = sampleIs
	    
	    
	    coloredTableOut.write(sample.name+' '+sampleIs+'\t')
	coloredTableOut.write('\n\t\t\t')
	for sample in refSamplesFirst:
	    coloredTableOut.write('pat%='+str(sample.patPercentage)+''+'\t')
	    sample.onDiskVariantsSummary.write('pat%='+str(sample.patPercentage)+''+'\n')
	coloredTableOut.write('\n\t\t\t')
	for sample in refSamplesFirst:
	    coloredTableOut.write('don%='+str(sample.donPercentage)+''+'\t')
	    sample.onDiskVariantsSummary.write('don%='+str(sample.donPercentage)+''+'\n')
	coloredTableOut.write('\n\t\t\t')
	for sample in refSamplesFirst:
	    coloredTableOut.write('mix%='+str(sample.mixPercentage)+''+'\t')
	    sample.onDiskVariantsSummary.write('mix%='+str(sample.mixPercentage)+''+'\n')
	coloredTableOut.write('\n\t\t\t')
	for sample in refSamplesFirst:
	    coloredTableOut.write('other%='+str(sample.otherPercentage)+''+'\t')
	    sample.onDiskVariantsSummary.write('other%='+str(sample.otherPercentage)+''+'\n')
	coloredTableOut.write('\n\t\t\t')
	for sample in refSamplesFirst:
	    coloredTableOut.write('total='+str(sample.totalVariations)+'\t')
	    sample.onDiskVariantsSummary.write('total='+str(sample.totalVariations)+'\n')
	coloredTableOut.write('\n')

	for sample in refSamplesFirst: sample.onDiskVariantsList.close()
	for sample in refSamplesFirst: sample.onDiskVariantsSummary.close()
	for sample in refSamplesFirst: sample.onDiskclassification.close()
	sys.stdout.write('Identification found '+thousandString(str(informativeVariationCount))+' informative variant out of '+thousandString(str(totalVariationCount))+' parsed variants.\n')
	coloredTableOut.write('Found '+thousandString(str(informativeVariationCount))+' informative variant out of '+thousandString(str(totalVariationCount))+' parsed variants.\n')
	coloredTableOut.close()
	self.makeIdentificationHeatmap(donId,patId,heatmapLists,refSamplesFirst)
	self.makeIdentificationieChart(donId,patId,heatmapLists,refSamplesFirst)
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# All done.\n')

    def makeIdentificationHeatmap(self,donId,patId,heatmapLists,refSamplesFirst):
        
	#
	# Import packages
	#
	import sys
	import re 
	import matplotlib
	import matplotlib.colors as col
	import matplotlib.cm as cm
	import matplotlib.pyplot as plt
	import pandas as pd
	import numpy as np
	import time

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# making heatmap for identification of cell origin.\n')
	patlable = 'Patient'
	donlable = 'Donor'
	mixlable = 'Mixed'
	patcolor = '#1975FF'
	doncolor = '#FF3399'
	mixcolor = '#CC9900'
	othercolor = "#e70000"
	tmpdict = {patId:patId.split('.')[0]+patlable,donId:donId.split('.')[0]+donlable}
	labels = [sample.name if (sample.name != donId and sample.name != patId) else tmpdict[sample.name] for sample in refSamplesFirst]
    
	## Make heat map
	#print 'making heatmap'
	
	totalHeatmapLength = len(heatmapLists['SNP_ID'])
	columnsPerRow = 150
	rows = totalHeatmapLength/columnsPerRow+1
	lengthOfLast = totalHeatmapLength%columnsPerRow
	start = 0
	end = columnsPerRow
	
	fig, axes = plt.subplots(rows,1)
	if rows==1:axes=[axes]
	plots = []
	for row in range(rows):
	    tmpHeatmapLists = {}
	    values = set([0])
	    for sample in refSamplesFirst:
		tmpHeatmapLists[sample.name] = pd.Series(heatmapLists[sample.name][start:end],index=heatmapLists['SNP_ID'][start:end])
		values.union(set(heatmapLists[sample.name][start:end]))
		if row == max(range(rows)):
		    tmpHeatmapLists[sample.name] = pd.Series(heatmapLists[sample.name][start:end]+[0 for i in range(150-lengthOfLast)],index=heatmapLists['SNP_ID'][start:end]+['' for i in range(150-lengthOfLast)])
		    
	    
	    start += columnsPerRow
	    end   += columnsPerRow
	    if end > len(heatmapLists['SNP_ID']):end = len(heatmapLists['SNP_ID'])
	    
	    data = pd.DataFrame(tmpHeatmapLists)
	    #print data
	    data = pd.DataFrame(data, columns=[sample.name for sample in refSamplesFirst])
	    #data = pd.DataFrame(data, index=['chromosome','position','SNP_ID']+[sample.name for sample in refSamplesFirst])
	    #data = pd.DataFrame(data, index=heatmapLists['SNP_ID'])
	    data = data.transpose()
	    #print data
	    
	    #print 'plotting'
	    # define individual colors as hex values
	    cpool2 = ['#D0D0D0',mixcolor,patcolor,doncolor]#,othercolor]
	    if max(values) == 4: cpool2 = ['#D0D0D0',mixcolor,patcolor,doncolor,othercolor]
	    cmap3 = col.ListedColormap(cpool2, 'indexed')
	    cm.register_cmap(cmap=cmap3)
	    
	    # Plot it out
	    plots.append(axes[row].pcolor(data, cmap=cmap3))
	    #heatmap = ax.pcolor(data, cmap=cmap3)#,shading='faceted',edgecolors='white',linewidths=1)#, alpha=0.8)
	    #ax.patch.set_hatch('x')
	    
	    # turn off the frame
	    axes[row].set_frame_on(False)
	    
	    # put the major ticks at the middle of each cell
	    axes[row].set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)
	    axes[row].set_xticks(np.arange(data.shape[1]) + 0.5, minor=False)
	    
	    # want a more natural, table-like display
	    axes[row].invert_yaxis()
	    axes[row].xaxis.tick_top()
	    
	    # note I could have used nba_sort.columns but made "labels" instead
	    axes[row].set_xticklabels(data.columns, minor=False,fontsize=10,rotation=90)
	    axes[row].set_yticklabels(labels, minor=False,fontsize=10)
	    
	    axes[row].grid(False)
	    
	    # Turn off all the ticks
	    axes[row] = plt.gca()
	    
	    for t in axes[row].xaxis.get_major_ticks():
		#t.label.set_fontsize(5)
		#t[2].lable.set_rotation('vertical')
		t.tick1On = False
		t.tick2On = False
	    
	    for t in axes[row].yaxis.get_major_ticks():
		#t.label.set_fontsize(5)
		t.tick1On = False
		t.tick2On = False

	# Format
	fig = plt.gcf()
	#fig.set_size_inches(len(heatmapLists['SNP_ID'])/5.0,len(refSamplesFirst)/3.0)
	fig.set_size_inches(columnsPerRow/5.0,rows*len(refSamplesFirst)/3.0)

	# rotate the
	plt.xticks(rotation=90)
	
	#print 'saving'
	plt.savefig(AnalysisPipe.path+'/graphics/'+'heatmap'+'.pdrd='+str(AnalysisPipe.settings.patientAndDonorDPcutoff)+'.pdgq='+str(AnalysisPipe.settings.patientAndDonorGQcutoff)+'.scrd='+str(AnalysisPipe.settings.sampleIdentificationDPcutoff)+'.scgq='+str(AnalysisPipe.settings.sampleIdentificationGQcutoff)+".pdf",dpi=100,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/'+'heatmap'+'.pdrd='+str(AnalysisPipe.settings.patientAndDonorDPcutoff)+'.pdgq='+str(AnalysisPipe.settings.patientAndDonorGQcutoff)+'.scrd='+str(AnalysisPipe.settings.sampleIdentificationDPcutoff)+'.scgq='+str(AnalysisPipe.settings.sampleIdentificationGQcutoff)+".png",dpi=100,bbox_inches='tight')
	plt.close()

    def makeIdentificationieChart(self,donId,patId,heatmapLists,refSamplesFirst ):
	import sys
	import re 
	import matplotlib
	import matplotlib.colors as col
	import matplotlib.cm as cm
	import matplotlib.pyplot as plt
	import pandas as pd
	import numpy as np
	import time
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# making piechart for identification of cell origin.\n')

	patlable = 'Patient'
	donlable = 'Donor'
	mixlable = 'Mixed'
	patcolor = '#1975FF'
	doncolor = '#FF3399'
	mixcolor = '#CC9900'
	othercolor='#e70000'

	fakesample = Sample(sampleName='fakeSample',sampleId='999',refType=None)
	fakesample.patPercentage = 0.25
	fakesample.donPercentage = 0.25
	fakesample.mixPercentage = 0.25
	fakesample.otherPercentage = 0.25
	fakesample.totalVariations = 1000
	fakesample.classification = 'fakeSample'

	from matplotlib.gridspec import GridSpec
	fig = plt.figure(figsize=(min((len(refSamplesFirst)+1)*3,32760/300), 4), dpi=300)
	the_grid = GridSpec(1,(len(refSamplesFirst)+1))
	sampleNumber = 0
	for sample in refSamplesFirst+[fakesample]:
	    labels = []
	    colors = []
	    sizes = []
	    ax = plt.subplot(the_grid[0, sampleNumber], aspect=1)
	    ax.set_title(sample.name+'\n('+str(sample.totalVariations)+' variants)\n'+sample.classification)
	    if sample.totalVariations:
		patfrac = sample.patPercentage# round(100*float(piechart[sample]['PAT'])/float(piechart[sample]['TOTAL']),2)
		donfrac = sample.donPercentage# round(100*float(piechart[sample]['DON'])/float(piechart[sample]['TOTAL']),2)
		mixfrac = sample.mixPercentage#round(100*float(piechart[sample]['MIX'])/float(piechart[sample]['TOTAL']),2)
		otherfrac = sample.otherPercentage
	    else:
		patfrac = 0
		donfrac = 0
		mixfrac = 0
		otherfrac = 0
	    if patfrac != 0 and patfrac != 'NA':
		labels.append(patlable)
		colors.append(patcolor)
		sizes.append(patfrac)
	    if donfrac != 0 and donfrac != 'NA':
		labels.append(donlable)
		colors.append(doncolor)
		sizes.append(donfrac)
	    if mixfrac != 0 and mixfrac != 'NA':
		labels.append(mixlable)
		colors.append(mixcolor)
		sizes.append(mixfrac)
	    if otherfrac != 0 and otherfrac != 'NA':
		labels.append('other')
		colors.append(othercolor)
		sizes.append(otherfrac)
	    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=45)
	    sampleNumber += 1
	
	plt.axis('equal')
	plt.savefig(AnalysisPipe.path+'/graphics/'+'pieChart'+
		    '.pdrd='+str(AnalysisPipe.settings.patientAndDonorDPcutoff)+
		    '.pdgq='+str(AnalysisPipe.settings.patientAndDonorGQcutoff)+
		    '.scrd='+str(AnalysisPipe.settings.sampleIdentificationDPcutoff)+
		    '.scgq='+str(AnalysisPipe.settings.sampleIdentificationGQcutoff)+
		    ".pdf",dpi=100,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/'+'pieChart'+
		    '.pdrd='+str(AnalysisPipe.settings.patientAndDonorDPcutoff)+
		    '.pdgq='+str(AnalysisPipe.settings.patientAndDonorGQcutoff)+
		    '.scrd='+str(AnalysisPipe.settings.sampleIdentificationDPcutoff)+
		    '.scgq='+str(AnalysisPipe.settings.sampleIdentificationGQcutoff)+
		    ".png",dpi=100,bbox_inches='tight')

	plt.close()    

    def parseVcfForADOrate(self):

	import sys
	import operator
	import time

	patId = None
	donId = None
	samplesbyName = {}
	samplesNamesNoPD = []
	samplesbyId = {}
	refSamplesFirst = []
	samples = []
	refSamples = []
	for sample in AnalysisPipe.database.getSamples():
	    samplesbyId[sample.id]=sample
	    samplesbyName[sample.name]=sample
	    samples.append(sample)
	    if sample.refType:
		refSamplesFirst.append(sample)
		refSamples.append(sample)
	    if sample.refType == 'Patient': patId = sample.name
	    if sample.refType == 'Donor': donId = sample.name
	    if not sample.refType: samplesNamesNoPD.append(sample.name)

	for sampleName in sorted_nicely(samplesNamesNoPD):
	    sample = samplesbyName[sampleName]
	    if sample not in refSamplesFirst: refSamplesFirst.append(sample)

	mainReferenceSample = 'no sample name here'
	if len(refSamples) == 1:
	    mainReferenceSample = refSamples[0].name
	elif len(refSamples) > 1:
	    if patId:
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# using patient sample ' +patId+ ' as reference sample for ADO rate estimation etc.\n')
		mainReferenceSample = patId
	    else: mainReferenceSample = raw_input('cannot find which sample to use as reference in ADO rate estimation etc\nfollowing reference samples are around:\n'+'\n'.join([sample.name for sample in refSamples])+'\nplease give a Sample name to use as reference: ')
	elif len(refSamples) == 0: mainReferenceSample = raw_input('cannot find any sample to use as reference in ADO rate estimation etc\nfollowing samples are around:\n'+'\n'.join(sorted_nicely(samplesbyName.keys()))+'\nplease give a Sample name to use as reference: ')

	try: mainReferenceSample = samplesbyName[mainReferenceSample]
	except KeyError:
	    msg = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# sample name ' +mainReferenceSample+ ' is not valid exiting.\n'
	    sys.stderr.write(msg)
	    AnalysisPipe.logfile.write(msg)
	    sys.exit()

	referenceSamples = [mainReferenceSample]
	if patId and donId:
	    referenceSamples.append(samplesbyName[donId])
	for mainReferenceSample in referenceSamples:
	    for sample in refSamplesFirst:
		sample.onDiskVariantsList = open(sample.dataPath+'/adoVariantsList.ref='+mainReferenceSample.name+'.tsv','w')
		sample.onDiskVariantsSummary = open(sample.dataPath+'/adoVariantsSummary.ref='+mainReferenceSample.name+'.txt','w')
	    coloredTableOut = open(self.path+'/adoRate.colored.ref='+mainReferenceSample.name+'.txt','w')
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# using sample ' +mainReferenceSample.name+ ' as reference sample for ADO rate estimation etc.\n')
    
    
	    identifcationPercentages = {}
	    for sample in samples: identifcationPercentages[sample.name] = {'totalVariationCount':0,'correct':0,'dropout':0,'other':0}
    
	    totalVariationCount = 0
	    informativeVariationCount = 0
    
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Parsing variation per chromosome.\n')
	    for chrom in sorted_nicely(self.variations.keys()):#,variations in sorted(self.variations.iteritems(), key=operator.itemgetter(0)):
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# now at '+str(chrom)+'.\n')
		for variation in self.variations[ chrom ]:
    
		    totalVariationCount += 1
    
		    # patient and donor filters
		    refDP = bool(variation.perSampleInfo[mainReferenceSample.name]['DP'] >= int(AnalysisPipe.settings.referenceSampleDPcutoff))
		    refQual = bool(variation.perSampleInfo[mainReferenceSample.name]['GQ'] >= int(AnalysisPipe.settings.referenceSampleGQcutoff))
		    refHom = bool(variation.perSampleInfo[mainReferenceSample.name]['GT'].split('/').count(variation.perSampleInfo[mainReferenceSample.name]['GT'].split('/')[0])==2)
    
		    if refDP and not refHom and refQual:
			informativeVariationCount += 1
			identify = {variation.perSampleInfo[mainReferenceSample.name]['GT']:'\033[34m'+'correct'+'\033[0m',
				    './.':'\033[90m'+'NoData'+'\033[0m',
				    str(variation.perSampleInfo[mainReferenceSample.name]['GT'].split('/')[0])+'/'+variation.perSampleInfo[mainReferenceSample.name]['GT'].split('/')[0]:'\033[93m'+'dropout'+'\033[0m',
				    str(variation.perSampleInfo[mainReferenceSample.name]['GT'].split('/')[1])+'/'+variation.perSampleInfo[mainReferenceSample.name]['GT'].split('/')[1]:'\033[93m'+'dropout'+'\033[0m'}
			identifyNoColor = {variation.perSampleInfo[mainReferenceSample.name]['GT']:'correct',
				    './.':'NoData',
				    str(variation.perSampleInfo[mainReferenceSample.name]['GT'].split('/')[0])+'/'+variation.perSampleInfo[mainReferenceSample.name]['GT'].split('/')[0]:'dropout',
				    str(variation.perSampleInfo[mainReferenceSample.name]['GT'].split('/')[1])+'/'+variation.perSampleInfo[mainReferenceSample.name]['GT'].split('/')[1]:'dropout'}
			test=''
			pfSamples = 0
			for sample in refSamplesFirst:
			    sampleDP = None
			    sampleQual = None
    
			    # per sample filters
			    sampleDP   = bool(variation.perSampleInfo[sample.name]['DP'] >= int(AnalysisPipe.settings.sampleADOestDPcutoff))
			    sampleQual = bool(variation.perSampleInfo[sample.name]['GQ'] >= int(AnalysisPipe.settings.sampleADOestGQcutoff))
    
			    if sampleDP and sampleQual:
				try:
				    test+=sample.name+'='+identify[variation.perSampleInfo[sample.name]['GT']]+'('+variation.perSampleInfo[sample.name]['GT']+')\t'
				    identifcationPercentages[sample.name][identifyNoColor[variation.perSampleInfo[sample.name]['GT']]] += 1
				    identifcationPercentages[sample.name]['totalVariationCount'] += 1
				    pfSamples+=1
				except KeyError:
				    test+=sample.name+'='+'\033[31m''other('+variation.perSampleInfo[sample.name]['GT']+')\033[0m\t'
				    identifcationPercentages[sample.name]['other']

				try:             sample.onDiskVariantsList.write(str(variation.chrom)+'\t'+str(variation.pos)+'\t'+str(variation.id)+'\t'+identifyNoColor[variation.perSampleInfo[sample.name]['GT']]+'\t'+variation.perSampleInfo[sample.name]['GT']+'\t'+str(variation.perSampleInfo[sample.name]['GQ'])+'\t'+str(variation.perSampleInfo[sample.name]['DP'])+'\t'+str(variation.perSampleInfo[sample.name]['AD'])+'\n')
				except KeyError: sample.onDiskVariantsList.write(str(variation.chrom)+'\t'+str(variation.pos)+'\t'+str(variation.id)+'\t'+'other'                                                    +'\t'+variation.perSampleInfo[sample.name]['GT']+'\t'+str(variation.perSampleInfo[sample.name]['GQ'])+'\t'+str(variation.perSampleInfo[sample.name]['DP'])+'\t'+str(variation.perSampleInfo[sample.name]['AD'])+'\n')

			    else: test+=sample.name+'='+'\033[90m'+'LowDPorGQ'+'\033[0m'+'\t'
			if pfSamples >= 3: coloredTableOut.write( str(variation.chrom)+'\t'+str(variation.pos)+'\t'+str(variation.id)+'\t'+variation.perSampleInfo[mainReferenceSample.name]['GT']+'\t'+test+'\n')
    
	    coloredTableOut.write('\nSAMPLESUMS:')
	    coloredTableOut.write('\n\t\t\t\t')
	    for sample in refSamplesFirst:
		sampleIs = 'Unknown'
		sample.correctPercentage = percentage(identifcationPercentages[sample.name]['correct'],identifcationPercentages[sample.name]['totalVariationCount'])
		sample.adoPercentage = percentage(identifcationPercentages[sample.name]['dropout'],  identifcationPercentages[sample.name]['totalVariationCount'])
		#print sample.name
		#print sample.name,'-correct->',identifcationPercentages[sample.name]['correct']
		#print sample.name,'-dropout->',identifcationPercentages[sample.name]['dropout']
		#print sample.name,'---total->',identifcationPercentages[sample.name]['totalVariationCount']

		coloredTableOut.write(sample.name+'\t')
	    coloredTableOut.write('\n\t\t\t\t')
	    for sample in refSamplesFirst:
		coloredTableOut.write('correct%='+str(sample.correctPercentage)+''+'\t')
		sample.onDiskVariantsSummary.write('correct%='+str(sample.correctPercentage)+''+'\n')
	    coloredTableOut.write('\n\t\t\t\t')
	    for sample in refSamplesFirst:
		coloredTableOut.write('ADO%='+str(sample.adoPercentage)+''+'\t')
		sample.onDiskVariantsSummary.write('ADO%='+str(sample.adoPercentage)+''+'\n')
	    coloredTableOut.write('\n\t\t\t\t')
	    for sample in refSamplesFirst:
		coloredTableOut.write('other%='+str(percentage(identifcationPercentages[sample.name]['other'],identifcationPercentages[sample.name]['totalVariationCount']))+''+'\t')
		sample.onDiskVariantsSummary.write('other%='+str(percentage(identifcationPercentages[sample.name]['other'],identifcationPercentages[sample.name]['totalVariationCount']))+''+'\n')
	    coloredTableOut.write('\n\t\t\t\t')
	    for sample in refSamplesFirst:
		coloredTableOut.write('total='+str(identifcationPercentages[sample.name]['totalVariationCount'])+'\t')
		sample.onDiskVariantsSummary.write('total='+str(identifcationPercentages[sample.name]['totalVariationCount'])+'\n')
	    coloredTableOut.write('\n')
    
	    sys.stdout.write('ADOcheck: Found '+thousandString(str(informativeVariationCount))+' informative variant out of '+thousandString(str(totalVariationCount))+' parsed variants.\n')
	    coloredTableOut.write('Found '+thousandString(str(informativeVariationCount))+' informative variant out of '+thousandString(str(totalVariationCount))+' parsed variants.\n')
	    coloredTableOut.close()
	for sample in refSamplesFirst: sample.onDiskVariantsList.close()
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# All done.\n')

    def variationCheck(self):
	self.readVcfToMem()
	self.pairwiseVariantConcoordance()
	return
	self.parseVcfToIdentify()
	self.parseVcfForADOrate()
	self.parseVcfForBetwenSampleVariation()
	for sample in AnalysisPipe.database.getSamples():
	    print sample.name
	    sample.getSNPchunkyness()

    def pairwiseVariantConcoordance(self, ):
	import itertools
	import sys
	import operator
	import time
	samplesbyName = {}
	samplesNamesNoPD = []
	samplesbyId = {}
	refSamples = []
	refSamplesFirst = []
	samples = []
	for sample in AnalysisPipe.database.getSamples():
	    if sample.name[-1]== "1":
		samplesbyId[sample.id]=sample
		samplesbyName[sample.name]=sample
		samples.append(sample)
		if sample.refType:
		    refSamplesFirst.append(sample)
		    refSamples.append(sample)
		if sample.refType == 'Patient': patId = sample.name
		if sample.refType == 'Donor': donId = sample.name
		if not sample.refType: samplesNamesNoPD.append(sample.name)
	for sampleName in sorted_nicely(samplesNamesNoPD):
	    sample = samplesbyName[sampleName]
	    if sample not in refSamplesFirst: refSamplesFirst.append(sample)
	combos = list(itertools.combinations(samples,2))
	comboResults = {combo:{'commonVars':0,'agree':0} for combo in combos}
	print "#TEST#"
	subsets = {}
	for i in range(len(samples)):
	    for combo in itertools.combinations(samples,i+1):
		#print [sample.name for sample in combo]
		subsets[combo] = {'totalCount':0}
	print len(subsets)
	print "#TEST#"
	
	mainReferenceSample = 'no sample name here'
	if len(refSamples) == 1:
	    mainReferenceSample = refSamples[0].name
	elif len(refSamples) > 1:
		mainReferenceSample = raw_input('cannot find which sample to use as reference in ADO rate estimation etc\nfollowing reference samples are around:\n'+'\n'.join([sample.name for sample in refSamples])+'\nplease give a Sample name to use as reference: ')
	elif len(refSamples) == 0:
		mainReferenceSample = raw_input('cannot find any sample to use as reference in ADO rate estimation etc\nfollowing samples are around:\n'+'\n'.join(sorted_nicely(samplesbyName.keys()))+'\nplease give a Sample name to use as reference: ')

	try: mainReferenceSample = samplesbyName[mainReferenceSample]
	except KeyError:
	    msg = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# sample name ' +mainReferenceSample+ ' is not valid exiting.\n'
	    sys.stderr.write(msg)
	    AnalysisPipe.logfile.write(msg)
	    sys.exit()
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# using sample ' +mainReferenceSample.name+ ' as reference sample for variant concoordance check.\n')
	#referenceSamples = [mainReferenceSample]
	#if patId and donId:
	#    referenceSamples.append(samplesbyName[donId])
	
	totalVariationCount = 0
	informativeVariationCount = 0
	pairwiseBase = 0
	#sampleNameWarning = False
	#print 'chrom\tpos\t'+'\t'.join([sample.name for sample in refSamplesFirst])

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Parsing variation per chromosome.\n')
	for chrom in sorted_nicely(self.variations.keys()):#,variations in sorted(self.variations.iteritems(), key=operator.itemgetter(0)):
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# now at '+str(chrom)+'.\n')
	    for variation in self.variations[ chrom ]:
		#if set(variation.perSampleInfo.keys()) != set(refSamplesFirst):
		#    if not sampleNameWarning:
		#	sampleNameWarning = True
		#	msg = '#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# The sample names in the vcf does not match the ones in the database, using vcf sample names...\n'
		#	AnalysisPipe.logfile.write(msg)
		#	sys.stderr.write(msg)
		#	refSamplesFirst = [sample for sample in refSamplesFirst if sample.name in variation.perSampleInfo.keys()]
		#	#print 'chrom\tpos\t'+'\t'.join([sample.name for sample in refSamplesFirst])

		totalVariationCount += 1

		# variation filters
		NotIndbSNP = bool( variation.id[0:2] != 'rs' )

		bases = [variation.refBase]+list(variation.altBases)
		outputline = variation.chrom+'\t'+variation.pos+'\t'
		pfSamples = []
		refDP   = bool(variation.perSampleInfo[mainReferenceSample.name]['DP'] >= 10)#int(AnalysisPipe.settings.sampleIdentificationDPcutoff))
		refQual = bool(variation.perSampleInfo[mainReferenceSample.name]['GQ'] >= 30)#int(AnalysisPipe.settings.sampleIdentificationGQcutoff))
		if refDP and refQual:
		    for sample in refSamplesFirst:
			sampleDP = None
			sampleQual = None
    
			# per sample filters
			sampleDP   = bool(variation.perSampleInfo[sample.name]['DP'] >= 10)#int(AnalysisPipe.settings.sampleIdentificationDPcutoff))
			sampleQual = bool(variation.perSampleInfo[sample.name]['GQ'] >= 30)#int(AnalysisPipe.settings.sampleIdentificationGQcutoff))
			sampleSameGTasRef = bool(variation.perSampleInfo[sample.name]['GT']==variation.perSampleInfo[mainReferenceSample.name]['GT'])
			if sampleDP and sampleQual and sampleSameGTasRef: pfSamples.append(sample)
		    if pfSamples:
			subsets[tuple(pfSamples)]['totalCount']+=1
			informativeVariationCount+=1
		
		pfCombos = 0
		for combo in combos:
		    sample1, sample2 = combo
		    
		    samplesDP = None
		    samplesQual = None

		    # per sample filters
		    samplesDP   = bool(variation.perSampleInfo[sample1.name]['DP'] >= 10) and bool(variation.perSampleInfo[sample2.name]['DP'] >= 10)#int(AnalysisPipe.settings.sampleIdentificationDPcutoff))
		    samplesQual = bool(variation.perSampleInfo[sample1.name]['GQ'] >= 30) and bool(variation.perSampleInfo[sample2.name]['GQ'] >= 30)#int(AnalysisPipe.settings.sampleIdentificationGQcutoff))

		    if samplesDP and samplesQual:
			pfCombos+=1
			comboResults[combo]['commonVars']+=1

			if variation.perSampleInfo[sample1.name]['GT'] == variation.perSampleInfo[sample2.name]['GT']:
			    comboResults[combo]['agree']+=1
		if pfCombos: pairwiseBase +=1

	print 'varCount\tSubset'
	for combo in subsets:
	    if subsets[combo]['totalCount']:print str(subsets[combo]['totalCount'])+'\t'+','.join([sample.name for sample in combo])
	print 'total variants with PF calls',informativeVariationCount
	print 'total variants parsed',totalVariationCount
	print 'sample1\tsample2\tcommonPFVars\tagree\t%agree'
	for combo in combos:
	    sample1, sample2 = combo
	    print sample1.name+'\t'+sample2.name+'\t'+str(comboResults[combo]['commonVars'])+'\t'+str(comboResults[combo]['agree'])+'\t'+str(percentage(comboResults[combo]['agree'],comboResults[combo]['commonVars']))+'%'
	print 'based on '+str(pairwiseBase)+' variants'

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# pairwise check done.\n')

    def parseVcfForBetwenSampleVariation(self):

	import sys
	import operator
	import time

	patId = None
	donId = None
	samplesbyName = {}
	samplesNamesNoPD = []
	samplesbyId = {}
	refSamplesFirst = []
	samples = []
	for sample in AnalysisPipe.database.getSamples():
	    samplesbyId[sample.id]=sample
	    samplesbyName[sample.name]=sample
	    samples.append(sample)
	    if sample.refType: refSamplesFirst.append(sample)
	    if sample.refType == 'Patient': patId = sample.name
	    if sample.refType == 'Donor': donId = sample.name
	    if not sample.refType: samplesNamesNoPD.append(sample.name)
	for sampleName in sorted_nicely(samplesNamesNoPD):
	    sample = samplesbyName[sampleName]
	    if sample not in refSamplesFirst: refSamplesFirst.append(sample)
	
	totalVariationCount = 0
	informativeVariationCount = 0
	sampleNameWarning = False
	print 'chrom\tpos\t'+'\t'.join([sample.name for sample in refSamplesFirst])

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Parsing variation per chromosome.\n')
	for chrom in sorted_nicely(self.variations.keys()):#,variations in sorted(self.variations.iteritems(), key=operator.itemgetter(0)):
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# now at '+str(chrom)+'.\n')
	    for variation in self.variations[ chrom ]:
		if set(variation.perSampleInfo.keys()) != set(refSamplesFirst):
		    if not sampleNameWarning:
			sampleNameWarning = True
			msg = '#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# The sample names in the vcf does not match the ones in the database, using vcf sample names...\n'
			AnalysisPipe.logfile.write(msg)
			sys.stderr.write(msg)
			refSamplesFirst = [sample for sample in refSamplesFirst if sample.name in variation.perSampleInfo.keys()]
			print 'chrom\tpos\t'+'\t'.join([sample.name for sample in refSamplesFirst])

		totalVariationCount += 1

		# variation filters
		NotIndbSNP = bool( variation.id[0:2] != 'rs' )
		gts = set([variation.perSampleInfo[sample.name]['GT'] for sample in refSamplesFirst])
		allTheSame = {'./.':False}
		for gt in gts: allTheSame[gt] = True
		allHaveData = bool(not allTheSame['./.'])
		allTheSame = bool( len(allTheSame.keys()) <= 2)

		if NotIndbSNP and not allTheSame:# and allHaveData:
		    test=''
		    pfSamples = 0
		    bases = [variation.refBase]+list(variation.altBases)
		    outputline = variation.chrom+'\t'+variation.pos+'\t'
		    for sample in refSamplesFirst:
			sampleDP = None
			sampleQual = None

			# per sample filters
			sampleDP   = bool(variation.perSampleInfo[sample.name]['DP'] >= 10)#int(AnalysisPipe.settings.sampleIdentificationDPcutoff))
			sampleQual = bool(variation.perSampleInfo[sample.name]['GQ'] >= 30)#int(AnalysisPipe.settings.sampleIdentificationGQcutoff))

			if sampleDP and sampleQual:
			    pfSamples+=1
			    try:
				outputline += '/'.join([bases[int(tmp)] for tmp in variation.perSampleInfo[sample.name]['GT'].split('/')])+'\t'
			    except IndexError:
				outputline += variation.perSampleInfo[sample.name]['GT']+'\t'
			    except ValueError:
				outputline += variation.perSampleInfo[sample.name]['GT']+'\t'
			else:
			    outputline += 'NoPass'+'\t'
			    test+=sample.name+'='+'\033[90m'+'LowDPorGQ'+'\033[0m'+'\t'

		    if pfSamples >= 2:#== len(refSamplesFirst):
			informativeVariationCount += 1
#			for sample in refSamplesFirst:
			print outputline

	sys.stdout.write('Found '+thousandString(str(informativeVariationCount))+' informative variant out of '+thousandString(str(totalVariationCount))+' parsed variants.\n')
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# All done.\n')

    def changeSetting(self,):

	import sys
	import time
	
	try:
	    settings2change = sys.argv[3:]
	    if not settings2change: raise IndexError
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Please supply atleast one variable name and value on your commandline on the format variableName=value or include the term "listsettings" to get a list of available variable names (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	
	if 'listsettings' in settings2change:
	    longestName = max([len(x) for x in AnalysisPipe.settings.explenations.keys()])
	    sys.stderr.write('VariableName:'+''.join(' ' for i in range(longestName-len('VariableName:')))+'\t'+'Description:'+'\n')
	    for variableName, description in AnalysisPipe.settings.explenations.iteritems():
		sys.stderr.write(variableName+''.join(' ' for i in range(longestName-len(variableName)))+'\t'+description+' (currentValue='+str(AnalysisPipe.settings.__dict__[variableName])+')\n')

	for setting2change in settings2change:
	    if setting2change == 'listsettings': continue
	    variableName,value = setting2change.split('=')
	    AnalysisPipe.settings.setVariable(variableName,value)

class VcfEntry(object):
    
    def __init__(self,line,sampleNames):

	import time
	import sys

	self.isIndel = None
	self.vcfLine = line
	line = self.vcfLine.rstrip().split('\t')
	self.perSampleInfo = {sampleName:{} for sampleName in sampleNames}

	for i in range(len(line)):
	    if i == 0:   self.chrom = line[i]
	    elif i == 1: self.pos = line[i]
	    elif i == 2:
		self.id = line[i]
		if line[i] == '.':
		    AnalysisPipe.nonDbSNPVarCount+=1;
		    self .id = 'novel#'+str(AnalysisPipe.nonDbSNPVarCount)
	    elif i == 3: self.refBase = line[i]
	    elif i == 4: self.altBases = line[i].split(',')
	    elif i == 5: self.varQual = line[i]
	    elif i == 6: self.passFilter = line[i]
	    elif i == 7: self.info = line[i]
	    elif i == 8: self.perSampleFormat = line[i]
	    elif i >= 9:
		sampleInfo = line[i]
		infoTags = self.perSampleFormat.split(':')
		if infoTags[0] == 'GT' and sampleInfo[0:2] != './.' or sampleInfo != './.':
		    sampleInfoList = sampleInfo.split(':') #GT:AD:DP:GQ:PL
		    #if sampleInfoList[0] == './.':continue
		    for i2 in range(len(sampleInfoList)): self.perSampleInfo[sampleNames[i-9]][infoTags[i2]] = sampleInfoList[i2]
		    try:
			if self.perSampleInfo[sampleNames[i-9]]['DP'] == '.': self.perSampleInfo[sampleNames[i-9]]['DP'] = 0
			self.perSampleInfo[sampleNames[i-9]]['DP'] = int(self.perSampleInfo[sampleNames[i-9]]['DP'])
		    except KeyError: self.perSampleInfo[sampleNames[i-9]]['DP'] = 0
		    try:
			if self.perSampleInfo[sampleNames[i-9]]['GQ'] == '.': self.perSampleInfo[sampleNames[i-9]]['GQ'] = 0
			self.perSampleInfo[sampleNames[i-9]]['GQ'] = int(self.perSampleInfo[sampleNames[i-9]]['GQ'])
		    except KeyError: self.perSampleInfo[sampleNames[i-9]]['GQ'] = 0
		    if 'GT' not in self.perSampleInfo[sampleNames[i-9]]: self.perSampleInfo[sampleNames[i-9]]['GT'] = 'Unknown'
		    if 'AD' not in self.perSampleInfo[sampleNames[i-9]]: self.perSampleInfo[sampleNames[i-9]]['AD'] = 'Unknown'
		    try:
			if self.perSampleInfo[sampleNames[i-9]]['DP'] == '.': self.perSampleInfo[sampleNames[i-9]]['DP'] = 0
		    except KeyError:
			msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# VcfEntry format error: '+str(infoTags)+' '+str(sampleInfoList)+' '+str(self.perSampleInfo[sampleNames[i-9]])+' '+str(i)+' >'+str(sampleInfo)+'<'+'.\n'
			AnalysisPipe.logfile.write(msg)
			sys.stderr.write(msg)

	for altBase in list(self.altBases):
	    if len(self.refBase) != len(altBase):
		self.type = 'INDEL'
		self.isIndel = True
	    elif not self.isIndel:
		self.type = 'SNV'
		self.isIndel = False

class Database(object):

    def __init__(self, dbPath):
	self.path = dbPath

    def getConnection(self,):
	#
	# Import useful stuff
	#
	import sqlite3
	import sys

	#
	# Create database and set
	#
	try: self.conn = sqlite3.connect(self.path)
	except sqlite3.OperationalError:
	    print 'ERROR: Trouble with the database, plase check your commandline.'
	    sys.exit()
	self.c = self.conn.cursor()

    def commitAndClose(self,):
	#
	# commit changes and close connection
	#
	self.conn.commit()
	self.conn.close()

    def create(self,):
	""" creates the database holding all information used in the analysis """
	
	self.getConnection()
	
	#
	# Create tables
	#
	self.c.execute('''CREATE TABLE runs (startTime,command,commandLine,finishedSuccessfully,masterPid)''')
	self.c.execute('''CREATE TABLE fastqs (filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId,PRIMARY KEY (filePairId))''');
	self.c.execute('''CREATE TABLE settings (variableName,defaultValue,value,setTime,PRIMARY KEY (variableName))''')
	self.c.execute('''CREATE TABLE results (resultName,defaultValue,value,setTime,PRIMARY KEY (resultName))''')
	self.c.execute('''CREATE TABLE samples (sampleId,sampleName,refType,PRIMARY KEY (sampleId))''')
	
	self.commitAndClose()
	
	import os
	os.chmod(self.path, 0664)

    def addToRunsTable(self, startTime, command, commandLine, finishedSuccessfully, masterPid):
	
	self.getConnection()
	
	#
	# check if pid already in database
	#
	t = (masterPid,)
	data = self.c.execute('SELECT masterPid, startTime FROM runs WHERE masterPid=?',t).fetchall()        
	if data:
	    for tmp1,tmp2 in data:

	#
	# if pid and startTime matches update the "finishedSuccessfully" entry
	#
		if tmp1 == masterPid and tmp2 == startTime:
		    values = (startTime, command, commandLine, finishedSuccessfully, masterPid)
		    self.c.execute('UPDATE runs SET finishedSuccessfully=? WHERE masterPid=? AND startTime=?', (finishedSuccessfully,masterPid,startTime))
	
	#
	# if not in the database add a new row
	#
	else:
	    values = (startTime, command, commandLine, finishedSuccessfully, masterPid)
	    self.c.execute('INSERT INTO runs VALUES (?,?,?,?,?)', values)
	
	self.commitAndClose()
	
	return 0

    def addSample(self, newSampleName,newSampleRefType=None):
	
	#
	# Imports
	#
	import sys
	import time
	
	#
	# open connection to database
	#
	self.getConnection()
	
	sampleNames = []
	sampleIds = []
	
	#
	# check if any of the fastqs already in database
	#
	data = self.c.execute('SELECT sampleId,sampleName,refType FROM samples').fetchall()
	if data:
	    for (sampleId,sampleName,sampleRefType) in data:
		#sampleName = sampleName[0]
		sampleNames.append(sampleName)
		sampleIds.append(sampleId)
	    if newSampleName in sampleNames:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# SampleName must be uniq, there is already a sample with name '+newSampleName+' , exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	
	if sampleIds:  sampleId = max(sampleIds)+1
	else:          sampleId = 0 
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Adding sample '+newSampleName+' to database with id '+str(sampleId)+'.\n')
	values = (sampleId,newSampleName,newSampleRefType)
	self.c.execute('INSERT INTO samples VALUES (?,?,?)', values)
	
	sample = Sample(sampleName=newSampleName, sampleId=sampleId,refType=newSampleRefType)
	sample.createDirs()
	
	self.commitAndClose()
	
	return 0

    def getSamples(self):
	#
	# Imports
	#
	import sys
	import time
	
	#
	# open connection to database
	#
	self.getConnection()
	
	refSamples = []
	samples = []
	
	data = self.c.execute('SELECT sampleId,sampleName,refType FROM samples').fetchall()
	if data:
	    for (sampleId,sampleName,sampleRefType) in data:
		if sampleRefType: refSamples.append( Sample(sampleName=sampleName,sampleId=int(sampleId),refType=sampleRefType) )
		else:                samples.append( Sample(sampleName=sampleName,sampleId=int(sampleId),refType=None) )
	
	self.commitAndClose()
	
	return refSamples+samples

    def updateFastqReadCount(self,sample):

	self.getConnection()

	#
	# check if any of the fastqs already in database
	#
	data = self.c.execute('SELECT filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId FROM fastqs').fetchall()
	if data:
	    for filePair in data:
		if int(sample.id) == int(filePair[-1]):
		    filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId = filePair
		    tmp = extractData(infile=sample.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.r1.log.txt',        pattern="Running wgaAdapterTrimmer.py\nProcessed a total of\t(?P<totalReads>\d+)\treads. \(.+\)\nProcessed a total of\t(?P<totalBases>\d+)\tbases \(.+\).\ntrimmed a total of\t(?P<trimmedBases>\d+)\tbases in the start of reads \(.+\).\nwgaAdapterTrimmer.py done exiting ...\n?")
		    if type(tmp) != str:newreadcount = int(tmp['totalReads'])
		    else: newreadcount = 'Unknown'
		    self.c.execute('UPDATE fastqs SET readCount=? WHERE filePairId=?', (newreadcount,filePairId))

	self.commitAndClose()

    def addFastqs(self, sampleNameOrId, fastq1, fastq2):

	#
	# Imports
	#
	import sys
	import os
	import time
	
	fastq1 = os.path.abspath(fastq1)
	fastq2 = os.path.abspath(fastq2)
	
	samples = AnalysisPipe.database.getSamples()
	samplesbyName = {}
	samplesbyId = {}
	for sample in samples:
	    samplesbyId[sample.id]=sample
	    samplesbyName[sample.name]=sample
	sampleName = None
	sampleId = None
	try:
	    if sampleNameOrId.isdigit() and (int(sampleNameOrId) in [sample.id for sample in samples]):
		sampleId = int(sampleNameOrId);
		sampleName = str(samplesbyId[int(sampleId)].name)
	except ValueError: pass
	if type(sampleId) == int and type(sampleName) == str: pass
	elif   sampleNameOrId  in samplesbyName.keys():
	    sampleName = sampleNameOrId;
	    sampleId = samplesbyName[sampleName].id
	else:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# SampleName (or id) must be registered in the database, there is no sample with name or id '+str(sampleNameOrId)+' ('+str(type(sampleNameOrId))+') , exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	#
	# open connection to database
	#
	self.getConnection()
	
	filePairId = None
	filePairIds = []
	
	#
	# check if any of the fastqs already in database
	#
	data = self.c.execute('SELECT filePairId,fastq1,fastq2 FROM fastqs').fetchall()
	if data:
	    for filePair in data:
		filePairId = int(filePair[0])
		filePairIds.append(filePairId)
		for fastq in [fastq1, fastq2]:
		    if fastq in filePair:
			message = 'ERROR: '+fastq+' already in the database.\nExiting after error.'
			print message
			AnalysisPipe.logfile.write(message+'\n')
			sys.exit(1)
	#
	# if not in the database add a new row
	#
	AnalysisPipe.logfile.write('Getting readcount for file'+fastq1+' ... \n')
	readCount = 'Unknown'#bufcount(fastq1)/4 #one read is four lines
	AnalysisPipe.logfile.write('...done. The file has '+str(readCount)+' reads.\n')
	addedToReadsTable = False#SEAseqPipeLine.startTimeStr
	minReadLength = 'NA'

	if filePairIds: filePairId = max(filePairIds)+1
	else: filePairId = 0
	values = (filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId)
	self.c.execute('INSERT INTO fastqs VALUES (?,?,?,?,?,?,?)', values)
	
	self.commitAndClose()
	
	return 0

    def getFastqs(self,):
	#
	# Imports
	#
	import sys
	
	#
	# open connection to database
	#
	self.getConnection()
		
	#
	# get att data in fastqs table
	#
	filePairs = self.c.execute('SELECT filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId FROM fastqs').fetchall()
	
	self.commitAndClose()
	
	#return [[readCount,fastq1,fastq2] if (not addedToReadsTable) else None for filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength in filePairs]
	return [[filePairId,readCount,fastq1,fastq2,sampleId] for filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId in filePairs]

    def getRuns(self, runTypes):

	self.getConnection()

	runsInfo = []
	data = self.c.execute('SELECT * FROM runs').fetchall()
	for startTime, command, commandLine, finishedSuccessfully, masterPid in data:
	    if command in runTypes: runsInfo.append([startTime, command, commandLine, finishedSuccessfully, masterPid])

	self.commitAndClose()

	return runsInfo

class Sample(object):

    def __init__(self, sampleName=None,sampleId=None,refType=None):
	self.name = sampleName
	self.id = int(sampleId)
	self.refType = refType
	self.path = AnalysisPipe.path+'/samples/'+self.name
	self.scriptPath = AnalysisPipe.path+'/samples/'+self.name+'/script'
	self.dataPath   = AnalysisPipe.path+'/samples/'+self.name+'/data'
	self.logPath    = AnalysisPipe.path+'/samples/'+self.name+'/logs'
	self.plotsPath  = AnalysisPipe.path+'/samples/'+self.name+'/plots'
	self.fastqcPath = AnalysisPipe.path+'/samples/'+self.name+'/fastQC'
	self.dependencies = {}
	
	self.tempPath = "$SNIC_TMP"
	#self.tempPath = self.dataPath

    @property
    def readCount(self, ):
	tmpCounter = 0
	for filePairId,readCount,fastq1,fastq2,sampleId in AnalysisPipe.database.getFastqs():
	    if int(sampleId) == self.id:
		try: tmpCounter+= readCount
		except TypeError: return 'Unknown'
	return tmpCounter

    def getFastqs(self,):
	self.fastqIds = []
	for filePairId,readCount,fastq1,fastq2,sampleId in AnalysisPipe.database.getFastqs():
	    if int(sampleId) == self.id:
		self.fastqIds.append(filePairId)
		yield [filePairId,readCount,fastq1,fastq2,sampleId]

    def createDirs(self):
        import os
        try: os.makedirs(self.path)
        except OSError:pass
        try: os.makedirs(self.scriptPath)
        except OSError:pass
        try: os.makedirs(self.dataPath)
        except OSError:pass
        try: os.makedirs(self.fastqcPath)
        except OSError:pass
        try: os.makedirs(self.logPath)
        except OSError:pass
        try: os.makedirs(self.plotsPath)
        except OSError:pass

    def trimFastqs(self):
	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():

	    import sys
	    import time
	    try: project = sys.argv[3]
	    except IndexError:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	    #
	    # sbatch header
	    #
	    output = ''
	    output += '#! /bin/bash -l'+'\n'
	    output += '#SBATCH -A '+project+'\n'
	    output += '#SBATCH -n 2 -p core'+'\n'
	    output += '#SBATCH -t 240:00:00'+'\n'
	    output += '#SBATCH -J trim.'+self.name+'.'+str(filePairId)+'\n'
	    output += '#SBATCH -e '+self.logPath+'/stderr.trimming.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    output += '#SBATCH -o '+self.logPath+'/stdout.trimming.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    
	    try:
		output += '#SBATCH --mail-type=All'+'\n'
		output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	    except IndexError: pass
	    	    
	    #
	    # define variebles and go to path
	    #
	    output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	    output += 'cd '+self.path+'\n'
	    output += 'echo "-----"'+'\n'
	    
	    output += 'module load bioinfo-tools python/2.7 pysam  FastQC cutadapt/1.8.0 bowtie2 samtools picard/1.114 BEDTools/2.16.2  GATK/3.1.1'+'\n'
	    #output += 'workon py2.7\n\n'
	    
	    r1_in = fastq1
	    r2_in = fastq2

	    #
	    # WGA adapter trimming
	    #
	    if not AnalysisPipe.settings.skiprubicon:
		output += ''+AnalysisPipe.scriptPath+'/wgaAdapterTrimmer.py -i '+r1_in+' > '+self.tempPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq 2> '+self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.r1.log.txt &\n'
		output += ''+AnalysisPipe.scriptPath+'/wgaAdapterTrimmer.py -i '+r2_in+' > '+self.tempPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq 2> '+self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.r2.log.txt &\n'
		output += 'wait\n'
		r1_in = self.tempPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq'
		r2_in = self.tempPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq'
	    
	    if not AnalysisPipe.settings.skipmalbac:
		output += '\n'
		output += 'cutadapt -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC '+r1_in+' > '+self.tempPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq  2> '+self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.r1.log.txt &\n'
		output += 'cutadapt -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC '+r2_in+' > '+self.tempPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq  2> '+self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.r2.log.txt &\n'
		output += 'wait\n'
		output += '\n'
		r1_in = self.tempPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq'
		r2_in = self.tempPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq'
		if not AnalysisPipe.settings.skiprubicon:output += 'rm -v '+self.tempPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq '+self.tempPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq\n'

	    if not AnalysisPipe.settings.skipampli1:
		output += '\n'
		output += 'cutadapt -n 10 -g AGTGGGATTCCTGCTGTCAGT '+r1_in+' > '+self.tempPath+'/'+str(filePairId)+'.r1.wgaTrimmed3.fq  2> '+self.logPath+'/ampli1WgaTrimming.'+str(filePairId)+'.r1.log.txt &\n'
		output += 'cutadapt -n 10 -g AGTGGGATTCCTGCTGTCAGT '+r2_in+' > '+self.tempPath+'/'+str(filePairId)+'.r2.wgaTrimmed3.fq  2> '+self.logPath+'/ampli1WgaTrimming.'+str(filePairId)+'.r2.log.txt &\n'
		r1_in = self.tempPath+'/'+str(filePairId)+'.r1.wgaTrimmed3.fq'
		r2_in = self.tempPath+'/'+str(filePairId)+'.r2.wgaTrimmed3.fq'
		output += 'wait\n'
		output += '\n'
		if not AnalysisPipe.settings.skiprubicon and AnalysisPipe.settings.skipmalbac:
		    output += 'rm -v '+self.tempPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq '+self.tempPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq\n'
		if not AnalysisPipe.settings.skipmalbac:
		    output += 'rm -v '+self.tempPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq '+self.tempPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq\n'
	    
	    #
	    # illumina  adapter trimming
	    #
	    adaptersToTrim = '-a CTGTCTCTTATACACATCTGACGCTGCCGACGA -a CTGTCTCTTATACACATCTCCGAGCCCACGAGAC -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -a AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT'
	    output += 'cutadapt -n 3 '+adaptersToTrim+' '+r1_in+' > '+self.tempPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq 2> '+self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += 'cutadapt -n 3 '+adaptersToTrim+' '+r2_in+' > '+self.tempPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq 2> '+self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'

	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.tempPath+'/'+str(filePairId)+'.r1.wgaTrimmed3.fq '+self.tempPath+'/'+str(filePairId)+'.r2.wgaTrimmed3.fq \n'
	    output += 'wait\n'
	    
	    #
	    # quality trimmming
	    #
	    output += ''+AnalysisPipe.scriptPath+'/TrimBWAstyle.pl -q 20 '+self.tempPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq > '+self.tempPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq 2> '+self.logPath+'/qualityTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += ''+AnalysisPipe.scriptPath+'/TrimBWAstyle.pl -q 20 '+self.tempPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq > '+self.tempPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq 2> '+self.logPath+'/qualityTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'
	    
	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.tempPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq '+self.tempPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq\n'
	    output += 'wait\n'
	    
	    #
	    # remove empty or "N" only sequences
	    #
	    output += 'python '+AnalysisPipe.scriptPath+'/removeEmptyReads.py '
	    output += self.tempPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq '
	    output += self.tempPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.singletts.fq '
	    output += '>&2 2> '+self.logPath+'/removeEmptyReads.'+str(filePairId)+'.log.txt\n'
	    
	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq\n'
	    output += 'wait\n'
	    
	    #
	    # compress files
	    #
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq &\n'
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq  &\n'
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.singletts.fq &\n'
	    output += 'wait\n'
	    
	    #
	    # FASTQC
	    #
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq.gz &\n'
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq.gz &\n'
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.singletts.fq.gz &\n'
	    output += 'wait\n'
	    output += 'mv -v '+self.dataPath+'/*fastqc* '+self.fastqcPath+'/\n'
	    
	    #
	    # Final output and write script to file
	    #
            #output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
	    output += 'echo'+'\n'
	    output += 'wait'+'\n'
	    output += 'echo "$(date) AllDone"'+'\n'
	    output += 'echo "$(date) AllDone" >&2'+'\n'
	    with open(self.scriptPath+'/trimming.'+self.name+'.'+str(filePairId)+'.sh','w') as outfile: outfile.write(output)

    def mapFastqs(self):

	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():

	    import sys
	    import time
	    try: project = sys.argv[3]
	    except IndexError:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	    #
	    # sbatch header
	    #
	    output = '#! /bin/bash -l'+'\n'
	    output += '#SBATCH -A '+project+'\n'
	    output += '#SBATCH -n 16 -p node'+'\n'
	    output += '#SBATCH -t 240:00:00'+'\n'
	    output += '#SBATCH -J map.'+self.name+'.'+str(filePairId)+'\n'
	    output += '#SBATCH -e '+self.logPath+'/stderr.mapping.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    output += '#SBATCH -o '+self.logPath+'/stdout.mapping.'+self.name+'.'+str(filePairId)+'.txt'+'\n'

	    try:
		output += '#SBATCH --mail-type=All'+'\n'
		output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	    except IndexError: pass

	    #
	    # define variebles and go to path
	    #
	    output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	    output += 'cd '+self.path+'\n'
	    output += 'echo'+'\n'
	    
	    output += 'module load bioinfo-tools python/2.7 pysam  FastQC cutadapt/1.8.0 bowtie2 samtools picard/1.114 BEDTools/2.16.2  GATK/3.1.1'+'\n'

	    #
	    # Bowtie2 mapping
	    #output += 'module load bioinfo-tools pysam bwa/0.7.8\n'
	    #output += 'bwa mem -t 16 /sw/data/uppnex/reference/Homo_sapiens/GRCh37/program_files/bwa/concat.fa '+self.r1files[0]+' '+self.r2files[0]+' > '+self.sam+'\n'
	    #output += 'bowtie2 -1 '+self.r1files[0]+' -2 '+self.r2files[0]+' --very-sensitive-local -p16 -x '+self.reference+' > '+self.sam+'\n'
	    output += 'bowtie2 --maxins 2000 -p16 '
	    output += '-1 '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq.gz '
	    output += '-2 '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq.gz '
#	    output += '-U '+self.dataPath+'/'+str(filePairId)+'.singletts.fq.gz '
	    output += '-x '+AnalysisPipe.bowtie2Reference+' '
	    #output += '> '+self.tempPath+'/'+str(filePairId)+'.sam '
	    output += '2> '+self.logPath+'/stderr.bowtie2.'+str(filePairId)+'.txt |'
	    #
	    # convert to bam file
	    #
	    output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/SamFormatConverter.jar MAX_RECORDS_IN_RAM=2500000 '
	    output += 'INPUT='+ '/dev/stdin '#self.tempPath+'/'+str(filePairId)+'.sam '
	    output += 'OUTPUT='+self.dataPath+'/'+str(filePairId)+'.bam '
	    output += '1>&2  2> '+self.logPath+'/stderr.'+str(filePairId)+'.sam2bam.'+self.name+'.txt \n'
	    output += 'echo -e "sam2bam Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
	    output += 'rm -v '+ self.dataPath+'/'+str(filePairId)+'.sam\n'

	    output += 'echo -e "mapping Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq.gz'+'\n'
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq.gz'+'\n'
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.singletts.fq.gz'+'\n'

	    #
	    # Final output and write script to file
	    #
	    #output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
	    output += 'wait'+'\n'
	    output += 'echo "$(date) AllDone"'+'\n'
	    output += 'echo "$(date) AllDone" >&2'+'\n'
	    with open(self.scriptPath+'/mapping.'+self.name+'.'+str(filePairId)+'.sh','w') as outfile: outfile.write(output)

    def mergeMapped(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	#
	# sbatch header
	#
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 240:00:00'+'\n'
	output += '#SBATCH -J merge.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.merge.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.merge.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass

	#
	# define variebles and go to path
	#
	output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	output += 'cd '+self.path+'\n'
	output += 'echo'+'\n'
	output += 'module load bioinfo-tools python/2.7 pysam  FastQC cutadapt/1.8.0 bowtie2 samtools picard/1.114 BEDTools/2.16.2  GATK/3.1.1'+'\n'

	#
	# merge
	#
	if len(list(self.getFastqs())) > 1:
	    inputFiles = ' INPUT='+' INPUT='.join([self.dataPath+'/'+str(filePairId)+'.bam' for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs()])
	    output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/MergeSamFiles.jar '+inputFiles+' OUTPUT='+self.dataPath+'/'+self.name+'.merged.bam '
	    output += '1>&2  2>  '+self.logPath+'/stderr.merging.'+self.name+'.txt \n'
	    output += 'echo -e "mapping Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
	    output += 'rm -v '+' '.join([self.dataPath+'/'+str(filePairId)+'.bam' for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs()])+'\n'
	elif len(list(self.getFastqs())) == 1:
	    output += 'mv -v '+' '.join([self.dataPath+'/'+str(filePairId)+'.bam' for filePairId,readCount,fastq1,fastq2,sampleId in [list(self.getFastqs())[0]]])+' '+self.dataPath+'/'+self.name+'.merged.bam\n'
	else: print 'ERROR: no fastqfiles for sample '+self.name

	#
	# Final output and write script to file
	#
	output += 'wait'+'\n'
	output += 'echo "$(date) AllDone"'+'\n'
	output += 'echo "$(date) AllDone" >&2'+'\n'
	with open(self.scriptPath+'/mergeMapped.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def filterAndFixMerged(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 240:00:00'+'\n'
	output += '#SBATCH -J fnf.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.filterAndFix.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.filterAndFix.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass

        #
        # define variebles and go to path
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'cd '+self.path+'\n'
        output += 'echo'+'\n'
	output += 'module load bioinfo-tools python/2.7 pysam  FastQC cutadapt/1.8.0 bowtie2 samtools picard/1.114 BEDTools/2.16.2  GATK/3.1.1'+'\n'

        #
        # sort the bam file
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/SortSam.jar MAX_RECORDS_IN_RAM=2500000 SORT_ORDER=coordinate '
	output += 'INPUT='+ self.dataPath+'/'+self.name+'.merged.bam '
        output += 'OUTPUT='+self.tempPath+'/'+self.name+'.sorted.bam '
	output += 'CREATE_INDEX=true 1>&2  2> '
	output += self.logPath+'/stderr.sortBam.'+self.name+'.txt \n'
        output += 'echo -e "bam2sort Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.merged.bam\n'

        #
        # mark duplicates
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/MarkDuplicates.jar MAX_RECORDS_IN_RAM=2500000 VALIDATION_STRINGENCY=LENIENT '
	output += 'INPUT='+ self.tempPath+'/'+self.name+'.sorted.bam '
	output += 'OUTPUT='+self.tempPath+'/'+self.name+'.marked.bam '
	output += 'METRICS_FILE='+self.logPath+'/markDuplicatesMetrix.'+self.name+'.txt '
	output += '1>&2  2> '+self.logPath+'/stderr.markDuplicates.'+self.name+'.txt \n'
        output += 'echo -e "mark Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.tempPath+'/'+self.name+'.sorted.bam\n'

        #
        # fix missing information
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/AddOrReplaceReadGroups.jar '
        output += 'MAX_RECORDS_IN_RAM=2500000 '
        output += 'INPUT='+ self.tempPath+'/'+self.name+'.marked.bam '
        output += 'OUTPUT='+self.dataPath+'/'+self.name+'.fixed.bam '
        output += 'CREATE_INDEX=true RGID='+self.name+' RGLB='+self.name+' RGPL=ILLUMINA RGSM='+self.name+' RGCN="NA" RGPU="NA"'+'  '
	output += '1>&2  2> '+self.logPath+'/stderr.addAndReplaceReadGroups.'+self.name+'.txt \n'
        output += 'echo "addorreplace Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.tempPath+'/'+self.name+'.marked.bam\n'

        #
        # samtools flagstat
        #
        output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.fixed.bam'+' > '+self.logPath+'/fixedBamFlagstat.'+self.name+'.txt \n'
        output += 'echo "flagstat Done. $(date) Running on: $(hostname)" 1>&2'+'\n'

        #
        # Final output and write script to file
        #
        #output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
        output += 'wait'+'\n'
        output += 'echo "$(date) AllDone"'+'\n'
        output += 'echo "$(date) AllDone" >&2'+'\n'
	with open(self.scriptPath+'/FilterAndFix.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def realignerTargetCreator(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #

	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 16 -p node'+'\n'
	output += '#SBATCH -t 240:00:00'+'\n'
	output += '#SBATCH -J realTC.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.realTC.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.realTC.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass

        #
        # define variebles and go to path
        #
        output += 'echo "$(date) Running on: $(hostname)"\n'
	output += 'module load bioinfo-tools python/2.7 pysam  FastQC cutadapt/1.8.0 bowtie2 samtools picard/1.114 BEDTools/2.16.2  GATK/3.1.1'+'\n'

        #
        # Find targets for indel realignment
        #
        output += 'echo -e "-> RealignerTargetCreator <-"\n'
        output += 'java -Xmx72g -jar '+AnalysisPipe.gatkLocation+' -T RealignerTargetCreator '
	output += '-nt 16 '
	output += '-I '+self.dataPath+'/'+self.name+'.fixed.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-o '+self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/Mills_and_1000G_gold_standard.indels.b37.vcf'
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/1000G_phase1.indels.b37.vcf '
	output += '1>&2 2> '+self.logPath+'/stderr.RealignerTargetCreator.'+self.name+'.txt;'
        output += '\n'
        
        with open(self.scriptPath+'/realignerTargetCreator.'+self.name+'.sh','w') as outfile: outfile.write(output)
    
    def reAlignAndReCalibrate(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 240:00:00'+'\n'
	output += '#SBATCH -J reAlign.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.reAlign.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.reAlign.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'
	output += 'module load bioinfo-tools python/2.7 pysam  FastQC cutadapt/1.8.0 bowtie2 samtools picard/1.114 BEDTools/2.16.2  GATK/3.1.1'+'\n'

        #
        # Realign reads around indels
        #
        output += 'echo -e "-> IndelRealigner <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T IndelRealigner '
	output += '-I '+self.dataPath+'/'+self.name+'.fixed.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-targetIntervals '+self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
        output += ' -o '+self.tempPath+'/'+self.name+'.reAligned.bam'+' '
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/Mills_and_1000G_gold_standard.indels.b37.vcf'
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/1000G_phase1.indels.b37.vcf  '
	output += '1>&2 2> '+self.logPath+'/stderr.indelRealigner.'+self.name+'.txt;'+'\n'
        output += '\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'echo "$(date) Running on: $(hostname)"\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.fixed.bam'+'\n'
        
        #
        # Quality recalibration
        #
        output += 'echo -e "-> BaseRecalibrator <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T BaseRecalibrator '
	output += '-I '+self.tempPath+'/'+self.name+'.reAligned.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-o '+self.dataPath+'/'+self.name+'.BQSR.grp'+' '
        output += ' -knownSites '+AnalysisPipe.gatkBundleLocation+'/dbsnp_138.b37.vcf '
	output += '1>&2 2> '+self.logPath+'/stderr.baseRecalibrator.'+self.name+'.txt;'+'\n'

        output += '\n'
        output += 'echo -e "-> PrintReads <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T PrintReads '
	output += '-I '+self.tempPath+'/'+self.name+'.reAligned.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-BQSR '+self.dataPath+'/'+self.name+'.BQSR.grp'+' '
	output += '-o '+self.tempPath+'/'+self.name+'.reCalibrated.bam'+' '
	output += '1>&2 2> '+self.logPath+'/stderr.printreads.txt ;\n'
	output += 'rm -v '+self.tempPath+'/'+self.name+'.reAligned.bam'+'\n'
        output += 'samtools flagstat '+self.tempPath+'/'+self.name+'.reCalibrated.bam > '+self.logPath+'/reCalibratedBamFlagstat.'+self.name+'.txt \n'

	output += 'samtools view -b -F 4 '   +self.tempPath+'/'+self.name+'.reCalibrated.bam > '+self.tempPath+'/'+self.name+'.unmapRemoved.bam  2> '+self.logPath+'/stderr.samtoolsView.removeUnmap.'+self.name+'.txt \n'
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.tempPath+'/'+self.name+'.unmapRemoved.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex1.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.tempPath+'/'+self.name+'.unmapRemoved.bam > '+self.logPath+'/unmapRemovedBamFlagstat.'+self.name+'.txt \n'
        output += 'rm -v '+self.tempPath+'/'+self.name+'.reCalibrated.bam\n'

	output += 'samtools view -b -q 20 '  +self.tempPath+'/'+self.name+'.unmapRemoved.bam > '+self.tempPath+'/'+self.name+'.qualFiltered.bam  2> '+self.logPath+'/stderr.samtoolsView.qualFilter.'+self.name+'.txt \n'
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.tempPath+'/'+self.name+'.qualFiltered.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex2.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.tempPath+'/'+self.name+'.qualFiltered.bam > '+self.logPath+'/qualFilteredBamFlagstat.'+self.name+'.txt \n'
	output += 'rm -v '+self.tempPath+'/'+self.name+'.unmapRemoved.bam\n'

	output += 'samtools view -b -F 1024 '+self.tempPath+'/'+self.name+'.qualFiltered.bam > '+self.dataPath+'/'+self.name+'.noDuplicates.bam  2> '+self.logPath+'/stderr.samtoolsView.removeDups.'+self.name+'.txt \n'
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.dataPath+'/'+self.name+'.noDuplicates.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex3.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.noDuplicates.bam > '+self.logPath+'/noDuplicatesBamFlagstat.'+self.name+'.txt \n'
	output += 'rm -v '+self.tempPath+'/'+self.name+'.qualFiltered.bam\n'

        #
        # Final output and write script to file
        #
        #output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'wait\n'
        output += 'echo "$(date) AllDone"\n'
        with open(self.scriptPath+'/reAlignAndReCalibrate.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def haplotypeCalling(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 16 -p node'+'\n'
	output += '#SBATCH -t 240:00:00'+'\n'
	output += '#SBATCH -J hapCal.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.haplotypeCalling.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.haplotypeCalling.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'
	output += 'module load bioinfo-tools python/2.7 pysam  FastQC cutadapt/1.8.0 bowtie2 samtools picard/1.114 BEDTools/2.16.2  GATK/3.1.1'+'\n'

        output += 'echo "HC" '+'\n'
        
        output += 'cp -v '+self.dataPath+'/'+self.name+'.noDuplicates.bam $SNIC_TMP\n'
	output += 'cp -v '+self.dataPath+'/'+self.name+'.noDuplicates.bai $SNIC_TMP\n'
	output += 'java -Xmx50g -jar '+AnalysisPipe.gatkLocation+' '
        output += '-T HaplotypeCaller -nct 16 '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
        output += '-I $SNIC_TMP'+'/'+self.name+'.noDuplicates.bam '
        output += '--genotyping_mode DISCOVERY '
        output += '-stand_emit_conf 10 '
        output += '-stand_call_conf 30 '
        if wgsOrExome == 'exome': output += '-L '+AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem.bed '
	elif wgsOrExome == 'wgs': output += '-L '+AnalysisPipe.referencePath+'/wgs.bed '
        output += '--dbsnp '+AnalysisPipe.gatkBundleLocation+'/dbsnp_138.b37.vcf '
        output += '--annotation AlleleBalance --annotation AlleleBalanceBySample --annotation BaseCounts --annotation BaseQualityRankSumTest '
        output += '--annotation ChromosomeCounts --annotation ClippingRankSumTest --annotation Coverage --annotation DepthPerAlleleBySample '
        output += '--annotation DepthPerSampleHC --annotation FisherStrand --annotation GCContent --annotation HaplotypeScore --annotation HardyWeinberg '
        output += '--annotation HomopolymerRun --annotation InbreedingCoeff --annotation LikelihoodRankSumTest --annotation LowMQ '
        output += '--annotation MVLikelihoodRatio --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation MappingQualityZeroBySample '
        output += '--annotation NBaseCount --annotation QualByDepth --annotation RMSMappingQuality --annotation ReadPosRankSumTest --annotation SampleList '
        output += '--annotation SnpEff --annotation SpanningDeletions --annotation StrandBiasBySample --annotation TandemRepeatAnnotator '
        output += '--annotation TransmissionDisequilibriumTest --annotation VariantType '#--annotation StrandOddsRatio 
        output += '--emitRefConfidence GVCF --variant_index_type LINEAR --variant_index_parameter 128000 '
        output += '-o '+self.dataPath+'/'+self.name+'.gvcf '
	output += '1>&2 2> '+self.logPath+'/stderr.haplotypeCallerGatk.'+self.name+'.txt &'+'\n'
        output += 'wait'+'\n'

        #
        # Final output and write script to file
        #
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'wait\n'
        output += 'echo "$(date) AllDone"\n'
        with open(self.scriptPath+'/haplotypeCalling.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def qcSteps(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 240:00:00'+'\n'
	output += '#SBATCH -J qcSteps.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.qcSteps.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.qcSteps.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'
	output += 'module load bioinfo-tools python/2.7 pysam  FastQC cutadapt/1.8.0 bowtie2 samtools picard/1.114 BEDTools/2.16.2  GATK/3.1.1'+'\n'

        #
        # GATK callable Loci
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo -e "-> CallableLoci <-"'+'\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T CallableLoci '
	output +='-I '+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output +='-summary '+self.dataPath+'/'+self.name+'.callableLociSummary.txt '
	output +='-o '+self.dataPath+'/'+self.name+'.callableLoci.bed '
	output +='-R '+AnalysisPipe.bowtie2Reference+' '+'\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'
        output += 'echo'+'\n'
        output += 'echo "-----"'+'\n'

        #
        # qacompute
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo -e "-> Pauls qacompute <-"'+'\n'
        output += '/proj/b2010052/scripts/qaCompute -d -q 10 '
	output += '-m '+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output += self.dataPath+'/'+self.name+'.qacompute.out '
	output += '> '+self.logPath+'/'+self.name+'.qacompute.stdout.txt '
	output += '2> '+self.logPath+'/'+self.name+'.qacompute.stderr.txt '+'\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'

        #
        # picard HS metrics
        #
        output += 'java -Xmx3g -jar '+AnalysisPipe.picardLocation+'/CalculateHsMetrics.jar '
	if wgsOrExome == 'exome':output += 'BAIT_INTERVALS='  +AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem '
	elif wgsOrExome == 'wgs':output += 'BAIT_INTERVALS='  +AnalysisPipe.referencePath+'/wgs '
	if wgsOrExome == 'exome':output += 'TARGET_INTERVALS='+AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem '
	elif wgsOrExome == 'wgs':output += 'TARGET_INTERVALS='+AnalysisPipe.referencePath+'/wgs '
	output += 'INPUT='+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output += 'OUTPUT='+self.dataPath+'/'+self.name+'.hs_metrics.summary.txt '
	output += 'PER_TARGET_COVERAGE='+self.dataPath+'/'+self.name+'.hs_metrics.perTargetCoverage.txt '
	output += 'REFERENCE_SEQUENCE='+AnalysisPipe.bowtie2Reference+'  '
	output += '1>&2 2> '+self.logPath+'/'+self.name+'.stderr.caluclateHsmetrics.txt \n'


	#
	# make files for coverage checks
	#
	if wgsOrExome == 'exome':
	    output += "bedtools coverage -abam "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" -b "+AnalysisPipe.referencePath+"/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem.bed -d | sort -k 1,1 -k2,2n -k3,6n > "+self.dataPath+'/'+self.name+'.bedtools.coverage.bed\n'
	    output += "bedtools coverage -abam "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" -b "+AnalysisPipe.referencePath+"/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem.bed -hist > "+self.dataPath+'/'+self.name+'.bedtools.exomecoverage.histogram\n'
	output += "bedtools genomecov -ibam "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" > "+self.dataPath+'/'+self.name+'.bedtools.genomecov.histogram\n'
	output += "bedtools genomecov -bga -ibam "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" > "+self.dataPath+'/'+self.name+'.bedtools.genomecov.bedgraph\n'

	if wgsOrExome == 'exome':
	    output += "bedtools coverage -split -abam "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" -b "+AnalysisPipe.referencePath+"/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem.bed -d | sort -k 1,1 -k2,2n -k3,6n > "+self.dataPath+'/'+self.name+'.bedtools.coverage.nonPhysical.bed\n'
	    output += "bedtools coverage -split -abam "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" -b "+AnalysisPipe.referencePath+"/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem.bed -hist > "+self.dataPath+'/'+self.name+'.bedtools.exomecoverage.nonPhysical.histogram\n'
	output += "bedtools genomecov -split -ibam "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" > "+self.dataPath+'/'+self.name+'.bedtools.genomecov.nonPhysical.histogram\n'
	output += "bedtools genomecov -split -bga -ibam "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" > "+self.dataPath+'/'+self.name+'.bedtools.genomecov.nonPhysical.bedgraph\n'

	output += "awk '{print $7}' "+self.dataPath+'/'+self.name+'.bedtools.coverage.bed'+" | sort -n | uniq -c | awk '{print $1\"\\t\"$2}'> "+self.dataPath+'/'+self.name+".coverageDistribution.tsv\n"
	output += "awk '{print $1 \"\\t\" $2+$6-1 \"\\t\" $7}' "+self.dataPath+'/'+self.name+'.bedtools.coverage.bed'+" > "+self.dataPath+'/'+self.name+".depthPerPosition.tsv\n"

	if wgsOrExome == 'exome':
	    output += 'gzip -v9 '+self.dataPath+'/'+self.name+'.bedtools.coverage.bed\n'
	    output += 'gzip -v9 '+self.dataPath+'/'+self.name+'.bedtools.exomecoverage.histogram\n'
	    output += 'gzip -v9 '+self.dataPath+'/'+self.name+'.bedtools.coverage.nonPhysical.bed\n'
	    output += 'gzip -v9 '+self.dataPath+'/'+self.name+'.bedtools.exomecoverage.nonPhysical.histogram\n'
	output += 'gzip -v9 '+self.dataPath+'/'+self.name+'.bedtools.genomecov.histogram\n'
	output += 'gzip -v9 '+self.dataPath+'/'+self.name+'.bedtools.genomecov.bedgraph\n'
	output += 'gzip -v9 '+self.dataPath+'/'+self.name+'.bedtools.genomecov.nonPhysical.histogram\n'
	output += 'gzip -v9 '+self.dataPath+'/'+self.name+'.bedtools.genomecov.nonPhysical.bedgraph\n'
	output += 'gzip -v9 '+self.dataPath+'/'+self.name+".coverageDistribution.tsv\n"
	output += 'gzip -v9 '+self.dataPath+'/'+self.name+".depthPerPosition.tsv\n"
	
	#
	# create bed for cnv stuff
	#
	output += "bamToBed -i "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" > "+self.dataPath+'/'+self.name+'.bed'+"\n"
	output += "gzip -v9 "+self.dataPath+'/'+self.name+'.bed'+"\n"
	
        #
        # Final output and write script to file
        #
        output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
	output += 'echo'+'\n'
        output += 'wait'+'\n'
        output += 'echo "$(date) AllDone"'+'\n'
        output += 'echo "$(date) AllDone" >&2'+'\n'
        with open(self.scriptPath+'/qcSteps.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def updateOrgReadCounts(self,):
	AnalysisPipe.database.updateFastqReadCount(self)

    def getReadOrientationStats(self,):

	import time
	import operator
	import shutil
	import os
	import sys
	import pysam
	import time
	import gzip
	orientations = {'PP':0,'FR':0,'FRsp':0,'FF':0,'RF':0,'RR':0,'difChrom':0}
	
	if not os.path.exists(self.dataPath+'/orientations.pylist.gz'):	
	
	    try: uppmax_temp = os.environ["SNIC_TMP"]
	    except:
		uppmax_temp = None
		print 'Not on uppmax no temporary directory'
	
	    if not os.path.exists(self.dataPath+'/'+self.name+'.noDuplicates.bam'): AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping oriantations stats for sample '+self.name+' the infile has not been created yet...\n'); return orientations
	
	    if uppmax_temp:
		try:os.mkdir(uppmax_temp+'/fnuttglugg_TMP')
		except OSError:pass
		if not os.path.exists(uppmax_temp+'/fnuttglugg_TMP'+'/'+self.name+'.noDuplicates.bam'):
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Copying '+self.dataPath+'/'+self.name+'.noDuplicates.bam'+' to temp location for faster reading from disk, '+uppmax_temp+'/fnuttglugg_TMP'+'/'+self.name+'.noDuplicates.bam'+' \n')
		    shutil.copy(self.dataPath+'/'+self.name+'.noDuplicates.bam',uppmax_temp+'/fnuttglugg_TMP'+'/'+self.name+'.noDuplicates.bam')
		else:
		    print 'WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+self.name+'.noDuplicates.bam'+' skipping copy!!'
		    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+self.name+'.noDuplicates.bam'+' skipping copy!!\n')
		bamfileName  = uppmax_temp+'/fnuttglugg_TMP'+'/'+self.name+'.noDuplicates.bam'
	    else:bamfileName = self.dataPath+'/'+self.name+'.noDuplicates.bam'
	    if uppmax_temp:
		try:os.mkdir(uppmax_temp+'/fnuttglugg_TMP')
		except OSError:pass
		if not os.path.exists(uppmax_temp+'/fnuttglugg_TMP'+'/'+self.name+'.noDuplicates.bai'):
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Copying '+self.dataPath+'/'+self.name+'.noDuplicates.bai'+' to temp location for faster reading from disk, '+uppmax_temp+'/fnuttglugg_TMP'+'/'+self.name+'.noDuplicates.bai'+' \n')
		    try: shutil.copy(self.dataPath+'/'+self.name+'.noDuplicates.bai',uppmax_temp+'/fnuttglugg_TMP'+'/'+self.name+'.noDuplicates.bai')
		    except IOError as e: pass
		else:
		    print 'WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+self.name+'.noDuplicates.bai'+' skipping copy!!'
		    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+self.name+'.noDuplicates.bai'+' skipping copy!!\n')
	    
    #	bamfileName = 	self.dataPath+'/'+self.name+'.noDuplicates.bam'
	    try: bamfile = pysam.Samfile(bamfileName, "rb")
	    except IOError:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping oriantation stats for sample '+self.name+' the infile has not been created yet...\n')
		return orientations
	    except ValueError:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping oriantation stats for sample '+self.name+' the infile is not finished for processing...\n')
		return orientations
	
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading oriantation stats for sample '+self.name+'...\n')
	    try:
		for read in bamfile.fetch():
    
		    orientation = None
		    if read.is_paired and not read.is_unmapped and not read.mate_is_unmapped:
			if read.tid == read.rnext:
			    if read.is_proper_pair:
				orientation = 'PP'
			    elif   read.pos < read.pnext:
				if read.is_reverse: orientation = 'R'
				else: orientation = 'F'
				if read.mate_is_reverse: orientation += 'R'
				else: orientation += 'F'
			    elif read.pnext < read.pos:
				if read.mate_is_reverse: orientation = 'R'
				else: orientation = 'F'
				if read.is_reverse: orientation += 'R'
				else: orientation += 'F'
			    elif   read.pos == read.pnext: orientation = 'FRsp'
			else: orientation = 'difChrom'
			orientations[orientation]+=1
		total = sum(orientations.values())
    
		output = self.name+'\n'
		for thingy,plingy in orientations.iteritems():
		    output+= thingy+' '+str(percentage(plingy,total))+'\n'
		print output
    
	    except ValueError as e:
		if e == 'fetch called on bamfile without index':
		    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping insert size plot for sample '+self.name+' the bam index is not present...\n')
		    sys.stderr.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Skipping insert size plot for sample '+self.name+' the bam index is not present...\n')
		    return orientations

	    orientationsOnDisk = gzip.open(self.dataPath+'/orientations.pylist.gz','wb',9)
	    orientationsOnDisk.write(str(orientations))
	    orientationsOnDisk.close()
	else:
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loading orientations from orintations.pylist-file '+self.name+' ...\n')
	    orientations = eval(gzip.open(self.dataPath+'/orientations.pylist.gz','rb').read())

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Loaded oriantation stats for sample '+self.name+' joining to main...\n')
	return orientations

    def getStats(self):
	
	import re
	import time
	import sys
	import os
	import glob
	
	self.updateOrgReadCounts()
	
	stats = {}
	stats['orientations'] = self.getReadOrientationStats()
	stats['rubiconWgaTrimming'] = {}
	stats['malbacWgaTrimming']  = {}
	stats['illuminaAndNexteraTrimming']  = {}
	stats['ampliOneTrimming']  = {}
	stats['qualityTrimming']  = {}
	stats['removeEmptyReads']  = {}
	stats['bowtie2'] = {}
	
	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
	    stats['rubiconWgaTrimming'][filePairId] = {'r1':None,'r2':None}
	    stats['malbacWgaTrimming'][filePairId]  = {'r1':None,'r2':None}
	    stats['illuminaAndNexteraTrimming'][filePairId]  = {'r1':None,'r2':None}
	    stats['ampliOneTrimming'][filePairId]  = {'r1':None,'r2':None}
	    stats['qualityTrimming'][filePairId]    = {'r1':None,'r2':None}
	    
	    for read in ['r1','r2']:
		stats['rubiconWgaTrimming'][filePairId][read]         = extractData(infile=self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.'+read+'.log.txt',        pattern="Running wgaAdapterTrimmer.py\nProcessed a total of\t(?P<totalReads>\d+)\treads. \(.+\)\nProcessed a total of\t(?P<totalBases>\d+)\tbases \(.+\).\ntrimmed a total of\t(?P<trimmedBases>\d+)\tbases in the start of reads \(.+\).\nwgaAdapterTrimmer.py done exiting ...\n?")
		stats['malbacWgaTrimming'][filePairId][read]          = extractData(infile=self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.'+read+'.log.txt',         pattern="(.+)?cutadapt .+\nCommand line parameters: -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC .+\nMaximum error rate\: .+\%\n\s+No. of adapters\: 2\n\s+Processed reads\:\s+(?P<totalReads>\d+)\n\s+Processed bases\:\s+(?P<totalBases>\d+) bp \(.+ Mbp\)\n\s+Trimmed reads\:\s+(?P<trimmedReads>\d+) \(.+\%\)\n\s+Trimmed bases\:\s+(?P<trimmedBases>\d+) bp \(.+ Mbp\) \(.+\% of total\)\n\s+Too short reads\:\s+.+ \(.+\% of processed reads\)\n\s+Too long reads\:\s+.+ \(.+\% of processed reads\)\n\s+Total time\:\s+.+ s\n\s+Time per read\:\s+.+ ms")
		stats['illuminaAndNexteraTrimming'][filePairId][read] = extractData(infile=self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.'+read+'.log.txt',pattern="(.+)?cutadapt .+\nCommand line parameters: -n .+\nMaximum error rate\: .+\%\n\s+No. of adapters\: 4\n\s+Processed reads\:\s+(?P<totalReads>\d+)\n\s+Processed bases\:\s+(?P<totalBases>\d+) bp \(.+ Mbp\)\n\s+Trimmed reads\:\s+(?P<trimmedReads>\d+) \(.+\%\)\n\s+Trimmed bases\:\s+(?P<trimmedBases>\d+) bp \(.+ Mbp\) \(.+\% of total\)\n\s+Too short reads\:\s+.+ \(.+\% of processed reads\)\n\s+Too long reads\:\s+.+ \(.+\% of processed reads\)\n\s+Total time\:\s+.+ s\n\s+Time per read\:\s+.+ ms")
		stats['ampliOneTrimming'][filePairId][read]           = extractData(infile=self.logPath+'/ampli1WgaTrimming.'+str(filePairId)+'.'+read+'.log.txt',pattern="(.+)?cutadapt .+\nCommand line parameters: -n .+\nMaximum error rate\: .+\%\n\s+No. of adapters\: \d+\n\s+Processed reads\:\s+(?P<totalReads>\d+)\n\s+Processed bases\:\s+(?P<totalBases>\d+) bp \(.+ Mbp\)\n\s+Trimmed reads\:\s+(?P<trimmedReads>\d+) \(.+\%\)\n\s+Trimmed bases\:\s+(?P<trimmedBases>\d+) bp \(.+ Mbp\) \(.+\% of total\)\n\s+Too short reads\:\s+.+ \(.+\% of processed reads\)\n\s+Too long reads\:\s+.+ \(.+\% of processed reads\)\n\s+Total time\:\s+.+ s\n\s+Time per read\:\s+.+ ms")

		if stats['ampliOneTrimming'][filePairId][read] == 'NoMatchFound':
		    stats['ampliOneTrimming'][filePairId][read]      = extractData(infile=self.logPath+'/ampli1WgaTrimming.'+str(filePairId)+'.'+read+'.log.txt',         pattern="(.+)?cutadapt .+\nCommand line parameters: .+\nTrimming .+\nFinished in .+\n\n\=\=\= Summary \=\=\=\n\nTotal reads processed\:\s+(?P<totalReads>([\d\,]+))\nReads with adapters\:.+\nReads written \(passing filters\)\:.+\n\nTotal basepairs processed\:\s+(?P<totalBases>([\d\,]+)) bp\nTotal written \(filtered\)\:\s+(?P<writtenBases>([\d\,]+)) bp \(\d+(\.\d+)?\%\)\n")
		    try:
			stats['ampliOneTrimming'][filePairId][read]['totalReads']   = stats['ampliOneTrimming'][filePairId][read]['totalReads'].replace(',','')
			stats['ampliOneTrimming'][filePairId][read]['writtenBases'] = stats['ampliOneTrimming'][filePairId][read]['writtenBases'].replace(',','')
			stats['ampliOneTrimming'][filePairId][read]['totalBases']   = stats['ampliOneTrimming'][filePairId][read]['totalBases'].replace(',','')
			stats['ampliOneTrimming'][filePairId][read]['trimmedBases'] = int(stats['ampliOneTrimming'][filePairId][read]['totalBases']) - int(stats['ampliOneTrimming'][filePairId][read]['writtenBases'])
			#print 'match!!!!',stats['ampliOneTrimming'][filePairId][read],filePairId,read,self.id,self.logPath+'/ampli1WgaTrimming.'+str(filePairId)+'.'+read+'.log.txt'
		    except TypeError: pass#print 'PLATS1!!!!',stats['ampliOneTrimming'][filePairId][read],filePairId,read,self.id,self.logPath+'/ampli1WgaTrimming.'+str(filePairId)+'.'+read+'.log.txt'

		if stats['malbacWgaTrimming'][filePairId][read] == 'NoMatchFound':
		    stats['malbacWgaTrimming'][filePairId][read]      = extractData(infile=self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.'+read+'.log.txt',         pattern="(.+)?cutadapt .+\nCommand line parameters: .+\nTrimming .+\nFinished in .+\n\n\=\=\= Summary \=\=\=\n\nTotal reads processed\:\s+(?P<totalReads>([\d\,]+))\nReads with adapters\:.+\nReads written \(passing filters\)\:.+\n\nTotal basepairs processed\:\s+(?P<totalBases>([\d\,]+)) bp\nTotal written \(filtered\)\:\s+(?P<writtenBases>([\d\,]+)) bp \(\d+(\.\d+)?\%\)\n")
		    try:
			stats['malbacWgaTrimming'][filePairId][read]['totalReads']   = stats['malbacWgaTrimming'][filePairId][read]['totalReads'].replace(',','')
			stats['malbacWgaTrimming'][filePairId][read]['writtenBases'] = stats['malbacWgaTrimming'][filePairId][read]['writtenBases'].replace(',','')
			stats['malbacWgaTrimming'][filePairId][read]['totalBases']   = stats['malbacWgaTrimming'][filePairId][read]['totalBases'].replace(',','')
			stats['malbacWgaTrimming'][filePairId][read]['trimmedBases'] = int(stats['malbacWgaTrimming'][filePairId][read]['totalBases']) - int(stats['malbacWgaTrimming'][filePairId][read]['writtenBases'])
			#print 'match!!!!',stats['malbacWgaTrimming'][filePairId][read],filePairId,read,self.id
		    except TypeError: pass#print 'PLATS2!!!!',stats['malbacWgaTrimming'][filePairId][read],filePairId,read,self.id

		if stats['illuminaAndNexteraTrimming'][filePairId][read] == 'NoMatchFound':
		    stats['illuminaAndNexteraTrimming'][filePairId][read]      = extractData(infile=self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.'+read+'.log.txt',pattern="(.+)?cutadapt .+\nCommand line parameters: .+\nTrimming .+\nFinished in .+\n\n\=\=\= Summary \=\=\=\n\nTotal reads processed\:\s+(?P<totalReads>([\d\,]+))\nReads with adapters\:.+\nReads written \(passing filters\)\:.+\n\nTotal basepairs processed\:\s+(?P<totalBases>([\d\,]+)) bp\nTotal written \(filtered\)\:\s+(?P<writtenBases>([\d\,]+)) bp \(\d+(\.\d+)?\%\)\n")
		    try:
			stats['illuminaAndNexteraTrimming'][filePairId][read]['totalReads']   = stats['illuminaAndNexteraTrimming'][filePairId][read]['totalReads'].replace(',','')
			stats['illuminaAndNexteraTrimming'][filePairId][read]['writtenBases'] = stats['illuminaAndNexteraTrimming'][filePairId][read]['writtenBases'].replace(',','')
			stats['illuminaAndNexteraTrimming'][filePairId][read]['totalBases']   = stats['illuminaAndNexteraTrimming'][filePairId][read]['totalBases'].replace(',','')
			stats['illuminaAndNexteraTrimming'][filePairId][read]['trimmedBases'] = int(stats['illuminaAndNexteraTrimming'][filePairId][read]['totalBases']) - int(stats['illuminaAndNexteraTrimming'][filePairId][read]['writtenBases'])
			#print 'match!!!!',stats['illuminaAndNexteraTrimming'][filePairId][read],filePairId,read,self.id
		    except TypeError: pass#print 'PLATS3!!!!',stats['illuminaAndNexteraTrimming'][filePairId][read],filePairId,read,self.id

		stats['qualityTrimming'][filePairId][read]            = extractData(infile=self.logPath+'/qualityTrimming.'+str(filePairId)+'.'+read+'.log.txt',           pattern='(?P<totalBasess>\d+)\tbases\n(?P<trimmedBases>\d+)\ttrimmed')

	    stats['removeEmptyReads'][filePairId] = extractData(infile=self.logPath+'/removeEmptyReads.'+str(filePairId)+'.log.txt',pattern="""Running removeEmptyReads.py:\nHeader one is empty exiting.\n(?P<totalReads>\d+) read pairs processed.\n(?P<pairsOut>\d+) read pairs to outfiles .+.\n(?P<singlets>\d+) single reads to outfile .+.\nremoveEmptyReads Exiting.""")
	    stats['bowtie2'][filePairId]          = extractData(infile=self.logPath+'/stderr.bowtie2.'+str(filePairId)+'.txt',      pattern="""(?P<totalReads>\d+) reads; of these:\n\s+(?P<pairedReads>\d+) \(\d+.\d+\%\) were paired; of these:\n\s+(?P<notPropMapedPair>\d+) \(\d+.\d+\%\) aligned concordantly 0 times\n\s+(?P<properPairs>\d+) \(\d+.\d+\%\) aligned concordantly exactly 1 time\n\s+(?P<properPairsMultiMap>\d+) \(\d+.\d+\%\) aligned concordantly >1 times\n\s+----\n\s+(?P<notPropMapedPair2>\d+) pairs aligned concordantly 0 times; of these:\n\s+(?P<discordantPairs>\d+) \(\d+.\d+\%\) aligned discordantly 1 time\n\s+----\n\s+(?P<unMappedPair>\d+) pairs aligned 0 times concordantly or discordantly; of these:\n\s+(?P<possibleSingletons>\d+) mates make up the pairs; of these:\n\s+(?P<unMappedReads>\d+) \(\d+.\d+\%\) aligned 0 times\n\s+(?P<singleSingleMap>\d+) \(\d+.\d+\%\) aligned exactly 1 time\n\s+(?P<singleMultiMap>\d+) \(\d+.\d+\%\) aligned >1 times\n(?P<overallAlignmentRate>\d+.\d+)\% overall alignment rate""")

	stats['merging'] = extractData(infile=self.logPath+'/stderr.merging.'+self.name+'.txt',pattern="Finished reading inputs.+\n.+picard.sam.MergeSamFiles done. Elapsed time",checkType='program')
	pattern = 'LIBRARY\tUNPAIRED_READS_EXAMINED\tREAD_PAIRS_EXAMINED\tUNMAPPED_READS\tUNPAIRED_READ_DUPLICATES\tREAD_PAIR_DUPLICATES\tREAD_PAIR_OPTICAL_DUPLICATES\tPERCENT_DUPLICATION\tESTIMATED_LIBRARY_SIZE\n(?P<LIBRARY>.+)\t(?P<UNPAIRED_READS_EXAMINED>\d+)\t(?P<READ_PAIRS_EXAMINED>\d+)\t(?P<UNMAPPED_READS>\d+)\t(?P<UNPAIRED_READ_DUPLICATES>\d+)\t(?P<READ_PAIR_DUPLICATES>\d+)\t(?P<READ_PAIR_OPTICAL_DUPLICATES>\d+)\t(?P<PERCENT_DUPLICATION>\d+\,\d+)\t(?P<ESTIMATED_LIBRARY_SIZE>\d+)'
	oldpattern="""LIBRARY\tUNPAIRED_READS_EXAMINED\tREAD_PAIRS_EXAMINED\tUNMAPPED_READS\tUNPAIRED_READ_DUPLICATES\tREAD_PAIR_DUPLICATES\tREAD_PAIR_OPTICAL_DUPLICATES\tPERCENT_DUPLICATION\tESTIMATED_LIBRARY_SIZE\n(?P<Library>.+)\s+(?P<unPairedReads>\d+)\s+(?P<totalReads>\d+)\s+(?P<unMapped>\d+)\s+(?P<unPairedDups>\d+)\s+(?P<pairDups>\d+)\s+(?P<opticalDups>\d+)\s+(?P<percentageDuplication>\d+\,\d+)\s+(?P<estLibSize>\d+)"""
	stats['markDuplicatesMetrix'] = extractData(infile=self.logPath+'/markDuplicatesMetrix.'+self.name+'.txt',pattern=pattern)
	stats['fixedBamFlagstat']        = extractData(infile=self.logPath+'/fixedBamFlagstat.'+self.name+'.txt',       pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['reCalibratedBamFlagstat'] = extractData(infile=self.logPath+'/reCalibratedBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['unmapRemovedBamFlagstat'] = extractData(infile=self.logPath+'/unmapRemovedBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['qualFilteredBamFlagstat'] = extractData(infile=self.logPath+'/qualFilteredBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['noDuplicatesBamFlagstat'] = extractData(infile=self.logPath+'/noDuplicatesBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	
	#self.logPath+'/'+self.name+'.qacompute.stdout.txt'
	#self.logPath+'/'+self.name+'.qacompute.stderr.txt'
	#self.dataPath+'/'+self.name+'.qacompute.out '
	#self.logPath+'/'+self.name+'.stderr.caluclateHsmetrics.txt'
	pattern = 'READ_GROUP\n(?P<BAIT_SET>.+)\t(?P<GENOME_SIZE>\d+)\t(?P<BAIT_TERRITORY>\d+)\t(?P<TARGET_TERRITORY>\d+)\t(?P<BAIT_DESIGN_EFFICIENCY>\d+(\,\d+)?)\t(?P<TOTAL_READS>\d+)\t(?P<PF_READS>\d+)\t(?P<PF_UNIQUE_READS>\d+)\t(?P<PCT_PF_READS>\d+(\,\d+)?)\t(?P<PCT_PF_UQ_READS>\d+(\,\d+)?)\t(?P<PF_UQ_READS_ALIGNED>\d+)\t(?P<PCT_PF_UQ_READS_ALIGNED>\d+(\,\d+)?)\t(?P<PF_UQ_BASES_ALIGNED>\d+)\t(?P<ON_BAIT_BASES>\d+)\t(?P<NEAR_BAIT_BASES>\d+)\t(?P<OFF_BAIT_BASES>\d+)\t(?P<ON_TARGET_BASES>\d+)\t(?P<PCT_SELECTED_BASES>\d+(\,\d+)?)\t(?P<PCT_OFF_BAIT>\d+(\,\d+)?)\t(?P<ON_BAIT_VS_SELECTED>\d+(\,\d+)?)\t(?P<MEAN_BAIT_COVERAGE>\d+(\,\d+)?)\t(?P<MEAN_TARGET_COVERAGE>\d+(\,\d+)?)\t(?P<PCT_USABLE_BASES_ON_BAIT>\d+(\,\d+)?)\t(?P<PCT_USABLE_BASES_ON_TARGET>\d+(\,\d+)?)\t(?P<FOLD_ENRICHMENT>\d+(\,\d+)?)\t(?P<ZERO_CVG_TARGETS_PCT>\d+(\,\d+)?)\t(?P<FOLD_80_BASE_PENALTY>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_2X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_10X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_20X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_30X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_40X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_50X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_100X>\d+(\,\d+)?)\t(?P<HS_LIBRARY_SIZE>(\s?)|(\d+(\,\d+)?))\t(?P<HS_PENALTY_10X>\d+(\,\d+)?)\t(?P<HS_PENALTY_20X>\d+(\,\d+)?)\t(?P<HS_PENALTY_30X>\d+(\,\d+)?)\t(?P<HS_PENALTY_40X>\d+(\,\d+)?)\t(?P<HS_PENALTY_50X>\d+(\,\d+)?)\t(?P<HS_PENALTY_100X>\d+(\,\d+)?)'
	stats['hs_metrics.summary'] = extractData(infile=self.dataPath+'/'+self.name+'.hs_metrics.summary.txt',pattern=pattern)

	# make sums
	if not list(self.getFastqs()):
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# No fastq files found for sample: '+self.name+' continuing with next sample.\n')
	    sys.stderr.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# No fastq files found for sample: '+self.name+' continuing with next sample.\n')
	    self.stats = stats
	    return
	for program in ['illuminaAndNexteraTrimming','malbacWgaTrimming','qualityTrimming','rubiconWgaTrimming','ampliOneTrimming']:
	    try:
    		sums = {variable:0 for variable in stats[program][self.getFastqs().next()[0]]['r1'].keys()}
		for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
		    for read in ['r1','r2']:
			for variable, value in stats[program][filePairId][read].iteritems(): sums[variable]+=float(value)
		stats[program]['sum']= sums
	    except AttributeError: pass
	for program in ['removeEmptyReads','bowtie2']:
	    try:
		sums = {variable:0 for variable in stats[program][self.getFastqs().next()[0]].keys()}
		for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
		    for variable, value in stats[program][filePairId].iteritems(): sums[variable]+=float(value)
		stats[program]['sum']= sums
	    except AttributeError: pass

	# classification
	if os.path.exists(self.dataPath+'/sampleClassification.txt'):
	    data = open(self.dataPath+'/sampleClassification.txt').read().split('\n')
	    stats['sampleIs'] = data[0]
	    if data[0] == 'mix' or data[0] =='Unknown': refsample = None
	    else: refsample = data[1]
	    if 'LowConf' in data: stats['sampleIsLowConf'] = True
	    else:stats['sampleIsLowConf'] = False
	    data = open(self.dataPath+'/identificationVariantsSummary.txt').read().split('\n')
	    stats['pat%'] = data[0].split('=')[-1]
	    stats['don%'] = data[1].split('=')[-1]
	    stats['mix%'] = data[2].split('=')[-1]
	    stats['other%'] = data[3].split('=')[-1]
	    stats['totalIdVar'] = data[4].split('=')[-1]
	else:
	    stats['sampleIsLowConf'] = 'NA'
	    stats['sampleIs'] = 'NA'
	    stats['pat%'] = 'NA'
	    stats['don%'] = 'NA'
	    stats['mix%'] = 'NA'
	    stats['other%'] = 'NA'
	    stats['totalIdVar'] = 'NA'

	# ado summary:
	#self.dataPath+'/adoVariantsSummary.ref='+mainReferenceSample.name+'.txt'
	potentialInFiles = list(glob.iglob( self.dataPath+'/adoVariantsSummary.ref=*.txt' ))
	eraseAdo = False
	if len(potentialInFiles) >= 1:
	    if len(potentialInFiles) == 1: infile = potentialInFiles[0]
	    if len(potentialInFiles) > 1:
		if refsample: infile = self.dataPath+'/adoVariantsSummary.ref='+refsample+'.txt'
		else:  infile = potentialInFiles[0]; eraseAdo = True
	    with open(infile) as infile:
		stats['refsample'] = infile.name.split('=')[-1][:-4]
		data = infile.read().split('\n')
		stats['correct%'] = data[0].split('=')[-1]
		stats['dropout%'] = data[1].split('=')[-1]
		stats['other%'] = data[2].split('=')[-1]
		stats['totalAdoVar'] = data[3].split('=')[-1]
	else:
		stats['refsample']= 'NA'
		stats['correct%'] = 'NA'
		stats['dropout%'] = 'NA'
		stats['other%'] = 'NA'
		stats['totalAdoVar'] = 'NA'
	if eraseAdo:
		stats['refsample']= 'NA'
		stats['correct%'] = 'NA'
		stats['dropout%'] = 'NA'
		stats['other%'] = 'NA'
		stats['totalAdoVar'] = 'NA'

	# target coverage
	if os.path.exists(self.dataPath+'/targetCoveragestat.txt'):
	    with open(self.dataPath+'/targetCoveragestat.txt') as data: stats['averageTartgetCoverage'] = data.read().rstrip().split(' ')[-2]
	else: stats['averageTartgetCoverage'] = 'NA'
	    
	self.stats = stats

	# debug output
	#print "\n######## "+self.name+" ######## "
	#for key,value in stats.iteritems():
	#    print '    ',key
	#    try:
	#	for key2,value2 in value.iteritems():
	#	    assert type(value2) == dict
	#	    print '        ',key2,value2
	#    except:print '        ',value
	
	return 0

    def getSNPchunkyness(self, ):
	
	import os
	import sys
	import pysam
	import glob
	import time
	
	
	#
	# get chromosome sizes
	#
	if not os.path.exists(self.dataPath+'/'+self.name+'.noDuplicates.bam'):
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Cannot fetch chromsizes from bam for self '+self.name+' the infile has not been created yet...\n')
	    return self
	bamfileName = self.dataPath+'/'+self.name+'.noDuplicates.bam'
	try: bamfile = pysam.Samfile(bamfileName, "rb")
	except IOError:
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Cannot fetch chromsizes from bam for self '+self.name+' the infile has not been created yet...\n')
	    return self
	except ValueError:
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'#'+str(AnalysisPipe.masterPid)+'# Cannot fetch chromsizes from bam for self '+self.name+' the infile is not finished for processing...\n')
	    return self
	lengthOfChromByName = {}
	for chrom in bamfile.header['SQ']: lengthOfChromByName[chrom['SN']] = chrom['LN']

	#
	# Get positions of SNPs for identification and ADOrate estimation
	#
	if os.path.exists(self.dataPath+'/identificationVariantsList.tsv'):
	    outfile1 = open(self.dataPath+'/identificationVarsChunkyness.bed','w')
	    #snps = {chrom:{'Patient':[],'Donor':[],'mix':[],'other':[]} for chrom in lengthOfChromByName}
	    snps = {chrom:[] for chrom in lengthOfChromByName}
	    with open(self.dataPath+'/identificationVariantsList.tsv') as infile:
		for line in infile:
		    chrom,pos,rsid,classification,gt,gq,dp,ad = line.rstrip().split('\t')
		    #snps[chrom][classification].append(int(pos))
		    snps[chrom].append([int(pos),classification])
	    for chrom in sorted_nicely(lengthOfChromByName.keys()):
		lastChromosome = chrom
		if snps:
		    lastClass = None
		    chunkPositions = []
		    for pos,classification in snps[lastChromosome]:
			if classification != lastClass:
			    if chunkPositions and lastClass:
				outfile1.write( str(lastChromosome)+'\t'+str(chunkPositions[0])+'\t'+str(chunkPositions[-1])+'\t'+str(lastClass)+'\t'+str(chunkPositions)+'\t'+str(len(chunkPositions))+'\t'+str(chunkPositions[-1]-chunkPositions[0])+'\n')
			    chunkPositions = [pos]
			    lastClass = classification
			else:
			    chunkPositions.append(pos)
		    if chunkPositions and lastClass:
			outfile1.write( str(lastChromosome)+'\t'+str(chunkPositions[0])+'\t'+str(chunkPositions[-1])+'\t'+str(lastClass)+'\t'+str(chunkPositions)+'\t'+str(len(chunkPositions))+'\t'+str(chunkPositions[-1]-chunkPositions[0])+'\n')
	    outfile1.close()
	else: snps = None

	#potentialInFiles = list(glob.iglob( self.dataPath+'/adoVariantsList.ref=*.tsv' ))
	#eraseAdo = False
	#if len(potentialInFiles) >= 1:
	#    #ado = {chrom:{'dropout':[],'correct':[]} for chrom in lengthOfChromByName}
	#    ado = {chrom:[] for chrom in lengthOfChromByName}
	#    if len(potentialInFiles) == 1: infile = potentialInFiles[0]
	#    if len(potentialInFiles) > 1:
	#	data = open(self.dataPath+'/sampleClassification.txt').read().split('\n')
	#	refself = data[1]
	#	infile = self.dataPath+'/adoVariantsList.ref='+refself+'.tsv'
	#	if data[0] == 'mix' or data[0] =='Unknown':  infile = potentialInFiles[0]; eraseAdo = True
	#    with open(infile) as infile:
	#	for line in infile:
	#	    chrom,pos,rsid,classification,gt,gq,dp,ad = line.rstrip().split('\t')
	#	    if classification == 'NoData': continue
	#	    try: ado[chrom].append([int(pos),classification])
	#	    except KeyError: pass
	#else: ado = None
	#if eraseAdo:  ado = None

#	    if ado:
#		lastClass = None
#		chunkPositions = []
#		#print self.name, ado[lastChromosome]
#		for pos,classification in ado[lastChromosome]:
#		    if classification != lastClass:
#			#print self.name, lastChromosome, lastClass, len(chunkPositions), [thousandString(i) for i in chunkPositions]
#			if chunkPositions and lastClass:
#			    #print self.name, lastClass, chunkPositions,'BEFORE'
#			    #print self.name, lastClass, chunkPositions
#			    if   lastClass == 'correct': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.75 for i in chunkPositions],'o-',color='yellow')
#			    elif lastClass == 'dropout': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.70 for i in chunkPositions],'o-',color='orangered')
#			chunkPositions = [pos]
#			lastClass = classification
#		    else:
#			chunkPositions.append(pos)
#		if chunkPositions and lastClass:
#		    if   lastClass == 'correct': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.75 for i in chunkPositions],'o-',color='yellow')
#		    elif lastClass == 'dropout': axes[tmpCounter].plot(chunkPositions,[yscalemax*0.70 for i in chunkPositions],'o-',color='orangered')
#    #			if ado[lastChromosome]['correct']: axes[tmpCounter].plot(ado[lastChromosome]['correct'],[yscalemax*0.75 for i in ado[lastChromosome]['correct']],'o--',color='yellow')
#    #			if ado[lastChromosome]['dropout']: axes[tmpCounter].plot(ado[lastChromosome]['dropout'],[yscalemax*0.70 for i in ado[lastChromosome]['dropout']],'o--',color='orangered')
#    #		    tmpCounter+=1
	return

class Settings(object,):
    
    def __init__(self, ):
        """ object holding the settings used for each part of the analysis """
	
	self.defaultValues = {
	    'debug':False,
	    'uppmaxProject':'b2014005',
	    'parallelProcesses':16,
	    'mode':'exome',
	    'patientAndDonorDPcutoff':20,
	    'patientAndDonorGQcutoff':30,
	    'sampleIdentificationDPcutoff':1,
	    'sampleIdentificationGQcutoff':1,
	    'referenceSampleDPcutoff':20,
	    'referenceSampleGQcutoff':30,
	    'sampleADOestDPcutoff':20,
	    'sampleADOestGQcutoff':30,
	    'RDoverchromYscaleMax':0,
	    'skiprubicon':0,
	    'skipmalbac':0,
	    'skipampli1':0
	}
	self.explenations = {
	    'debug':'Flag for running the scripts in multiprocessing or as single process run [True/False] (default=False), Not functional',
	    'uppmaxProject':'Project id used at uppmax for sbatch scripts [bXXXXXXX] (default=b2014005)',
	    'parallelProcesses':'Number of process to run when doing multiprocess parts of analysis (defaul=16)',
	    'mode':'Type of analysis either Whole genome seguencing (wgs) or exome sequencing (exome) [wgs/exome] (default=exome)',
	    'patientAndDonorDPcutoff':'read depth cutoff for variation filtering during identification of informative variants to use when identifing cell origin (defaul=20)',
	    'patientAndDonorGQcutoff':'genotype quality cutoff for variation filtering during identification of informative variants to use when identifing cell origin (defaul=30)',
	    'sampleIdentificationDPcutoff':'read depth cutoff for variation in each sample filtering during identification of cell origin (defaul=1)',
	    'sampleIdentificationGQcutoff':'genotype quality cutoff for variation in each sample filtering during identification of cell origin (defaul=1)',
	    'referenceSampleDPcutoff':'read depth cutoff for variation filtering during identification of informative variants to use when estimating ADO (defaul=20)',
	    'referenceSampleGQcutoff':'genotype quality cutoff for variation filtering during identification of informative variants to use when estimating ADO (defaul=30)',
	    'sampleADOestDPcutoff':'read depth cutoff for variation in each sample filtering during ADO estimation (defaul=20)',
	    'sampleADOestGQcutoff':'genotype quality cutoff for variation in each sample filtering during ADO estimation (defaul=30)',
	    'RDoverchromYscaleMax':'the max value for the y scale of the RD over chrom graphs, 0 means automatic for each sample and chrom',
	    'skiprubicon':'flag for skipping rubicon wga adapter trimming (default 0/False)',
	    'skipmalbac':'flag for skipping malbac wga adapter trimming (default 0/False)',
	    'skipampli1':'flag for skipping ampliOne wga adapter trimming (default 0/False)'

	    }
	self.isDefault = {}
	self.setTime = {}

	self.debug = None
	self.uppmaxProject = None
	self.parallelProcesses = None
	self.mode = None
	self.patientAndDonorDPcutoff = None
	self.patientAndDonorGQcutoff = None
	self.sampleIdentificationDPcutoff = None
	self.sampleIdentificationGQcutoff = None
	self.referenceSampleDPcutoff = None
	self.referenceSampleGQcutoff = None
	self.sampleADOestDPcutoff = None
	self.sampleADOestGQcutoff = None
	self.RDoverchromYscaleMax = None
	self.skiprubicon = None
	self.skipmalbac = None
	self.skipampli1 = None

	self.setDefaults()

    def setDefaults(self,):
	for variableName, value in self.defaultValues.iteritems():
	    self.__dict__[variableName] = value
	    self.isDefault[variableName] = True
	    self.setTime[variableName] = None
	return 0

    def loadFromDb(self,):
	
	import time
	
	#
	# Get the connection
	#
	AnalysisPipe.database.getConnection()
	
	#
	# Select data
	#
	gotData = False
	while not gotData:
	    try:
		data = AnalysisPipe.database.c.execute('SELECT variableName,defaultValue,value,setTime FROM settings').fetchall()
		gotData = True
	    except sqlite3.OperationalError:
		time.sleep(1)
	
	#
	# Parse data and add to object __dict__
	#
	if data:
	    for variableName,default,value,setTime in data:
		self.__dict__[variableName]  = value
		self.isDefault[variableName] = default
		self.setTime[variableName]   = setTime
	
	#
	# close connection
	#
	AnalysisPipe.database.commitAndClose()

    def setVariable(self,variableName,value):
	import time
	assert variableName in self.explenations,'Error: you are trying to set an undefined variable.\n'
	self.__dict__[variableName]  = value
	self.isDefault[variableName] = False
	self.setTime[variableName]   = time.time()
	return 0

    def saveToDb(self,):
	
	#
	# imports
	#
	import time
	
	#
	# get connection
	#
	AnalysisPipe.database.getConnection()
	
        #
        # Look whats already in database, update it if older or default and set what is not
        #
	AnalysisPipe.logfile.write('checking whats in db.\n')
        alreadyInDb = {}
	data = AnalysisPipe.database.c.execute('SELECT variableName,defaultValue,value,setTime FROM settings').fetchall()
        if data:
            for variableName,default,value,setTime in data:
		#AnalysisPipe.logfile.write('processing variable '+variableName+'')
		alreadyInDb[variableName] = True
		
		if variableName in self.__dict__:
		    if default and not self.isDefault[variableName] or setTime < self.setTime[variableName]:
			if type(self.__dict__[variableName]) in [dict,list]: self.__dict__[variableName] = str(self.__dict__[variableName])
			AnalysisPipe.logfile.write('processing variable '+variableName+''+', updating from '+str(value)+' to '+str(self.__dict__[variableName])+', old_setTime '+str(time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime(setTime)))+' new_setTime '+str(time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime(self.setTime[variableName])))+'.\n')
			AnalysisPipe.database.c.execute('UPDATE settings SET defaultValue=?, value=?, setTime=? WHERE variableName=?', (self.isDefault[variableName],self.__dict__[variableName],self.setTime[variableName],variableName))
		    else: pass#AnalysisPipe.logfile.write(' no update needed.\n')
        
        #
        # Add new vars to database
        #
	AnalysisPipe.logfile.write('adding new vars to db:\n')
        for variableName in self.__dict__:
	    if variableName in ['explenations','defaultValues','isDefault','setTime']:continue
	    if variableName not in alreadyInDb:
		if type(self.__dict__[variableName]) in [dict,list]: self.__dict__[variableName] = str(self.__dict__[variableName])
		values = (variableName,self.isDefault[variableName],self.__dict__[variableName],self.setTime[variableName])
		AnalysisPipe.database.c.execute('INSERT INTO settings VALUES (?,?,?,?)', values)
		AnalysisPipe.logfile.write('variable '+variableName+' added to db with value '+str(self.__dict__[variableName])+',')
		if self.isDefault[variableName]:AnalysisPipe.logfile.write(' this is the default value.\n')
		else:AnalysisPipe.logfile.write(' non-default value.\n')
	    else: pass#SEAseqPipeLine.logfile.write('variable\t'+variableName+'\talready in db.\n')
        
	AnalysisPipe.logfile.write('commiting changes to database.\n')
        AnalysisPipe.database.commitAndClose()
        
        return 0

if __name__ == "__main__": main()


# Should add
# program checks

# Possible picard tols to utilize
# BamIndexStats
# CollectMultipleMetrics
#    CollectAlignmentSummaryMetrics
#    CollectInsertSizeMetrics
# CollectGcBiasMetrics
# EstimateLibraryComplexity
# CollectWgsMetrics or CalculateHsMetrics
# GenotypeConcordance

	### OUTFILES TO ADD CHECHS FOR LATER:
	#self.logPath+'/stderr.trimming.'+self.name+'.'+str(filePairId)+'.txt'
	#self.logPath+'/stdout.trimming.'+self.name+'.'+str(filePairId)+'.txt'
	#self.fastqcPath+'/\n'
	#self.logPath+'/stderr.mapping.'+self.name+'.'+str(filePairId)+'.txt'
	#self.logPath+'/stdout.mapping.'+self.name+'.'+str(filePairId)+'.txt'
	#self.logPath+'/stderr.merge.'+self.name+'.txt'
	#self.logPath+'/stdout.merge.'+self.name+'.txt'
	#self.logPath+'/stderr.filterAndFix.'+self.name+'.txt'
	#self.logPath+'/stdout.filterAndFix.'+self.name+'.txt'
	#self.logPath+'/stderr.sam2bam.'+self.name+'.txt'
	#self.logPath+'/stderr.sortBam.'+self.name+'.txt'
	#self.logPath+'/stderr.markDuplicates.'+self.name+'.txt'
	#self.logPath+'/stderr.addAndReplaceReadGroups.'+self.name+'.txt'
	#self.logPath+'/stderr.realTC.'+self.name+'.txt'
	#self.logPath+'/stdout.realTC.'+self.name+'.txt'
	#self.logPath+'/stderr.RealignerTargetCreator.'+self.name+'.txt;'
	#self.logPath+'/stderr.reAlign.'+self.name+'.txt'
	#self.logPath+'/stdout.reAlign.'+self.name+'.txt'
	#self.logPath+'/stderr.indelRealigner.'+self.name+'.txt;'+''
	#self.logPath+'/stderr.baseRecalibrator.'+self.name+'.txt;'+''
	#self.logPath+'/stderr.printreads.txt
	#self.logPath+'/stderr.samtoolsView.removeUnmap.'+self.name+'.txt'
	#self.logPath+'/stderr.samtoolsView.qualFilter.'+self.name+'.txt'
	#self.logPath+'/stderr.samtoolsView.removeDups.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex1.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex2.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex3.'+self.name+'.txt'
	#self.logPath+'/stderr.haplotypeCallerGatk.'+self.name+'.txt'
	#self.logPath+'/stderr.haplotypeCalling.'+self.name+'.txt'
	#self.logPath+'/stdout.haplotypeCalling.'+self.name+'.txt'
	#self.dataPath+'/'+self.name+'.gvcf '
	#self.logPath+'/stderr.qcSteps.'+self.name+'.txt'
	#self.logPath+'/stdout.qcSteps.'+self.name+'.txt'
	#self.dataPath+'/'+self.name+'.hs_metrics.perTargetCoverage.txt '
	#self.dataPath+'/'+str(filePairId)+'.sam'
	#self.dataPath+'/'+self.name+'.merged.sam'
	#self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
	#self.dataPath+'/'+self.name+'.BQSR.grp'+' '
	#self.dataPath+'/'+self.name+'.reCalibrated.bam\n'
	#self.dataPath+'/'+self.name+'.unmapRemoved.bam\n'
	#self.dataPath+'/'+self.name+'.qualFiltered.bam\n'
	#self.dataPath+'/'+self.name+'.noDuplicates.bam '
	#self.dataPath+'/'+self.name+'.callableLociSummary.txt '
	#self.dataPath+'/'+self.name+'.callableLoci.bed '

class colors: #s tolen from http://stackoverflow.com/questions/287871/print-in-terminal-with-colors-using-python
    '''Colors class:
    reset all colors with colors.reset
    two subclasses fg for foreground and bg for background.
    use as colors.subclass.colorname.
    i.e. colors.fg.red or colors.bg.green
    also, the generic bold, disable, underline, reverse, strikethrough,
    and invisible work with the main class
    i.e. colors.bold'''
    reset='\033[0m'
    bold='\033[01m'
    disable='\033[02m'
    underline='\033[04m'
    reverse='\033[07m'
    strikethrough='\033[09m'
    invisible='\033[08m'
    class fg:
        black='\033[30m'
        red='\033[31m'
        green='\033[32m'
        orange='\033[33m'
        blue='\033[34m'
        purple='\033[35m'
        cyan='\033[36m'
        lightgrey='\033[37m'
        darkgrey='\033[90m'
        lightred='\033[91m'
        lightgreen='\033[92m'
        yellow='\033[93m'
        lightblue='\033[94m'
        pink='\033[95m'
        lightcyan='\033[96m'
    class bg:
        black='\033[40m'
        red='\033[41m'
        green='\033[42m'
        orange='\033[43m'
        blue='\033[44m'
        purple='\033[45m'
        cyan='\033[46m'
        lightgrey='\033[47m'