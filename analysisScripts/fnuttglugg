#!/usr/bin/env python

helpMessage = """
############ fnuttglugg ################
a pipe for DNA sequence analysis

use:
fnuttglugg <analysisPath> addSample <sampleName> [refType eg. Pat/Don/gDNA/True]
fnuttglugg <analysisPath> addFastq <sampleName> <r1> <r2>
fnuttglugg <analysisPath> createScripts <uppmaxAccount> <"wsg"|"exome"> <emailAdress>
fnuttglugg <analysisPath> submitScripts
...more to come ...
fnuttglugg <analysisPath> report
fnuttglugg <analysisPath> makeGraphics
fnuttglugg <analysisPath> reSubmit <uppmaxAccount> <"wsg"|"exome"> [<emailAdress>] <sampleId>
"""

def main():
    
    import sys
    
    for term in ['h','help','-h','--help']:
	if term in sys.argv:
	    print helpMessage
	    sys.exit()

    app = AnalysisPipe()
    app.run()

def myround(x, base=10): return int(base * round(float(x)/base))

def thousandString(string):
    outstring = ''
    for i in range(len(string)):
	outstring += string[-(i+1)]
	if (i+1)%3 == 0: outstring += ' '
    return outstring[::-1]

def percentage(count,total):
    if str in [type(count),type(total)]: return 'NA'
    if 'NA' in [total,count]: return 'NA'
    if float(total) <=0.0: return 'NA'
    #return round(float(count) / float(total),4)
    return round(100* float(count) / float(total),2)

def extractData(infile=None,pattern=None,checkType='data'):
    import re
    if not pattern: return 'NoPatternGiven'
    if not infile: return 'NoInfileGiven'
    try:
	with open(infile) as infile:
	    data = infile.read()
	    p = re.compile(pattern)
	    m = p.search(data)
	    if m:
		if   checkType=='data':   return m.groupdict()
		elif checkType=='program':return 'FinishedOK'
	    else: return 'NoMatchFound'
    except IOError, e:
	assert e.errno == 2;
	return 'NoFileFound'

def bufcount(filename): # funtion "stolen from the internet" and modified
	""" returns the number of lines in a file
	"""
	import gzip
	if filename.split('.')[-1] in ['gz','gzip']: f = gzip.open(filename)
	else: f = open(filename)
	lines = 0
	buf_size = 1024 * 1024 * 10 # ten megabyte
	read_f = f.read # loop optimization
	
	buf = read_f(buf_size)
	while buf:
		lines += buf.count('\n')
		buf = read_f(buf_size)
		f.close
	return lines

def submitSbatch(filename,dependency=None):
    import subprocess
    import sys
    if dependency: command = ['sbatch','--dependency=afterok:'+':'.join(dependency),filename]
    else:          command = ['sbatch',filename]
    sbatch = subprocess.Popen( command, stdout=subprocess.PIPE, stderr=subprocess.PIPE )
    sbatch_out, errdata = sbatch.communicate()
    if sbatch.returncode != 0:
	print 'sbatch view Error code', sbatch.returncode, errdata
	print sbatch_out
	print filename
	sys.exit()
    jobid = sbatch_out.split('\n')[0].split(' ')[3]
    return jobid

def getIsizeData(sample):
    import time
    import operator
    import shutil
    import os
    import sys
    import pysam
    
    try: uppmax_temp = os.environ["SNIC_TMP"]
    except:
	uppmax_temp = None
	print 'Not on uppmax no temporary directory'

    isizes = []
    
    if not os.path.exists(sample.dataPath+'/'+sample.name+'.noDuplicates.bam'):
	AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Skipping iSize stats for sample '+sample.name+' the infile has not been created yet...\n')
	return sample, isizes

    if uppmax_temp:
	try:os.mkdir(uppmax_temp+'/fnuttglugg_TMP')
	except OSError:pass
	if not os.path.exists(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'):
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Copying '+sample.dataPath+'/'+sample.name+'.noDuplicates.bam'+' to temp location for faster reading from disk, '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'+' \n')
	    shutil.copy(sample.dataPath+'/'+sample.name+'.noDuplicates.bam',uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam')
	else:
	    print 'WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'+' skipping copy!!'
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'+' skipping copy!!\n')
	bamfileName  = uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bam'
    else:bamfileName = sample.dataPath+'/'+sample.name+'.noDuplicates.bam'
    if uppmax_temp:
	try:os.mkdir(uppmax_temp+'/fnuttglugg_TMP')
	except OSError:pass
	if not os.path.exists(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'):
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Copying '+sample.dataPath+'/'+sample.name+'.noDuplicates.bai'+' to temp location for faster reading from disk, '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'+' \n')
	    try: shutil.copy(sample.dataPath+'/'+sample.name+'.noDuplicates.bai',uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai')
	    except IOError as e: pass
	else:
	    print 'WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'+' skipping copy!!'
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.noDuplicates.bai'+' skipping copy!!\n')

    # do work here
    try: bamfile = pysam.Samfile(bamfileName, "rb")
    except IOError:
	AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Skipping insert size plot for sample '+sample.name+' the infile has not been created yet...\n')
	return sample, isizes
    except ValueError:
	AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Skipping insert size plot for sample '+sample.name+' the infile is not finished for processing...\n')
	return sample, isizes

    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Loading insert sizes for sample '+sample.name+'...\n')
    try:
	for read in bamfile.fetch():
	    if read.tlen >= 1: isizes.append(int(read.tlen))
    except ValueError as e:
	if e == 'fetch called on bamfile without index':
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Skipping insert size plot for sample '+sample.name+' the bam index is not present...\n')
	    sys.stderr.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Skipping insert size plot for sample '+sample.name+' the bam index is not present...\n')
	    return sample, isizes

    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# joining to main, '+sample.name+' ...\n')
    return sample, isizes

def graphForeachSample(sample):

    import time
    import operator
    import shutil
    import os
    import sys
    
    try: uppmax_temp = os.environ["SNIC_TMP"]
    except:
	uppmax_temp = None
	print 'Not on uppmax no temporary directory'

    #LOADING
    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Loading data for sample '+sample.name+' ...\n')

    perBaseCoverages = {}
    depthPerPosition = {}
    referenceBaseCount = {}
    perBaseCoverages[sample.name] = {}
    depthPerPosition[sample.name] = {}
    referenceBaseCount[sample.name] = 0
    
    if not os.path.exists(sample.dataPath+'/'+sample.name+'.bedtools.coverage.bed'):
	AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Skipping coverage stats for sample '+sample.name+' the infile has not been created yet...\n')
	return False

    if uppmax_temp:
	try:os.mkdir(uppmax_temp+'/fnuttglugg_TMP')
	except OSError:pass
	if not os.path.exists(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed'):
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Copying '+sample.dataPath+'/'+sample.name+'.bedtools.coverage.bed to temp location for faster reading from disk, '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed \n')
	    shutil.copy(sample.dataPath+'/'+sample.name+'.bedtools.coverage.bed',uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed')
	else:
	    print 'WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed skipping copy!!'
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# WARNING: rerun of '+uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed skipping copy!!\n')
	bedtoolsFile  = open(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed')
    else:bedtoolsFile = open(sample.dataPath+'/'+sample.name+'.bedtools.coverage.bed')

    lastChrom = None
    referenceBaseCountChrom = {}
    counter = 0
    for line in bedtoolsFile:

	line = line.rstrip().split('\t')
	referenceBaseCount[sample.name] += 1
	try: referenceBaseCountChrom[line[0]] += 1
	except KeyError:referenceBaseCountChrom[line[0]] = 1

	try:            perBaseCoverages[sample.name][int(line[-1])] += 1
	except KeyError:perBaseCoverages[sample.name][int(line[-1])] = 1

	#print line[0], int(line[1]) + int(line[5])-1, int(line[-1])
	try:
	    depthPerPosition[sample.name][line[0]]['refPos'].append(int(line[1])+int(line[5])-1)
	    depthPerPosition[sample.name][line[0]]['rd'].append(int(line[-1])) 
	except KeyError:
	    depthPerPosition[sample.name][line[0]] = {'refPos':[int(line[1])+int(line[5])-1],'rd':[int(line[-1])]}

	if lastChrom != line[0]:
	    sys.stderr.write(sample.name+' now starting to read chrom '+line[0]+' ...\n')
	    lastChrom = line[0]
	counter += 1
	#if counter == 1e6: break

    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Data in subprocess memeory, '+sample.name+' ...\n')
    if uppmax_temp: os.remove(uppmax_temp+'/fnuttglugg_TMP'+'/'+sample.name+'.bedtools.coverage.bed')

    #GINI
    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Lorentz/Gini processing, '+sample.name+' ...\n')
    pre_x = [count for rd, count in sorted(perBaseCoverages[sample.name].iteritems(), key=operator.itemgetter(0))]
    pre_y = [rd*count for rd, count in sorted(perBaseCoverages[sample.name].iteritems(), key=operator.itemgetter(0))]
    pre_x2 = [sum(pre_x[:i]) for i in range(len(pre_x))]
    pre_y2 = [sum(pre_y[:i]) for i in range(len(pre_y))]
    sequencedBases = sum(pre_y)
    gini_x = [0]+[float(value)/referenceBaseCount[sample.name] for value in pre_x2]
    gini_y = [0]+[float(value)/sequencedBases  for value in pre_y2]

    # COVERAGE AT RD
    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Coverage@RD processing, '+sample.name+' ...\n')
    sample.MaxCoverage = max( perBaseCoverages[sample.name].keys() )
    sample.MinCoverage = min( perBaseCoverages[sample.name].keys() )
    sample.AverageCoverage = sum( [rd*count for rd,count in perBaseCoverages[sample.name].iteritems()] ) / sum( perBaseCoverages[sample.name].values() )
    #print sample.name,sampleMinCoverage,sampleMaxCoverage,sampleAverageCoverage
    cAtRd_x = sorted(perBaseCoverages[sample.name].keys())
    y = [count for rd,count in sorted(perBaseCoverages[sample.name].iteritems(),key=operator.itemgetter(0))]
    cAtRd_y = [percentage(sum(y[i:]),referenceBaseCount[sample.name]) for i in range(len(y))] # cumulative y

    # RD Over reference
    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# RD over ref processing, '+sample.name+' ...\n')
    outputEvery = 100
    slidingWindowSize = outputEvery*100
    chromDist = {}
    for chrom in depthPerPosition[sample.name].keys():
	referencePositions = depthPerPosition[sample.name][chrom]['refPos']
	readDepths = depthPerPosition[sample.name][chrom]['rd']
	assert len(readDepths) == len(referencePositions)
	#print referencePositions[:150]
	#print readDepths[:150]
	#windowAverage = [sum(readDepths[i:i+slidingWindowSize])/float(slidingWindowSize) for i in range(len(readDepths))]
	#rdOverRef_x = concatednatedRefPos
	#rdOverRef_y = windowAverage
	rdOverRef_y = []
	rdOverRef_x = []
	for i in range(len(referencePositions)):
	    if i%outputEvery == 0:
		#rdOverRef_x.append( referencePositions[i] )
		rdOverRef_x.append( i )
		rdOverRef_y.append( float(sum(readDepths[i:i+slidingWindowSize])) / float(slidingWindowSize) )
	#rdOverRef_x = range(len(rdOverRef_x))

	chromDist[chrom] = {'x':rdOverRef_x, 'y':rdOverRef_y}

    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# joining to main, '+sample.name+' ...\n')
    return sample, gini_x, gini_y, cAtRd_x, cAtRd_y, chromDist,referenceBaseCount[sample.name]

def sorted_nicely( l ): # funtion "stolen from the internet"
    """ Sort the given iterable in the way that humans expect.""" 
    convert = lambda text: int(text) if text.isdigit() else text 
    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] 
    return sorted(l, key = alphanum_key)

class AnalysisPipe(object):

    def __init__(self):
	
	import sys
	import os
	import time

	try:
	    path = sys.argv[1]
	    path = os.path.abspath(path)
	    if path[-1] == '/': path = path[:-1]
	    AnalysisPipe.path = path
	    AnalysisPipe.programPath = os.path.abspath(sys.argv[0])
	    AnalysisPipe.scriptPath    = '/'+'/'.join(AnalysisPipe.programPath.split('/')[:-1])
	    AnalysisPipe.referencePath = '/'+'/'.join(AnalysisPipe.programPath.split('/')[:-2])+'/references'
	except IndexError:
	    sys.stderr.write('#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a path on your commandline (currently: "'+' '.join(sys.argv)+'")\n')
	    sys.exit(1)

	AnalysisPipe.bowtie2Reference = '~/singleFatCellExomeAnalysis/references/GATKbundle/human_g1k_v37.fasta'
	AnalysisPipe.picardLocation = '~/bin/picard-tools-1.114'
	AnalysisPipe.gatkLocation = '~/singleFatCellExomeAnalysis/bin/GenomeAnalysisTK-3.1-1/GenomeAnalysisTK.jar'
	AnalysisPipe.gatkBundleLocation = '~/singleFatCellExomeAnalysis/references/GATKbundle/'

	self.openLogfileConnection()
	
	return None

    def getDataBase(self):

	import os
	import time

	AnalysisPipe.database = Database(self.path+'/data.db')
	if not os.path.exists(AnalysisPipe.database.path): AnalysisPipe.database.create()

    def run(self, ):
	
	import time
	import sys
	
	try: self.action = sys.argv[2]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply an action on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	
	self.getDataBase()
	
	import os
	
	AnalysisPipe.database.addToRunsTable(time.time(),self.action,' '.join(sys.argv),False,os.getpid())
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Running '+' '.join(sys.argv)+'\n')

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Action is '+sys.argv[2]+'\n')
	if self.action == 'addSample':self.addSample()
	elif self.action == 'addFastq':self.addFastq()
	elif self.action == 'report':self.report()
	elif self.action == 'createScripts':self.createScripts()
	elif self.action == 'submitScripts':self.submitScripts()
	elif self.action == 'makeGraphics':self.makeGraphics()
	elif self.action == 'reSubmit':self.reSubmit()
	else:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a Valid action on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

    def reSubmit(self, remove=True):

	import sys
	import time
	import glob
	import os
	import shutil

	try: sampleIdOrName = sys.argv[-1]
	except IndexError:
	    AnalysisPipe.logfile.write('#ERROR#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Bad command no sample id or name found ... \n')
	    sys.stderr.write('#ERROR#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Bad command no sample id or name found ... \n')
	    sys.exit()

	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Creating sbatch scripts:\n')

	sampleFound = False
	for sample in AnalysisPipe.database.getSamples():
	    
	    if sampleIdOrName == sample.name or (sampleIdOrName.isdigit() and int(sampleIdOrName) == sample.id):

		sampleFound = True
		if remove:
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Removing old sbatch scripts, data and logs ... \n')
		    files = list(glob.iglob( sample.scriptPath+'/*' ))
		    files+= list(glob.iglob( sample.dataPath  +'/*' ))
		    files+= list(glob.iglob( sample.logPath   +'/*' ))
		    files+= list(glob.iglob( sample.fastqcPath+'/*' ))
		    for filename in files:
			if os.path.isdir(filename): shutil.rmtree(filename);
			else: os.remove(filename)
			sys.stderr.write('removing '+filename+'\n')

		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# sample: '+sample.name+' ... \n')
		try: sample.getFastqs().next()
		except StopIteration:
		    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		    continue
		sample.trimFastqs()
		sample.mapFastqs()
		sample.mergeMapped()
		sample.filterAndFixMerged()
		sample.realignerTargetCreator()
		sample.reAlignAndReCalibrate()
		sample.haplotypeCalling()
		sample.qcSteps()
	
		allSampleDependency = []

		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitting sbatches for sample: '+sample.name+' ... \n')
		try: sample.getFastqs().next()
		except StopIteration:
		    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		    continue

		dependency = []
		for filePairId,readCount,fastq1,fastq2,sampleId in sample.getFastqs():

		    fileName = sample.scriptPath+'/trimming.'+sample.name+'.'+str(filePairId)+'.sh'
		    jobid = submitSbatch(fileName)
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted trimming of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

		    fileName = sample.scriptPath+'/mapping.'+sample.name+'.'+str(filePairId)+'.sh'
		    jobid = submitSbatch(fileName,dependency=[jobid])
		    dependency.append(jobid)
		    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted mapping of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

		fileName = sample.scriptPath+'/mergeMapped.'+sample.name+'.sh'
		jobid = submitSbatch(fileName,dependency=dependency)
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted mering of mapped data for '+sample.name+' with job id '+str(jobid)+' \n')

		fileName = sample.scriptPath+'/FilterAndFix.'+sample.name+'.sh'
		jobid = submitSbatch(fileName,dependency=[jobid])
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted fixing the of merged data for '+sample.name+' with job id '+str(jobid)+' \n')

		fileName = sample.scriptPath+'/realignerTargetCreator.'+sample.name+'.sh'
		jobid = submitSbatch(fileName,dependency=[jobid])
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted realignerTargetCreator of data for '+sample.name+' with job id '+str(jobid)+' \n')

		fileName = sample.scriptPath+'/reAlignAndReCalibrate.'+sample.name+'.sh'
		jobid = submitSbatch(fileName,dependency=[jobid])
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted reAlignAndReCalibrate of data for '+sample.name+' with job id '+str(jobid)+' \n')

		fileName = sample.scriptPath+'/haplotypeCalling.'+sample.name+'.sh'
		hapJobid = submitSbatch(fileName,dependency=[jobid])
		allSampleDependency.append(hapJobid)
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted haplotypecalling for '+sample.name+' with job id '+str(jobid)+' \n')

		fileName = sample.scriptPath+'/qcSteps.'+sample.name+'.sh'
		jobid = submitSbatch(fileName,dependency=[jobid])
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted qc steps for '+sample.name+' with job id '+str(jobid)+' \n')

	if not sampleFound:
	    AnalysisPipe.logfile.write('#ERROR#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Sample '+str(sampleIdOrName)+' not found in database ... \n')
	    sys.stderr.write(          '#ERROR#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Sample '+str(sampleIdOrName)+' not found in database ... \n')
	    sys.exit()

    def addFastq(self):

	import sys
	import time
	
	try: sample = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a sample name on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	try:
	    f1 = sys.argv[4]
	    f2 = sys.argv[5]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Please supply a pair of fastq files on your commandline (currently: "'+' '.join(sys.argv)+'"), exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	AnalysisPipe.database.addFastqs(sample,f1,f2)

    def addSample(self, ):
	import sys
	import time
	try: AnalysisPipe.database.addSample(sys.argv[3],newSampleRefType=sys.argv[4])
	except IndexError: AnalysisPipe.database.addSample(sys.argv[3])

    def openLogfileConnection(self,):
        """ open a connection to the logfile, creates a logfile if none is present """
        
        #
        # Imports
        #
        import sys
        import time
        import os
        
        #
        # for logmessages
        #        
        tmpLogMessages = []
        
        #
        # check if logfile present open connection or create
        #
        AnalysisPipe.logfile = self.path + '/logfile.txt'
        if os.path.isfile(AnalysisPipe.logfile):
            if 'initiateAnalysis' in sys.argv:
                sys.stderr.write('#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# The logfile already exists please use another path to initiate the analysis.\n')
                sys.exit(1)
            else:
                AnalysisPipe.logfile = open(AnalysisPipe.logfile,'a',1)
                AnalysisPipe.logfile.write('----------------\n#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Connection to logfile '+AnalysisPipe.logfile.name+' opened.\n')
                return 0
        else:
            tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Creating the logfile "'+AnalysisPipe.logfile+'".'
            tmpLogMessages.append(tmpLogMessage)
            tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Looking for folder '+self.path+'...'
            tmpLogMessages.append(tmpLogMessage)
	    if not os.path.isdir(self.path):
		tmpLogMessage = '#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Path '+self.path+' not found creating...'
		tmpLogMessages.append(tmpLogMessage)
		os.mkdir(self.path)
            AnalysisPipe.logfile = open(AnalysisPipe.logfile,'w',1)
        
	AnalysisPipe.logfile.write('\n'.join(tmpLogMessages)+'\n')
	
        return tmpLogMessages

    def report(self, ):

	import time
	import operator
	import os
	import glob
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Creating a report located at '+self.path+'/report.htm'+' \n')
    
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Getting stats for all samlpes ....'+' \n')
	samples = AnalysisPipe.database.getSamples()
	samplesbyName = {}
	samplesbyId = {}
	refSamplesFirst = []
	for sample in samples:
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Loading stats for '+sample.name+' ....'+' \n')
	    samplesbyId[sample.id]=sample
	    samplesbyName[sample.name]=sample
	    sample.getStats()
	    if sample.refType: refSamplesFirst.append(sample)
	for sampleId, sample in sorted(samplesbyId.iteritems(), key=operator.itemgetter(0)):
	    if sample not in refSamplesFirst: refSamplesFirst.append(sample)
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Sample stats loaded, creating output ....'+' \n')
	
	reportFile = open(self.path+'/report.htm','w',1)
	reportFile.write('<html>')
	reportFile.write("""<head><style>
	    body {
		background-color:white
		font-family : Arial,"Myriad Web",Verdana,Helvetica,sans-serif;
	    }
	    h1   {color:black}
	    p    {color:green}
	    
	    table, th, td {
		border: 1px solid black;
		border-collapse: collapse;
		
		font-size : 12;
	    }
	    th {
		text-align: center;
		background-color:darkgray;
		color:white;
		border: 1px solid black;
	    }
	    table {border-spacing: 1px;}
	    th,td {padding: 5px;}
	    td {text-align: center;}
	</style></head>""")

	reportFile.write('<body>')
	reportFile.write('<h1>Analysis Report '+self.path+'</h1>\n')

	reportFile.write('<h2>List of samples:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>Sample Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>Read Pair Count</th>')
	reportFile.write('<th>Pairs After Trimming</th>')
	reportFile.write('<th>Overall Mapping Rate<br>(bowtie)</th>')
	reportFile.write('<th>Overall Mapping Rate<br>(flagstat)</th>')
	reportFile.write('<th>Duplication Rate<br>(preFilter)</th>')
	reportFile.write('<th>Unmapped or<br>Low Mapping Quality</th>')
	reportFile.write('<th>Left After<br>All Filtering</th>')
	reportFile.write('</tr>')
	for sample in refSamplesFirst:
	    reportFile.write('<tr>')
	    reportFile.write('<td>')
	    reportFile.write(str(sample.id))
	    reportFile.write('</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    reportFile.write('<td>'+thousandString(str(sample.readCount))+'</td>')
	    try:            reportFile.write('<td>'+thousandString(str(int(sample.stats['removeEmptyReads']['sum']['pairsOut'])))+' ('+str(percentage(sample.stats['removeEmptyReads']['sum']['pairsOut'],sample.readCount))+'%)</td>')
	    except KeyError:reportFile.write('<td><font color="red">NA (NA%)</td>')
	    try:
		tmpCount  = sum([sample.stats['bowtie2']['sum'][tmp] for tmp in ['singleSingleMap','singleMultiMap']])
		tmpCount += sum([sample.stats['bowtie2']['sum'][tmp] for tmp in ['discordantPairs','properPairsMultiMap','properPairs']])*2
		reportFile.write('<td>'+str(percentage(tmpCount,sample.stats['bowtie2']['sum']['totalReads']*2))+'%</td>')
		assert sample.stats['bowtie2']['sum']['totalReads'] == sample.stats['removeEmptyReads']['sum']['pairsOut'], 'Error: pair counts do not match'
	    except KeyError:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['fixedBamFlagstat']['mapped']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['fixedBamFlagstat']['duplicates']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['fixedBamFlagstat']['totalReads'])-int(sample.stats['qualFilteredBamFlagstat']['totalReads']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['noDuplicatesBamFlagstat']['mapped']),int(sample.stats['fixedBamFlagstat']['totalReads'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')

	reportFile.write('<h2>List of fastqs:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>File Id</th>')
	reportFile.write('<th># Read Pairs</th> ')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>File Name (r1)</th>')
	reportFile.write('<th>FastQC r1</th>')
	reportFile.write('<th>FastQC r2</th>')
	reportFile.write('</tr>')
	for filePairId,readCount,fastq1,fastq2,sampleId in sorted(AnalysisPipe.database.getFastqs(), key=operator.itemgetter(0)):
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(filePairId)+'</td>')
	    reportFile.write('<td>'+thousandString(str(readCount))+'</td>')
	    reportFile.write('<td>'+samplesbyId[int(sampleId)].name+'</td>')
	    reportFile.write('<td>'+fastq1+'</td>')
	    if os.path.exists(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r1.allTrimmed_fastqc.html'): reportFile.write('<td><a href="'+os.path.relpath(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r1.allTrimmed_fastqc.html','/'.join(reportFile.name.split('/')[:-1]))+'">here</a></td>')
	    else:reportFile.write('<td><font color="red">NA</td>')
	    if os.path.exists(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r2.allTrimmed_fastqc.html'): reportFile.write('<td><a href="'+os.path.relpath(samplesbyId[int(sampleId)].fastqcPath+'/'+str(filePairId)+'.r2.allTrimmed_fastqc.html','/'.join(reportFile.name.split('/')[:-1]))+'">here</a></td>')
	    else:reportFile.write('<td><font color="red">NA</td>')
	    reportFile.write('</tr>')
	reportFile.write('</table>')

	reportFile.write('<h2>Trimming Details:</h2>')
	reportFile.write('<h3>Samples:</h3>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>Sample Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>Original Bases</th>')
	reportFile.write('<th>% rubicon adapter</th>')
	reportFile.write('<th>% malbac adapter</th>')
	reportFile.write('<th>% illumina Adapters</th>')
	reportFile.write('<th>% quality trimmed</th>')
	reportFile.write('</tr>')
	for sample in refSamplesFirst:
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(sample.id)+'</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    try: reportFile.write('<td>'+thousandString(str(int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'</td>')
	    except (KeyError, TypeError) as e: reportFile.write('<td><font color="red">NA</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['rubiconWgaTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['malbacWgaTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['illuminaAndNexteraTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(percentage(int(sample.stats['qualityTrimming']['sum']['trimmedBases']),int(sample.stats['rubiconWgaTrimming']['sum']['totalBases'])))+'%</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')

	reportFile.write('<h3>Files (r1 then r2):</h3>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>File Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th colspan="2">Original Bases</th>')
	reportFile.write('<th colspan="2">% rubicon adapter</th>')
	reportFile.write('<th colspan="2">% malbac adapter</th>')
	reportFile.write('<th colspan="2">% illumina Adapters</th>')
	reportFile.write('<th colspan="2">% quality trimmed</th>')
	reportFile.write('</tr>')
	for filePairId,readCount,fastq1,fastq2,sampleId in sorted(AnalysisPipe.database.getFastqs(), key=operator.itemgetter(0)):
	    sample = samplesbyId[int(sampleId)]
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(filePairId)+'</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    for read in ['r1','r2']:
		try: reportFile.write('<td>'+thousandString(str(int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'</td>')
		except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA</td>')
	    for read in ['r1','r2']:
		try: reportFile.write('<td>'+str(percentage(int(sample.stats['rubiconWgaTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    for read in ['r1','r2']:
		try: reportFile.write('<td>'+str(percentage(int(sample.stats['malbacWgaTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    for read in ['r1','r2']:
		try: reportFile.write('<td>'+str(percentage(int(sample.stats['illuminaAndNexteraTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    for read in ['r1','r2']:
		try: reportFile.write('<td>'+str(percentage(int(sample.stats['qualityTrimming'][filePairId][read]['trimmedBases']),int(sample.stats['rubiconWgaTrimming'][filePairId][read]['totalBases'])))+'%</td>')
		except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')

	reportFile.write('<h2>HS-METRICS:</h2>')
	reportFile.write('<table>')
	reportFile.write('<tr>')
	reportFile.write('<th>Sample Id</th>')
	reportFile.write('<th>Sample Name</th>')
	reportFile.write('<th>On Target</th>')
	reportFile.write('<th>% RD=0 targets</th>')
	reportFile.write('<th>% bases @2X</th>')
	reportFile.write('<th>% bases @10X</th>')
	reportFile.write('<th>% bases @20X</th>')
	reportFile.write('<th>% bases @30X</th>')
	reportFile.write('<th>% bases @40X</th>')
	reportFile.write('<th>% bases @50X</th>')
	reportFile.write('<th>% bases @100X</th>')
	reportFile.write('<th>FOLD_ENRICHMENT</th>')
	reportFile.write('</tr>')
	for sample in refSamplesFirst:
	    reportFile.write('<tr>')
	    reportFile.write('<td>'+str(sample.id)+'</td>')
	    reportFile.write('<td>'+sample.name+'</td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_SELECTED_BASES'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e: reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['ZERO_CVG_TARGETS_PCT'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_2X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_10X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_20X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_30X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_40X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_50X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(100*round(float(sample.stats['hs_metrics.summary']['PCT_TARGET_BASES_100X'].replace(',','.')),4))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA%</td>')
	    try: reportFile.write('<td>'+str(round(float(sample.stats['hs_metrics.summary']['FOLD_ENRICHMENT'].replace(',','.')),2))+'</td>')
	    except (KeyError, TypeError) as e:reportFile.write('<td><font color="red">NA</td>')
	    reportFile.write('<tr>')
	reportFile.write('</table>')
	
	reportFile.write('<h2>InsertSize graphs:</h2>')
	if os.path.exists(AnalysisPipe.path+'/graphics/insertSizes.png'):  reportFile.write('<img src="'+os.path.relpath(AnalysisPipe.path+'/graphics/insertSizes.png',  '/'.join(reportFile.name.split('/')[:-1]))+'">')
	reportFile.write('<h2>Coverage graphs:</h2>')
	if os.path.exists(AnalysisPipe.path+'/graphics/lorentzCurve.png'): reportFile.write('<img src="'+os.path.relpath(AnalysisPipe.path+'/graphics/lorentzCurve.png', '/'.join(reportFile.name.split('/')[:-1]))+'">')
	if os.path.exists(AnalysisPipe.path+'/graphics/exomecoverage.png'):reportFile.write('<img src="'+os.path.relpath(AnalysisPipe.path+'/graphics/exomecoverage.png','/'.join(reportFile.name.split('/')[:-1]))+'">')
	
	for imgFileName in glob.iglob(AnalysisPipe.path+'/graphics/readDepth.*.png'):
	    relPath = os.path.relpath(imgFileName, '/'.join(reportFile.name.split('/')[:-1]))
	    reportFile.write('<img src="'+relPath+'">')

	reportFile.write('</body></html>\n')
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Full report now written to '+self.path+'/report.htm'+' \n')
	return 0

    def createScripts(self, ):

	import time
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Creating sbatch scripts:\n')

	for sample in AnalysisPipe.database.getSamples():
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# sample: '+sample.name+' ... \n')
	    try: sample.getFastqs().next()
	    except StopIteration:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		continue
	    sample.trimFastqs()
	    sample.mapFastqs()
	    sample.mergeMapped()
	    sample.filterAndFixMerged()
	    sample.realignerTargetCreator()
	    sample.reAlignAndReCalibrate()
	    sample.haplotypeCalling()
	    sample.qcSteps()
	
	self.createPipeWideScripts()

    def submitScripts(self,sampleNameOrId=None):

	import time

	allSampleDependency = []
	for sample in AnalysisPipe.database.getSamples():

	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitting sbatches for sample: '+sample.name+' ... \n')
	    try: sample.getFastqs().next()
	    except StopIteration:
		AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# No fastq files found for sample: '+sample.name+' continuing with next sample.\n')
		continue

	    dependency = []
	    for filePairId,readCount,fastq1,fastq2,sampleId in sample.getFastqs():

		fileName = sample.scriptPath+'/trimming.'+sample.name+'.'+str(filePairId)+'.sh'
		jobid = submitSbatch(fileName)
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted trimming of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

		fileName = sample.scriptPath+'/mapping.'+sample.name+'.'+str(filePairId)+'.sh'
		jobid = submitSbatch(fileName,dependency=[jobid])
		dependency.append(jobid)
		AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted mapping of fastq '+str(filePairId)+' for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/mergeMapped.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=dependency)
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted mering of mapped data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/FilterAndFix.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted fixing the of merged data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/realignerTargetCreator.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted realignerTargetCreator of data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/reAlignAndReCalibrate.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted reAlignAndReCalibrate of data for '+sample.name+' with job id '+str(jobid)+' \n')

	    fileName = sample.scriptPath+'/haplotypeCalling.'+sample.name+'.sh'
	    hapJobid = submitSbatch(fileName,dependency=[jobid])
	    allSampleDependency.append(hapJobid)
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted haplotypecalling for '+sample.name+' with job id '+str(jobid)+' \n')
	    
	    fileName = sample.scriptPath+'/qcSteps.'+sample.name+'.sh'
	    jobid = submitSbatch(fileName,dependency=[jobid])
	    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Submitted qc steps for '+sample.name+' with job id '+str(jobid)+' \n')

    def makeISizePlot(self):

	import pysam
	import numpy as np
	import matplotlib.pyplot as plt
	import operator
	import sys
	import os
	import time
	import multiprocessing

	htmlColors = ['AliceBlue','AntiqueWhite','Aqua','Aquamarine','Azure','Beige','Bisque','Black','BlanchedAlmond','Blue','BlueViolet','Brown','BurlyWood','CadetBlue','Chartreuse','Chocolate','Coral','CornflowerBlue','Cornsilk','Crimson','Cyan','DarkBlue','DarkCyan','DarkGoldenRod','DarkGray','DarkGreen','DarkKhaki','DarkMagenta','DarkOliveGreen','DarkOrange','DarkOrchid','DarkRed','DarkSalmon','DarkSeaGreen','DarkSlateBlue','DarkSlateGray','DarkTurquoise','DarkViolet','DeepPink','DeepSkyBlue','DimGray','DodgerBlue','FireBrick','FloralWhite','ForestGreen','Fuchsia','Gainsboro','GhostWhite','Gold','GoldenRod','Gray','Green','GreenYellow','HoneyDew','HotPink','IndianRed','Indigo','Ivory','Khaki','Lavender','LavenderBlush','LawnGreen','LemonChiffon','LightBlue','LightCoral','LightCyan','LightGoldenRodYellow','LightGray','LightGreen','LightPink','LightSalmon','LightSeaGreen','LightSkyBlue','LightSlateGray','LightSteelBlue','LightYellow','Lime','LimeGreen','Linen','Magenta','Maroon','MediumAquaMarine','MediumBlue','MediumOrchid','MediumPurple','MediumSeaGreen','MediumSlateBlue','MediumSpringGreen','MediumTurquoise','MediumVioletRed','MidnightBlue','MintCream','MistyRose','Moccasin','NavajoWhite','Navy','OldLace','Olive','OliveDrab','Orange','OrangeRed','Orchid','PaleGoldenRod','PaleGreen','PaleTurquoise','PaleVioletRed','PapayaWhip','PeachPuff','Peru','Pink','Plum','PowderBlue','Purple','Red','RosyBrown','RoyalBlue','SaddleBrown','Salmon','SandyBrown','SeaGreen','SeaShell','Sienna','Silver','SkyBlue','SlateBlue','SlateGray','Snow','SpringGreen','SteelBlue','Tan','Teal','Thistle','Tomato','Turquoise','Violet','Wheat','WhiteSmoke','Yellow','YellowGreen']
	linestyles = ['-',':','--','-.']
	colors = ['b','b:','r','r:','g','g:','c','c:','m','m:','y','y:','k','k:']+[i+'--' for i in 'rgbcmyk']+[i+'-.' for i in 'rgbcmyk'] # ['-' | '--' | '-.' | ':' | 'None' | ' ' | '']
	isizes = {}

	sampleCount = sum([1 for sample in AnalysisPipe.database.getSamples()])
	poolOfProcesses = multiprocessing.Pool(min([multiprocessing.cpu_count(),sampleCount]),maxtasksperchild=1)
	parallelResults = poolOfProcesses.imap_unordered(getIsizeData,AnalysisPipe.database.getSamples(),chunksize=1)
	
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Loading and processing indata ...\n')
	for output in parallelResults:
	    sample, sampleInsSizes = output
	    isizes[sample.name] = sampleInsSizes
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# All iSize data loaded and processed.\n')
	poolOfProcesses.close()
	poolOfProcesses.join()

	#for sample in AnalysisPipe.database.getSamples():
	#    try: bamfile = pysam.Samfile(sample.dataPath+'/'+sample.name+'.noDuplicates.bam', "rb")
	#    except IOError:
	#	AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Skipping insert size plot for sample '+sample.name+' the infile has not been created yet...\n')
	#	continue
	#    except ValueError:
	#	AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Skipping insert size plot for sample '+sample.name+' the infile is not finished for processing...\n')
	#	continue
	#    AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Loading insert sizes for sample '+sample.name+'...\n')
	#    for read in bamfile.fetch():
	#	if read.tlen >= 1:
	#	    try:isizes[sample.name].append(int(read.tlen))
	#	    except KeyError:isizes[sample.name]= [read.tlen]

	if not isizes:
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Skipping insert size plot no sample has data ...\n')
	    return 0

	fig, axes = plt.subplots(1, sharex=True)
	plots = []
	incrementer=0
	incrementer2=0
	incrementer3=0

	for sample in AnalysisPipe.database.getSamples():
	    #n, bins, patches = axes.hist(isizes[sample.name], 50, normed=1, histtype='step',label=sample.name)
	    counter = {}
	    total = 0

	    for isize in isizes[sample.name]:
		try:counter[isize] += 1
		except KeyError: counter[isize] = 1
		total+=1

	    y = [percentage(value,total) for key, value in sorted(counter.iteritems(), key=operator.itemgetter(0))]
	    #y = [value for key, value in sorted(counter.iteritems(), key=operator.itemgetter(0))]
	    x = sorted(counter.keys())
	    try: plots.append(axes.plot(x, y,colors[incrementer],label=sample.name))
	    except IndexError:
		plots.append(axes.plot(x, y,color=htmlColors[incrementer2],linestyle=linestyles[incrementer3],label=sample.name))
		if incrementer3 == 3:
		    incrementer2+=1
		    incrementer3 = 0
		else: incrementer3+=1
	    incrementer+=1
	    axes.set_xlim([0,1000])

	handles, labels = axes.get_legend_handles_labels()
	hl = sorted(zip(handles, labels), key=operator.itemgetter(1))
	handles2, labels2 = zip(*hl)
	axes.legend(handles2, labels2,loc=0,fontsize='small')
	axes.set_xlabel('InsertSize (final filtered data)')
	axes.set_ylabel('Frequency')
	plt.savefig(AnalysisPipe.path+'/graphics/insertSizes.pdf',dpi=50,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/insertSizes.png',dpi=50,bbox_inches='tight')

    def makeGraphics(self,):

	import pysam
	import numpy as np
	import matplotlib.pyplot as plt
	import operator
	import sys
	import os
	import time
	import multiprocessing

	try: os.mkdir(AnalysisPipe.path+'/graphics')
	except OSError: pass

	# make isize plot
	self.makeISizePlot()
	#p = multiprocessing.Process(target=self.makeISizePlot,args=())
	#p.start()

	sampleCount = sum([1 for sample in AnalysisPipe.database.getSamples()])

	poolOfProcesses = multiprocessing.Pool(min([multiprocessing.cpu_count(),sampleCount]),maxtasksperchild=1)
	parallelResults = poolOfProcesses.imap_unordered(graphForeachSample,AnalysisPipe.database.getSamples(),chunksize=1)

	referenceBaseCount ={}
	plotData = {}
	rdOverRefDistYes = {}
	rdOverRefDistXes = {}
	returnedSampleNames = []
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Loading and processing indata ...\n')
	for output in parallelResults:
	    if not output: continue
	    sample, gini_x, gini_y, cAtRd_x, cAtRd_y, chromDist,referenceBaseCount[sample.name] = output
	    returnedSampleNames.append(sample.name)
	    for chrom, values in chromDist.iteritems():
		try: rdOverRefDistYes[chrom][sample.name] = values['y']
		except KeyError:rdOverRefDistYes[chrom] = {sample.name:values['y']}
		if chrom in rdOverRefDistXes: assert rdOverRefDistXes[chrom] == values['x']
		else: rdOverRefDistXes[chrom] = values['x']
	    plotData[sample.name] = {'gini_x':gini_x, 'gini_y':gini_y, 'cAtRd_x':cAtRd_x, 'cAtRd_y':cAtRd_y}
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# All coverage data loaded and processed.\n')
	
	samples = []
	for sample in AnalysisPipe.database.getSamples():
	    if sample.name in returnedSampleNames: samples.append(sample)
	if not samples:
	    #p.join()
	    return 0

	for value in referenceBaseCount.values(): assert value == referenceBaseCount.values()[0]
	htmlColors = ['AliceBlue','AntiqueWhite','Aqua','Aquamarine','Azure','Beige','Bisque','Black','BlanchedAlmond','Blue','BlueViolet','Brown','BurlyWood','CadetBlue','Chartreuse','Chocolate','Coral','CornflowerBlue','Cornsilk','Crimson','Cyan','DarkBlue','DarkCyan','DarkGoldenRod','DarkGray','DarkGreen','DarkKhaki','DarkMagenta','DarkOliveGreen','DarkOrange','DarkOrchid','DarkRed','DarkSalmon','DarkSeaGreen','DarkSlateBlue','DarkSlateGray','DarkTurquoise','DarkViolet','DeepPink','DeepSkyBlue','DimGray','DodgerBlue','FireBrick','FloralWhite','ForestGreen','Fuchsia','Gainsboro','GhostWhite','Gold','GoldenRod','Gray','Green','GreenYellow','HoneyDew','HotPink','IndianRed','Indigo','Ivory','Khaki','Lavender','LavenderBlush','LawnGreen','LemonChiffon','LightBlue','LightCoral','LightCyan','LightGoldenRodYellow','LightGray','LightGreen','LightPink','LightSalmon','LightSeaGreen','LightSkyBlue','LightSlateGray','LightSteelBlue','LightYellow','Lime','LimeGreen','Linen','Magenta','Maroon','MediumAquaMarine','MediumBlue','MediumOrchid','MediumPurple','MediumSeaGreen','MediumSlateBlue','MediumSpringGreen','MediumTurquoise','MediumVioletRed','MidnightBlue','MintCream','MistyRose','Moccasin','NavajoWhite','Navy','OldLace','Olive','OliveDrab','Orange','OrangeRed','Orchid','PaleGoldenRod','PaleGreen','PaleTurquoise','PaleVioletRed','PapayaWhip','PeachPuff','Peru','Pink','Plum','PowderBlue','Purple','Red','RosyBrown','RoyalBlue','SaddleBrown','Salmon','SandyBrown','SeaGreen','SeaShell','Sienna','Silver','SkyBlue','SlateBlue','SlateGray','Snow','SpringGreen','SteelBlue','Tan','Teal','Thistle','Tomato','Turquoise','Violet','Wheat','WhiteSmoke','Yellow','YellowGreen']
	linestyles = ['-',':','--','-.']
	colors = ['b','b:','r','r:','g','g:','c','c:','m','m:','y','y:','k','k:']+[i+'--' for i in 'rgbcmyk']+[i+'-.' for i in 'rgbcmyk'] # ['-' | '--' | '-.' | ':' | 'None' | ' ' | '']

	# lorenz curve and gini index
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Generating lorentzcurve graph...\n')
	fig, axes = plt.subplots(1, sharex=True)
	plots = []
	a = [i/10.0 for i in range(0,11,1)]
	plots.append(axes.plot(a, a,'b-o',label='EvenDist, 0.00%'))
	incrementer=0
	incrementer2=0
	incrementer3=0
	for sample in samples:
	    x = plotData[sample.name]['gini_x']
	    y = plotData[sample.name]['gini_y']
	    B = sum( [((y[i]+y[i+1])/2.0)*(x[i+1]-x[i]) for i in range(len(x)-1)] )
	    A = 0.5 - B
	    G = A / (A + B)
	    assert A + B == 0.5
	    assert G == 2*A
	    assert G == 1 - 2*B
	    giniApprox = round(100*G,2)
	    try: plots.append(axes.plot(x, y,colors[incrementer],label=sample.name+', '+str(giniApprox)+'%'))
	    except IndexError:
		plots.append(axes.plot(x, y,color=htmlColors[incrementer2],linestyle=linestyles[incrementer3],label=sample.name+', '+str(giniApprox)+'%'))
		if incrementer3 == 3:
		    incrementer2+=1
		    incrementer3 = 0
		else: incrementer3+=1
	    incrementer+=1
	handles, labels = axes.get_legend_handles_labels()
	hl = sorted(zip(handles, labels), key=operator.itemgetter(1))
	handles2, labels2 = zip(*hl)
	axes.legend(handles2, labels2,loc=0,fontsize='small')
	axes.set_xlabel('% of Targeted Bases')
	axes.set_ylabel('% of Sequenced Bases')
	plt.savefig(AnalysisPipe.path+'/graphics/lorentzCurve.pdf',dpi=50,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/lorentzCurve.png',dpi=50,bbox_inches='tight')

	# make coverage at rd plot
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Generating coverage at rd graph...\n')
	fig, axes = plt.subplots(1, sharex=True)
	plots = []
	incrementer=0
	incrementer2=0
	incrementer3=0
	for sample in samples:
	    try: plots.append(axes.plot(plotData[sample.name]['cAtRd_x'], plotData[sample.name]['cAtRd_y'], colors[incrementer],label=sample.name))
	    except IndexError:
		plots.append( axes.plot(plotData[sample.name]['cAtRd_x'], plotData[sample.name]['cAtRd_y'], color=htmlColors[incrementer2],linestyle=linestyles[incrementer3],label=sample.name) )
		if incrementer3 == 3:
		    incrementer2+=1
		    incrementer3 = 0
		else: incrementer3+=1
	    axes.set_xlim([0,50])
	    incrementer+=1
	handles, labels = axes.get_legend_handles_labels()
	hl = sorted(zip(handles, labels), key=operator.itemgetter(1))
	handles2, labels2 = zip(*hl)
	axes.legend(handles2, labels2,loc=0,fontsize='small')
	axes.set_xlabel('Coverage')
	axes.set_ylabel('% of targeted bases')
	plt.savefig(AnalysisPipe.path+'/graphics/exomecoverage.pdf',dpi=50,bbox_inches='tight')
	plt.savefig(AnalysisPipe.path+'/graphics/exomecoverage.png',dpi=50,bbox_inches='tight')

	# make rd over reference graph
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Generating rd over reference graph...\n')
	for chrom in rdOverRefDistXes.keys():

	    fig, axes = plt.subplots(sampleCount, sharex=True)
	    fig.set_size_inches(30,sampleCount*3)
	    axes[0].set_title("Chromosome "+chrom+" coverage over concatenated targets")

	    tmpCounter = 0
	    maxY = 0
	    for sample in samples:

		axes[tmpCounter].plot(rdOverRefDistXes[chrom], rdOverRefDistYes[chrom][sample.name], lw=1,color="green")
		axes[tmpCounter].fill_between(rdOverRefDistXes[chrom],0,rdOverRefDistYes[chrom][sample.name], color="green",alpha=0.5)
		axes[tmpCounter].set_xlabel(sample.name)
		tmpCounter+=1
		
		maxY = max([maxY,max(rdOverRefDistYes[chrom][sample.name])])

	    for i in range(sampleCount):
		axes[i].set_ylim( 0, min([30,max([myround(maxY),1])]) );
		axes[i].set_xlim( min(rdOverRefDistXes[chrom]), max(rdOverRefDistXes[chrom]) )
	    plt.savefig(AnalysisPipe.path+'/graphics/readDepth.'+chrom+'.pdf',dpi=50,bbox_inches="tight")
	    plt.savefig(AnalysisPipe.path+'/graphics/readDepth.'+chrom+'.png',dpi=50,bbox_inches="tight")

	#p.join()
	
	return 0

    def createPipeWideScripts(self,):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 16 -p node'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J graphAndRep.'+self.path.split('/')[-1]+'\n'
	output += '#SBATCH -e '+self.path+'/stderr.graphAndRep.txt'+'\n'
	output += '#SBATCH -o '+self.path+'/stdout.graphAndRep.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'

        output += 'echo "HC" '+'\n'
        output += 'workon py2.7\n'
        output += AnalysisPipe.programPath+' '+AnalysisPipe.path+' makeGraphics\n'
	output += AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
	output += 'wait'+'\n'

        #
        # Final output and write script to file
        #
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'wait\n'
        output += 'echo "$(date) AllDone"\n'
        with open(self.path+'/graphAndRep.sh','w') as outfile: outfile.write(output)

class Database(object):
    
    def __init__(self, dbPath):
	self.path = dbPath

    def getConnection(self,):
	#
	# Import useful stuff
	#
	import sqlite3
	import sys

	#
	# Create database and set
	#
	try: self.conn = sqlite3.connect(self.path)
	except sqlite3.OperationalError:
	    print 'ERROR: Trouble with the database, plase check your commandline.'
	    sys.exit()
	self.c = self.conn.cursor()

    def commitAndClose(self,):
	#
	# commit changes and close connection
	#
	self.conn.commit()
	self.conn.close()

    def create(self,):
	""" creates the database holding all information used in the analysis """
	
	self.getConnection()
	
	#
	# Create tables
	#
	self.c.execute('''CREATE TABLE runs (startTime,command,commandLine,finishedSuccessfully,masterPid)''')
	self.c.execute('''CREATE TABLE fastqs (filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId,PRIMARY KEY (filePairId))''');
	self.c.execute('''CREATE TABLE settings (variableName,defaultValue,value,setTime,PRIMARY KEY (variableName))''')
	self.c.execute('''CREATE TABLE results (resultName,defaultValue,value,setTime,PRIMARY KEY (resultName))''')
	self.c.execute('''CREATE TABLE samples (sampleId,sampleName,refType,PRIMARY KEY (sampleId))''')
	
	self.commitAndClose()

    def addToRunsTable(self, startTime, command, commandLine, finishedSuccessfully, masterPid):
	
	self.getConnection()
	
	#
	# check if pid already in database
	#
	t = (masterPid,)
	data = self.c.execute('SELECT masterPid, startTime FROM runs WHERE masterPid=?',t).fetchall()        
	if data:
	    for tmp1,tmp2 in data:

	#
	# if pid and startTime matches update the "finishedSuccessfully" entry
	#
		if tmp1 == masterPid and tmp2 == startTime:
		    values = (startTime, command, commandLine, finishedSuccessfully, masterPid)
		    self.c.execute('UPDATE runs SET finishedSuccessfully=? WHERE masterPid=? AND startTime=?', (finishedSuccessfully,masterPid,startTime))
	
	#
	# if not in the database add a new row
	#
	else:
	    values = (startTime, command, commandLine, finishedSuccessfully, masterPid)
	    self.c.execute('INSERT INTO runs VALUES (?,?,?,?,?)', values)
	
	self.commitAndClose()
	
	return 0

    def addSample(self, newSampleName,newSampleRefType=None):
	
	#
	# Imports
	#
	import sys
	import time
	
	#
	# open connection to database
	#
	self.getConnection()
	
	sampleNames = []
	sampleIds = []
	
	#
	# check if any of the fastqs already in database
	#
	data = self.c.execute('SELECT sampleId,sampleName,refType FROM samples').fetchall()
	if data:
	    for (sampleId,sampleName,sampleRefType) in data:
		#sampleName = sampleName[0]
		sampleNames.append(sampleName)
		sampleIds.append(sampleId)
	    if newSampleName in sampleNames:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# SampleName must be uniq, there is already a sample with name '+newSampleName+' , exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	
	if sampleIds:  sampleId = max(sampleIds)+1
	else:          sampleId = 0 
	AnalysisPipe.logfile.write('#LOGMSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# Adding sample '+newSampleName+' to database with id '+str(sampleId)+'.\n')
	values = (sampleId,newSampleName,newSampleRefType)
	self.c.execute('INSERT INTO samples VALUES (?,?,?)', values)
	
	sample = Sample(sampleName=newSampleName, sampleId=sampleId,refType=newSampleRefType)
	sample.createDirs()
	
	self.commitAndClose()
	
	return 0

    def getSamples(self):
	#
	# Imports
	#
	import sys
	import time
	
	#
	# open connection to database
	#
	self.getConnection()
	
	refSamples = []
	samples = []
	
	data = self.c.execute('SELECT sampleId,sampleName,refType FROM samples').fetchall()
	if data:
	    for (sampleId,sampleName,sampleRefType) in data:
		if sampleRefType: refSamples.append( Sample(sampleName=sampleName,sampleId=int(sampleId),refType=sampleRefType) )
		else:                samples.append( Sample(sampleName=sampleName,sampleId=int(sampleId),refType=None) )
	
	self.commitAndClose()
	
	return refSamples+samples

    def updateFastqReadCount(self,sample):

	self.getConnection()

	#
	# check if any of the fastqs already in database
	#
	data = self.c.execute('SELECT filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId FROM fastqs').fetchall()
	if data:
	    for filePair in data:
		if int(sample.id) == int(filePair[-1]):
		    filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId = filePair
		    newreadcount = int(extractData(infile=sample.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.r1.log.txt',        pattern="Running wgaAdapterTrimmer.py\nProcessed a total of\t(?P<totalReads>\d+)\treads. \(.+\)\nProcessed a total of\t(?P<totalBases>\d+)\tbases \(.+\).\ntrimmed a total of\t(?P<trimmedBases>\d+)\tbases in the start of reads \(.+\).\nwgaAdapterTrimmer.py done exiting ...\n?")['totalReads'])
		    self.c.execute('UPDATE fastqs SET readCount=? WHERE filePairId=?', (newreadcount,filePairId))

	self.commitAndClose()

    def addFastqs(self, sampleNameOrId, fastq1, fastq2):

	#
	# Imports
	#
	import sys
	import os
	import time
	
	fastq1 = os.path.abspath(fastq1)
	fastq2 = os.path.abspath(fastq2)
	
	samples = AnalysisPipe.database.getSamples()
	samplesbyName = {}
	samplesbyId = {}
	for sample in samples:
	    samplesbyId[sample.id]=sample
	    samplesbyName[sample.name]=sample
	sampleName = None
	sampleId = None
	try:
	    if sampleNameOrId.isdigit() and (int(sampleNameOrId) in [sample.id for sample in samples]):
		sampleId = int(sampleNameOrId);
		sampleName = str(samplesbyId[int(sampleId)].name)
	except ValueError: pass
	if type(sampleId) == int and type(sampleName) == str: pass
	elif   sampleNameOrId  in samplesbyName.keys():
	    sampleName = sampleNameOrId;
	    sampleId = samplesbyName[sampleName].id
	else:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# SampleName (or id) must be registered in the database, there is no sample with name or id '+str(sampleNameOrId)+' ('+str(type(sampleNameOrId))+') , exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

	#
	# open connection to database
	#
	self.getConnection()
	
	filePairId = None
	filePairIds = []
	
	#
	# check if any of the fastqs already in database
	#
	data = self.c.execute('SELECT filePairId,fastq1,fastq2 FROM fastqs').fetchall()
	if data:
	    for filePair in data:
		filePairId = int(filePair[0])
		filePairIds.append(filePairId)
		for fastq in [fastq1, fastq2]:
		    if fastq in filePair:
			message = 'ERROR: '+fastq+' already in the database.\nExiting after error.'
			print message
			AnalysisPipe.logfile.write(message+'\n')
			sys.exit(1)
	#
	# if not in the database add a new row
	#
	AnalysisPipe.logfile.write('Getting readcount for file'+fastq1+' ... \n')
	readCount = 'Unknown'#bufcount(fastq1)/4 #one read is four lines
	AnalysisPipe.logfile.write('...done. The file has '+str(readCount)+' reads.\n')
	addedToReadsTable = False#SEAseqPipeLine.startTimeStr
	minReadLength = 'NA'

	if filePairIds: filePairId = max(filePairIds)+1
	else: filePairId = 0
	values = (filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId)
	self.c.execute('INSERT INTO fastqs VALUES (?,?,?,?,?,?,?)', values)
	
	self.commitAndClose()
	
	return 0

    def getFastqs(self,):
	#
	# Imports
	#
	import sys
	
	#
	# open connection to database
	#
	self.getConnection()
		
	#
	# get att data in fastqs table
	#
	filePairs = self.c.execute('SELECT filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId FROM fastqs').fetchall()
	
	self.commitAndClose()
	
	#return [[readCount,fastq1,fastq2] if (not addedToReadsTable) else None for filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength in filePairs]
	return [[filePairId,readCount,fastq1,fastq2,sampleId] for filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,sampleId in filePairs]

    def getRuns(self, runTypes):

	self.getConnection()

	runsInfo = []
	data = self.c.execute('SELECT * FROM runs').fetchall()
	for startTime, command, commandLine, finishedSuccessfully, masterPid in data:
	    if command in runTypes: runsInfo.append([startTime, command, commandLine, finishedSuccessfully, masterPid])

	self.commitAndClose()

	return runsInfo

class Sample(object):

    def __init__(self, sampleName=None,sampleId=None,refType=None):
	self.name = sampleName
	self.id = int(sampleId)
	self.refType = refType
	self.path = AnalysisPipe.path+'/samples/'+self.name
	self.scriptPath = AnalysisPipe.path+'/samples/'+self.name+'/script'
	self.dataPath   = AnalysisPipe.path+'/samples/'+self.name+'/data'
	self.logPath    = AnalysisPipe.path+'/samples/'+self.name+'/logs'
	self.fastqcPath = AnalysisPipe.path+'/samples/'+self.name+'/fastQC'
	self.dependencies = {}

    @property
    def readCount(self, ):
	tmpCounter = 0
	for filePairId,readCount,fastq1,fastq2,sampleId in AnalysisPipe.database.getFastqs():
	    if int(sampleId) == self.id:
		try: tmpCounter+= readCount
		except TypeError: return 'Unknown'
	return tmpCounter

    def getFastqs(self):
	self.fastqIds = []
	for filePairId,readCount,fastq1,fastq2,sampleId in AnalysisPipe.database.getFastqs():
	    if int(sampleId) == self.id:
		self.fastqIds.append(filePairId)
		yield [filePairId,readCount,fastq1,fastq2,sampleId]

    def createDirs(self):
        import os
        try: os.makedirs(self.path)
        except OSError:pass
        try: os.makedirs(self.scriptPath)
        except OSError:pass
        try: os.makedirs(self.dataPath)
        except OSError:pass
        try: os.makedirs(self.fastqcPath)
        except OSError:pass
        try: os.makedirs(self.logPath)
        except OSError:pass

    def trimFastqs(self):
	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():

	    import sys
	    import time
	    try: project = sys.argv[3]
	    except IndexError:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	    #
	    # sbatch header
	    #
	    output = ''
	    output += '#! /bin/bash -l'+'\n'
	    output += '#SBATCH -A '+project+'\n'
	    output += '#SBATCH -n 2 -p core'+'\n'
	    output += '#SBATCH -t 72:00:00'+'\n'
	    output += '#SBATCH -J trim.'+self.name+'.'+str(filePairId)+'\n'
	    output += '#SBATCH -e '+self.logPath+'/stderr.trimming.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    output += '#SBATCH -o '+self.logPath+'/stdout.trimming.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    
	    try:
		output += '#SBATCH --mail-type=All'+'\n'
		output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	    except IndexError: pass
	    
	    #
	    # define variebles and go to path
	    #
	    output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	    output += 'cd '+self.path+'\n'
	    output += 'echo "-----"'+'\n'
	    
	    output += 'module load bioinfo-tools FastQC #cutadapt/1.5.0'+'\n'
	    output += 'workon py2.7\n\n'

	    #
	    # WGA adapter trimming
	    #
	    output += ''+AnalysisPipe.scriptPath+'/wgaAdapterTrimmer.py -i '+fastq1+' > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq 2> '+self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += ''+AnalysisPipe.scriptPath+'/wgaAdapterTrimmer.py -i '+fastq2+' > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq 2> '+self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'
	    
	    output += '\n'
	    output += 'cutadapt -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq  2> '+self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += 'cutadapt -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq  2> '+self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'
	    output += '\n'
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed.fq\n'
	    
	    #
	    # illumina  adapter trimming
	    #
	    adaptersToTrim = '-a CTGTCTCTTATACACATCTGACGCTGCCGACGA -a CTGTCTCTTATACACATCTCCGAGCCCACGAGAC -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -a AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT'
	    output += 'cutadapt -n 3 '+adaptersToTrim+' '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq 2> '+self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += 'cutadapt -n 3 '+adaptersToTrim+' '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq 2> '+self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'

	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaTrimmed2.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaTrimmed2.fq \n'
	    output += 'wait\n'
	    
	    #
	    # quality trimmming
	    #
	    output += ''+AnalysisPipe.scriptPath+'/TrimBWAstyle.pl -q 20 '+self.dataPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq 2> '+self.logPath+'/qualityTrimming.'+str(filePairId)+'.r1.log.txt &\n'
	    output += ''+AnalysisPipe.scriptPath+'/TrimBWAstyle.pl -q 20 '+self.dataPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq > '+self.dataPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq 2> '+self.logPath+'/qualityTrimming.'+str(filePairId)+'.r2.log.txt &\n'
	    output += 'wait\n'
	    
	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaAndilluminaTrimmed.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaAndilluminaTrimmed.fq\n'
	    output += 'wait\n'
	    
	    #
	    # remove empty or "N" only sequences
	    #
	    output += 'python '+AnalysisPipe.scriptPath+'/removeEmptyReads.py '
	    output += self.dataPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq '
	    output += self.dataPath+'/'+str(filePairId)+'.singletts.fq '
	    output += '>&2 2> '+self.logPath+'/removeEmptyReads.'+str(filePairId)+'.log.txt\n'
	    
	    #
	    # remove temp files
	    #
	    output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.wgaIlluminaAndQualityTrimmed.fq '+self.dataPath+'/'+str(filePairId)+'.r2.wgaIlluminaAndQualityTrimmed.fq\n'
	    output += 'wait\n'
	    
	    #
	    # compress files
	    #
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq &\n'
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq  &\n'
	    output += 'gzip -v9 '+self.dataPath+'/'+str(filePairId)+'.singletts.fq &\n'
	    output += 'wait\n'
	    
	    #
	    # FASTQC
	    #
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq.gz &\n'
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq.gz &\n'
	    output += 'fastqc '+self.dataPath+'/'+str(filePairId)+'.singletts.fq.gz &\n'
	    output += 'wait\n'
	    output += 'mv -v '+self.dataPath+'/*fastqc* '+self.fastqcPath+'/\n'
	    
	    #
	    # Final output and write script to file
	    #
            output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
	    output += 'echo'+'\n'
	    output += 'wait'+'\n'
	    output += 'echo "$(date) AllDone"'+'\n'
	    output += 'echo "$(date) AllDone" >&2'+'\n'
	    with open(self.scriptPath+'/trimming.'+self.name+'.'+str(filePairId)+'.sh','w') as outfile: outfile.write(output)

    def mapFastqs(self):

	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():

	    import sys
	    import time
	    try: project = sys.argv[3]
	    except IndexError:
		msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
		AnalysisPipe.logfile.write(msg)
		sys.stderr.write(msg)
		sys.exit(1)

	    #
	    # sbatch header
	    #
	    output = '#! /bin/bash -l'+'\n'
	    output += '#SBATCH -A '+project+'\n'
	    output += '#SBATCH -n 16 -p node'+'\n'
	    output += '#SBATCH -t 5:00:00'+'\n'
	    output += '#SBATCH -J map.'+self.name+'.'+str(filePairId)+'\n'
	    output += '#SBATCH -e '+self.logPath+'/stderr.mapping.'+self.name+'.'+str(filePairId)+'.txt'+'\n'
	    output += '#SBATCH -o '+self.logPath+'/stdout.mapping.'+self.name+'.'+str(filePairId)+'.txt'+'\n'

	    try:
		output += '#SBATCH --mail-type=All'+'\n'
		output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	    except IndexError: pass

	    #
	    # define variebles and go to path
	    #
	    output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	    output += 'cd '+self.path+'\n'
	    output += 'echo'+'\n'

	    #
	    # Bowtie2 mapping
	    #output += 'module load bioinfo-tools bwa/0.7.8\n'
	    #output += 'bwa mem -t 16 /sw/data/uppnex/reference/Homo_sapiens/GRCh37/program_files/bwa/concat.fa '+self.r1files[0]+' '+self.r2files[0]+' > '+self.sam+'\n'
	    #output += 'bowtie2 -1 '+self.r1files[0]+' -2 '+self.r2files[0]+' --very-sensitive-local -p16 -x '+self.reference+' > '+self.sam+'\n'
	    output += 'bowtie2 --maxins 2000 -p16 '
	    output += '-1 '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq.gz '
	    output += '-2 '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq.gz '
	    output += '-x '+AnalysisPipe.bowtie2Reference+' '
	    output += '> '+self.dataPath+'/'+str(filePairId)+'.sam '
	    output += '2> '+self.logPath+'/stderr.bowtie2.'+str(filePairId)+'.txt \n'
	    output += 'echo -e "mapping Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
	    #output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r1.allTrimmed.fq.gz'+'\n'
	    #output += 'rm -v '+self.dataPath+'/'+str(filePairId)+'.r2.allTrimmed.fq.gz'+'\n'
	    

	    #
	    # Final output and write script to file
	    #
	    output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
	    output += 'wait'+'\n'
	    output += 'echo "$(date) AllDone"'+'\n'
	    output += 'echo "$(date) AllDone" >&2'+'\n'
	    with open(self.scriptPath+'/mapping.'+self.name+'.'+str(filePairId)+'.sh','w') as outfile: outfile.write(output)

    def mergeMapped(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	#
	# sbatch header
	#
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 5:00:00'+'\n'
	output += '#SBATCH -J merge.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.merge.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.merge.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass

	#
	# define variebles and go to path
	#
	output += 'echo "$(date) Running on: $(hostname)"'+'\n'
	output += 'cd '+self.path+'\n'
	output += 'echo'+'\n'

	#
	# merge
	#
	inputFiles = ' INPUT='+' INPUT='.join([self.dataPath+'/'+str(filePairId)+'.sam' for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs()])
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/MergeSamFiles.jar '+inputFiles+' OUTPUT='+self.dataPath+'/'+self.name+'.merged.sam '
	output += '1>&2  2>  '+self.logPath+'/stderr.merging.'+self.name+'.txt \n'
	output += 'echo -e "mapping Done. $(date) Running on: $(hostname)" 1>&2'+'\n'

	#
	# Final output and write script to file
	#
	output += 'wait'+'\n'
	output += 'echo "$(date) AllDone"'+'\n'
	output += 'echo "$(date) AllDone" >&2'+'\n'
	with open(self.scriptPath+'/mergeMapped.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def filterAndFixMerged(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 5:00:00'+'\n'
	output += '#SBATCH -J fnf.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.filterAndFix.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.filterAndFix.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass

        #
        # define variebles and go to path
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'cd '+self.path+'\n'
        output += 'echo'+'\n'
        
        #
        # convert to bam file
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/SamFormatConverter.jar MAX_RECORDS_IN_RAM=2500000 '
	output += 'INPUT='+ self.dataPath+'/'+self.name+'.merged.sam '
	output += 'OUTPUT='+self.dataPath+'/'+self.name+'.merged.bam '
	output += '1>&2  2> '+self.logPath+'/stderr.sam2bam.'+self.name+'.txt \n'
        output += 'echo -e "sam2bam Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.merged.sam\n'

        #
        # sort the bam file
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/SortSam.jar MAX_RECORDS_IN_RAM=2500000 SORT_ORDER=coordinate '
	output += 'INPUT='+ self.dataPath+'/'+self.name+'.merged.bam '
        output += 'OUTPUT='+self.dataPath+'/'+self.name+'.sorted.bam '
	output += 'CREATE_INDEX=true 1>&2  2> '
	output += self.logPath+'/stderr.sortBam.'+self.name+'.txt \n'
        output += 'echo -e "bam2sort Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.merged.bam\n'

        #
        # mark duplicates
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/MarkDuplicates.jar MAX_RECORDS_IN_RAM=2500000 VALIDATION_STRINGENCY=LENIENT '
	output += 'INPUT='+ self.dataPath+'/'+self.name+'.sorted.bam '
	output += 'OUTPUT='+self.dataPath+'/'+self.name+'.marked.bam '
	output += 'METRICS_FILE='+self.logPath+'/markDuplicatesMetrix.'+self.name+'.txt '
	output += '1>&2  2> '+self.logPath+'/stderr.markDuplicates.'+self.name+'.txt \n'
        output += 'echo -e "mark Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.sorted.bam\n'

        #
        # fix missing information
        #
        output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/AddOrReplaceReadGroups.jar '
        output += 'MAX_RECORDS_IN_RAM=2500000 '
        output += 'INPUT='+ self.dataPath+'/'+self.name+'.marked.bam '
        output += 'OUTPUT='+self.dataPath+'/'+self.name+'.fixed.bam '
        output += 'CREATE_INDEX=true RGID='+self.name+' RGLB='+self.name+' RGPL=ILLUMINA RGSM='+self.name+' RGCN="NA" RGPU="NA"'+'  '
	output += '1>&2  2> '+self.logPath+'/stderr.addAndReplaceReadGroups.'+self.name+'.txt \n'
        output += 'echo "addorreplace Done. $(date) Running on: $(hostname)" 1>&2'+'\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.marked.bam\n'

        #
        # samtools flagstat
        #
        output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.fixed.bam'+' > '+self.logPath+'/fixedBamFlagstat.'+self.name+'.txt \n'
        output += 'echo "flagstat Done. $(date) Running on: $(hostname)" 1>&2'+'\n'

        #
        # Final output and write script to file
        #
        output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
        output += 'wait'+'\n'
        output += 'echo "$(date) AllDone"'+'\n'
        output += 'echo "$(date) AllDone" >&2'+'\n'
	with open(self.scriptPath+'/FilterAndFix.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def realignerTargetCreator(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #

	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 16 -p node'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J realTC.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.realTC.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.realTC.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass

        #
        # define variebles and go to path
        #
        output += 'echo "$(date) Running on: $(hostname)"\n'

        #
        # Find targets for indel realignment
        #
        output += 'echo -e "-> RealignerTargetCreator <-"\n'
        output += 'java -Xmx72g -jar '+AnalysisPipe.gatkLocation+' -T RealignerTargetCreator '
	output += '-nt 16 '
	output += '-I '+self.dataPath+'/'+self.name+'.fixed.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-o '+self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/Mills_and_1000G_gold_standard.indels.b37.vcf'
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/1000G_phase1.indels.b37.vcf '
	output += '1>&2 2> '+self.logPath+'/stderr.RealignerTargetCreator.'+self.name+'.txt;'
        output += '\n'
        
        with open(self.scriptPath+'/realignerTargetCreator.'+self.name+'.sh','w') as outfile: outfile.write(output)
    
    def reAlignAndReCalibrate(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J reAlign.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.reAlign.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.reAlign.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'

        #
        # Realign reads around indels
        #
        output += 'echo -e "-> IndelRealigner <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T IndelRealigner '
	output += '-I '+self.dataPath+'/'+self.name+'.fixed.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-targetIntervals '+self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
        output += ' -o '+self.dataPath+'/'+self.name+'.reAligned.bam'+' '
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/Mills_and_1000G_gold_standard.indels.b37.vcf'
        output += ' -known '+AnalysisPipe.gatkBundleLocation+'/1000G_phase1.indels.b37.vcf  '
	output += '1>&2 2> '+self.logPath+'/stderr.indelRealigner.'+self.name+'.txt;'+'\n'
        output += '\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'echo "$(date) Running on: $(hostname)"\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.fixed.bam'+'\n'
        
        #
        # Quality recalibration
        #
        output += 'echo -e "-> BaseRecalibrator <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T BaseRecalibrator '
	output += '-I '+self.dataPath+'/'+self.name+'.reAligned.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-o '+self.dataPath+'/'+self.name+'.BQSR.grp'+' '
        output += ' -knownSites '+AnalysisPipe.gatkBundleLocation+'/dbsnp_138.b37.vcf '
	output += '1>&2 2> '+self.logPath+'/stderr.baseRecalibrator.'+self.name+'.txt;'+'\n'

        output += '\n'
        output += 'echo -e "-> PrintReads <-"\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T PrintReads '
	output += '-I '+self.dataPath+'/'+self.name+'.reAligned.bam'+' '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
	output += '-BQSR '+self.dataPath+'/'+self.name+'.BQSR.grp'+' '
	output += '-o '+self.dataPath+'/'+self.name+'.reCalibrated.bam'+' '
	output += '1>&2 2> '+self.logPath+'/stderr.printreads.txt ;\n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.reAligned.bam'+'\n'
        output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.reCalibrated.bam > '+self.logPath+'/reCalibratedBamFlagstat.'+self.name+'.txt \n'

	output += 'samtools view -b -F 4 '   +self.dataPath+'/'+self.name+'.reCalibrated.bam > '+self.dataPath+'/'+self.name+'.unmapRemoved.bam  2> '+self.logPath+'/stderr.samtoolsView.removeUnmap.'+self.name+'.txt \n'
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.dataPath+'/'+self.name+'.unmapRemoved.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex1.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.unmapRemoved.bam > '+self.logPath+'/unmapRemovedBamFlagstat.'+self.name+'.txt \n'
        output += 'rm -v '+self.dataPath+'/'+self.name+'.reCalibrated.bam\n'

	output += 'samtools view -b -q 20 '  +self.dataPath+'/'+self.name+'.unmapRemoved.bam > '+self.dataPath+'/'+self.name+'.qualFiltered.bam  2> '+self.logPath+'/stderr.samtoolsView.qualFilter.'+self.name+'.txt \n'
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.dataPath+'/'+self.name+'.qualFiltered.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex2.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.qualFiltered.bam > '+self.logPath+'/qualFilteredBamFlagstat.'+self.name+'.txt \n'
	output += 'rm -v '+self.dataPath+'/'+self.name+'.unmapRemoved.bam\n'

	output += 'samtools view -b -F 1024 '+self.dataPath+'/'+self.name+'.qualFiltered.bam > '+self.dataPath+'/'+self.name+'.noDuplicates.bam  2> '+self.logPath+'/stderr.samtoolsView.removeDups.'+self.name+'.txt \n'
	output += 'java -Xmx5g -jar '+AnalysisPipe.picardLocation+'/BuildBamIndex.jar INPUT='+self.dataPath+'/'+self.name+'.noDuplicates.bam '+'1>&2  2>  '+self.logPath+'/stderr.buildIndex3.'+self.name+'.txt \n'
	output += 'samtools flagstat '+self.dataPath+'/'+self.name+'.noDuplicates.bam > '+self.logPath+'/noDuplicatesBamFlagstat.'+self.name+'.txt \n'
	output += 'rm -v '+self.dataPath+'/'+self.name+'.qualFiltered.bam\n'

        #
        # Final output and write script to file
        #
        output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'wait\n'
        output += 'echo "$(date) AllDone"\n'
        with open(self.scriptPath+'/reAlignAndReCalibrate.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def haplotypeCalling(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J hapCal.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.haplotypeCalling.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.haplotypeCalling.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'

        output += 'echo "HC" '+'\n'
        
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' '
        output += '-T HaplotypeCaller '
	output += '-R '+AnalysisPipe.bowtie2Reference+' '
        output += '-I '+self.dataPath+'/'+self.name+'.noDuplicates.bam '
        output += '--genotyping_mode DISCOVERY '
        output += '-stand_emit_conf 10 '
        output += '-stand_call_conf 30 '
        if wgsOrExome == 'exome': output += '-L '+AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem.bed '
	elif wgsOrExome == 'wgs': output += '-L '+AnalysisPipe.referencePath+'/wgs.bed '
        output += '--dbsnp '+AnalysisPipe.gatkBundleLocation+'/dbsnp_138.b37.vcf '
        output += '--annotation AlleleBalance --annotation AlleleBalanceBySample --annotation BaseCounts --annotation BaseQualityRankSumTest '
        output += '--annotation ChromosomeCounts --annotation ClippingRankSumTest --annotation Coverage --annotation DepthPerAlleleBySample '
        output += '--annotation DepthPerSampleHC --annotation FisherStrand --annotation GCContent --annotation HaplotypeScore --annotation HardyWeinberg '
        output += '--annotation HomopolymerRun --annotation InbreedingCoeff --annotation LikelihoodRankSumTest --annotation LowMQ '
        output += '--annotation MVLikelihoodRatio --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation MappingQualityZeroBySample '
        output += '--annotation NBaseCount --annotation QualByDepth --annotation RMSMappingQuality --annotation ReadPosRankSumTest --annotation SampleList '
        output += '--annotation SnpEff --annotation SpanningDeletions --annotation StrandBiasBySample --annotation TandemRepeatAnnotator '
        output += '--annotation TransmissionDisequilibriumTest --annotation VariantType '#--annotation StrandOddsRatio 
        output += '--emitRefConfidence GVCF --variant_index_type LINEAR --variant_index_parameter 128000 '
        output += '-o '+self.dataPath+'/'+self.name+'.gvcf '
	output += '1>&2 2> '+self.logPath+'/stderr.haplotypeCallerGatk.'+self.name+'.txt &'+'\n'
        output += 'wait'+'\n'

        #
        # Final output and write script to file
        #
        output += 'echo "Done. $(date) Running on: $(hostname)"\n'
        output += 'wait\n'
        output += 'echo "$(date) AllDone"\n'
        with open(self.scriptPath+'/haplotypeCalling.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def qcSteps(self):
	import sys
	import time
	try: project = sys.argv[3]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a project number for the creation of sbatch scripts, exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)
	try: wgsOrExome = sys.argv[4]
	except IndexError:
	    msg = '#ERROR_MSG#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# You must give a type for the anaylis wgs or exome, now exiting.\n'
	    AnalysisPipe.logfile.write(msg)
	    sys.stderr.write(msg)
	    sys.exit(1)

        #
        # sbatch header
        #
	output = '#! /bin/bash -l'+'\n'
	output += '#SBATCH -A '+project+'\n'
	output += '#SBATCH -n 1 -p core'+'\n'
	output += '#SBATCH -t 72:00:00'+'\n'
	output += '#SBATCH -J qcSteps.'+self.name+'\n'
	output += '#SBATCH -e '+self.logPath+'/stderr.qcSteps.'+self.name+'.txt'+'\n'
	output += '#SBATCH -o '+self.logPath+'/stdout.qcSteps.'+self.name+'.txt'+'\n'

	try:
	    output += '#SBATCH --mail-type=All'+'\n'
	    output += '#SBATCH --mail-user='+sys.argv[5]+'\n'
	except IndexError: pass
        output += 'echo "$(date) Running on: $(hostname)"\n'

        #
        # GATK callable Loci
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo -e "-> CallableLoci <-"'+'\n'
        output += 'java -Xmx5g -jar '+AnalysisPipe.gatkLocation+' -T CallableLoci '
	output +='-I '+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output +='-summary '+self.dataPath+'/'+self.name+'.callableLociSummary.txt '
	output +='-o '+self.dataPath+'/'+self.name+'.callableLoci.bed '
	output +='-R '+AnalysisPipe.bowtie2Reference+' '+'\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'
        output += 'echo'+'\n'
        output += 'echo "-----"'+'\n'

        #
        # qacompute
        #
        output += 'echo "$(date) Running on: $(hostname)"'+'\n'
        output += 'echo -e "-> Pauls qacompute <-"'+'\n'
        output += '/proj/b2010052/scripts/qaCompute -d -q 10 '
	output += '-m '+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output += self.dataPath+'/'+self.name+'.qacompute.out '
	output += '> '+self.logPath+'/'+self.name+'.qacompute.stdout.txt '
	output += '2> '+self.logPath+'/'+self.name+'.qacompute.stderr.txt '+'\n'
        output += 'echo "Done. $(date) Running on: $(hostname)"'+'\n'

        #
        # picard HS metrics
        #
        output += 'java -Xmx3g -jar '+AnalysisPipe.picardLocation+'/CalculateHsMetrics.jar '
	if wgsOrExome == 'exome':output += 'BAIT_INTERVALS='  +AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem '
	elif wgsOrExome == 'wgs':output += 'BAIT_INTERVALS='  +AnalysisPipe.referencePath+'/wgs '
	if wgsOrExome == 'exome':output += 'TARGET_INTERVALS='+AnalysisPipe.referencePath+'/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem '
	elif wgsOrExome == 'wgs':output += 'TARGET_INTERVALS='+AnalysisPipe.referencePath+'/wgs '
	output += 'INPUT='+self.dataPath+'/'+self.name+'.noDuplicates.bam '
	output += 'OUTPUT='+self.dataPath+'/'+self.name+'.hs_metrics.summary.txt '
	output += 'PER_TARGET_COVERAGE='+self.dataPath+'/'+self.name+'.hs_metrics.perTargetCoverage.txt '
	output += 'REFERENCE_SEQUENCE='+AnalysisPipe.bowtie2Reference+'  '
	output += '1>&2 2> '+self.logPath+'/'+self.name+'.stderr.caluclateHsmetrics.txt \n'


	#
	# make files for coverage checks
	#
	if wgsOrExome == 'exome':output += "bedtools coverage -abam "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" -b "+AnalysisPipe.referencePath+"/truseq_exome_targeted_regions.hg19.bed.chr.columnReOrdered.withHeader.chrRem.bed -d > "+self.dataPath+'/'+self.name+'.bedtools.coverage.bed\n'
	elif wgsOrExome == 'wgs':output += "bedtools coverage -abam "+self.dataPath+'/'+self.name+'.noDuplicates.bam'+" -b "+AnalysisPipe.referencePath+"/wgs.bed -d > "+self.dataPath+'/'+self.name+'.bedtools.coverage.bed\n'
	output += "awk '{print $7}' "+self.dataPath+'/'+self.name+'.bedtools.coverage.bed'+" | sort -n | uniq -c | awk '{print $1\"\\t\"$2}'> "+self.dataPath+'/'+self.name+".coverageDistribution.tsv\n"
	output += "awk '{print $1 \"\\t\" $2+$6-1 \"\\t\" $7}' "+self.dataPath+'/'+self.name+'.bedtools.coverage.bed'+" > "+self.dataPath+'/'+self.name+".depthPerPosition.tsv\n"

	
        #
        # Final output and write script to file
        #
        output += '\n'+AnalysisPipe.programPath+' '+AnalysisPipe.path+' report\n'
	output += 'echo'+'\n'
        output += 'wait'+'\n'
        output += 'echo "$(date) AllDone"'+'\n'
        output += 'echo "$(date) AllDone" >&2'+'\n'
        with open(self.scriptPath+'/qcSteps.'+self.name+'.sh','w') as outfile: outfile.write(output)

    def updateOrgReadCounts(self,):
	AnalysisPipe.database.updateFastqReadCount(self)

    def getStats(self):
	
	import re
	
	self.updateOrgReadCounts()
	
	stats = {}
	stats['rubiconWgaTrimming'] = {}
	stats['malbacWgaTrimming']  = {}
	stats['illuminaAndNexteraTrimming']  = {}
	stats['qualityTrimming']  = {}
	stats['removeEmptyReads']  = {}
	stats['bowtie2'] = {}
	
	for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
	    stats['rubiconWgaTrimming'][filePairId] = {'r1':None,'r2':None}
	    stats['malbacWgaTrimming'][filePairId]  = {'r1':None,'r2':None}
	    stats['illuminaAndNexteraTrimming'][filePairId]  = {'r1':None,'r2':None}
	    stats['qualityTrimming'][filePairId]    = {'r1':None,'r2':None}
	    
	    for read in ['r1','r2']:
		stats['rubiconWgaTrimming'][filePairId][read]         = extractData(infile=self.logPath+'/rubiconWgaTrimming.'+str(filePairId)+'.'+read+'.log.txt',        pattern="Running wgaAdapterTrimmer.py\nProcessed a total of\t(?P<totalReads>\d+)\treads. \(.+\)\nProcessed a total of\t(?P<totalBases>\d+)\tbases \(.+\).\ntrimmed a total of\t(?P<trimmedBases>\d+)\tbases in the start of reads \(.+\).\nwgaAdapterTrimmer.py done exiting ...\n?")
		stats['malbacWgaTrimming'][filePairId][read]          = extractData(infile=self.logPath+'/malbacWgaTrimming.'+str(filePairId)+'.'+read+'.log.txt',         pattern="cutadapt version .+\nCommand line parameters: -n 10 -g GTGAGTGATGGTTGAGGTAGTGTGGAG -a CTCCACACTACCTCAACCATCACTCAC .+\nMaximum error rate\: .+\%\n\s+No. of adapters\: 2\n\s+Processed reads\:\s+(?P<totalReads>\d+)\n\s+Processed bases\:\s+(?P<totalBases>\d+) bp \(.+ Mbp\)\n\s+Trimmed reads\:\s+(?P<trimmedReads>\d+) \(.+\%\)\n\s+Trimmed bases\:\s+(?P<trimmedBases>\d+) bp \(.+ Mbp\) \(.+\% of total\)\n\s+Too short reads\:\s+.+ \(.+\% of processed reads\)\n\s+Too long reads\:\s+.+ \(.+\% of processed reads\)\n\s+Total time\:\s+.+ s\n\s+Time per read\:\s+.+ ms")
		stats['illuminaAndNexteraTrimming'][filePairId][read] = extractData(infile=self.logPath+'/illuminaAndNexteraTrimming.'+str(filePairId)+'.'+read+'.log.txt',pattern="cutadapt version .+\nCommand line parameters: -n .+\nMaximum error rate\: .+\%\n\s+No. of adapters\: 4\n\s+Processed reads\:\s+(?P<totalReads>\d+)\n\s+Processed bases\:\s+(?P<totalBases>\d+) bp \(.+ Mbp\)\n\s+Trimmed reads\:\s+(?P<trimmedReads>\d+) \(.+\%\)\n\s+Trimmed bases\:\s+(?P<trimmedBases>\d+) bp \(.+ Mbp\) \(.+\% of total\)\n\s+Too short reads\:\s+.+ \(.+\% of processed reads\)\n\s+Too long reads\:\s+.+ \(.+\% of processed reads\)\n\s+Total time\:\s+.+ s\n\s+Time per read\:\s+.+ ms")
		stats['qualityTrimming'][filePairId][read]            = extractData(infile=self.logPath+'/qualityTrimming.'+str(filePairId)+'.'+read+'.log.txt',           pattern='(?P<totalBasess>\d+)\tbases\n(?P<trimmedBases>\d+)\ttrimmed')

	    stats['removeEmptyReads'][filePairId] = extractData(infile=self.logPath+'/removeEmptyReads.'+str(filePairId)+'.log.txt',pattern="""Running removeEmptyReads.py:\nHeader one is empty exiting.\n(?P<totalReads>\d+) read pairs processed.\n(?P<pairsOut>\d+) read pairs to outfiles .+.\n(?P<singlets>\d+) single reads to outfile .+.\nremoveEmptyReads Exiting.""")
	    stats['bowtie2'][filePairId]          = extractData(infile=self.logPath+'/stderr.bowtie2.'+str(filePairId)+'.txt',      pattern="""(?P<totalReads>\d+) reads; of these:\n\s+(?P<pairedReads>\d+) \(\d+.\d+\%\) were paired; of these:\n\s+(?P<notPropMapedPair>\d+) \(\d+.\d+\%\) aligned concordantly 0 times\n\s+(?P<properPairs>\d+) \(\d+.\d+\%\) aligned concordantly exactly 1 time\n\s+(?P<properPairsMultiMap>\d+) \(\d+.\d+\%\) aligned concordantly >1 times\n\s+----\n\s+(?P<notPropMapedPair2>\d+) pairs aligned concordantly 0 times; of these:\n\s+(?P<discordantPairs>\d+) \(\d+.\d+\%\) aligned discordantly 1 time\n\s+----\n\s+(?P<unMappedPair>\d+) pairs aligned 0 times concordantly or discordantly; of these:\n\s+(?P<possibleSingletons>\d+) mates make up the pairs; of these:\n\s+(?P<unMappedReads>\d+) \(\d+.\d+\%\) aligned 0 times\n\s+(?P<singleSingleMap>\d+) \(\d+.\d+\%\) aligned exactly 1 time\n\s+(?P<singleMultiMap>\d+) \(\d+.\d+\%\) aligned >1 times\n(?P<overallAlignmentRate>\d+.\d+)\% overall alignment rate""")

	stats['merging'] = extractData(infile=self.logPath+'/stderr.merging.'+self.name+'.txt',pattern="Finished reading inputs.+\n.+picard.sam.MergeSamFiles done. Elapsed time",checkType='program')
	pattern = 'LIBRARY\tUNPAIRED_READS_EXAMINED\tREAD_PAIRS_EXAMINED\tUNMAPPED_READS\tUNPAIRED_READ_DUPLICATES\tREAD_PAIR_DUPLICATES\tREAD_PAIR_OPTICAL_DUPLICATES\tPERCENT_DUPLICATION\tESTIMATED_LIBRARY_SIZE\n(?P<LIBRARY>.+)\t(?P<UNPAIRED_READS_EXAMINED>\d+)\t(?P<READ_PAIRS_EXAMINED>\d+)\t(?P<UNMAPPED_READS>\d+)\t(?P<UNPAIRED_READ_DUPLICATES>\d+)\t(?P<READ_PAIR_DUPLICATES>\d+)\t(?P<READ_PAIR_OPTICAL_DUPLICATES>\d+)\t(?P<PERCENT_DUPLICATION>\d+\,\d+)\t(?P<ESTIMATED_LIBRARY_SIZE>\d+)'
	oldpattern="""LIBRARY\tUNPAIRED_READS_EXAMINED\tREAD_PAIRS_EXAMINED\tUNMAPPED_READS\tUNPAIRED_READ_DUPLICATES\tREAD_PAIR_DUPLICATES\tREAD_PAIR_OPTICAL_DUPLICATES\tPERCENT_DUPLICATION\tESTIMATED_LIBRARY_SIZE\n(?P<Library>.+)\s+(?P<unPairedReads>\d+)\s+(?P<totalReads>\d+)\s+(?P<unMapped>\d+)\s+(?P<unPairedDups>\d+)\s+(?P<pairDups>\d+)\s+(?P<opticalDups>\d+)\s+(?P<percentageDuplication>\d+\,\d+)\s+(?P<estLibSize>\d+)"""
	stats['markDuplicatesMetrix'] = extractData(infile=self.logPath+'/markDuplicatesMetrix.'+self.name+'.txt',pattern=pattern)
	stats['fixedBamFlagstat']        = extractData(infile=self.logPath+'/fixedBamFlagstat.'+self.name+'.txt',       pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['reCalibratedBamFlagstat'] = extractData(infile=self.logPath+'/reCalibratedBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['unmapRemovedBamFlagstat'] = extractData(infile=self.logPath+'/unmapRemovedBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['qualFilteredBamFlagstat'] = extractData(infile=self.logPath+'/qualFilteredBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	stats['noDuplicatesBamFlagstat'] = extractData(infile=self.logPath+'/noDuplicatesBamFlagstat.'+self.name+'.txt',pattern="""(?P<totalReads>\d+) \+ 0 in total \(QC-passed reads \+ QC-failed reads\)\n(?P<duplicates>\d+) \+ 0 duplicates\n(?P<mapped>\d+) \+ 0 mapped \(\d+.\d+\%:-nan\%\)\n(?P<paired>\d+) \+ 0 paired in sequencing\n(?P<read1>\d+) \+ 0 read1\n(?P<read2>\d+) \+ 0 read2\n(?P<properlyPaired>\d+) \+ 0 properly paired \(\d+.\d+\%:-nan\%\)\n(?P<bothMapped>\d+) \+ 0 with itself and mate mapped\n(?P<singletons>\d+) \+ 0 singletons \(\d+.\d+\%:-nan\%\)\n(?P<mateOnDiffChr>\d+) \+ 0 with mate mapped to a different chr\n(?P<mateOnDiffChrq5>\d+) \+ 0 with mate mapped to a different chr \(mapQ>=5\)""")
	
	#self.logPath+'/'+self.name+'.qacompute.stdout.txt'
	#self.logPath+'/'+self.name+'.qacompute.stderr.txt'
	#self.dataPath+'/'+self.name+'.qacompute.out '
	#self.logPath+'/'+self.name+'.stderr.caluclateHsmetrics.txt'
	pattern = 'READ_GROUP\n(?P<BAIT_SET>.+)\t(?P<GENOME_SIZE>\d+)\t(?P<BAIT_TERRITORY>\d+)\t(?P<TARGET_TERRITORY>\d+)\t(?P<BAIT_DESIGN_EFFICIENCY>\d+(\,\d+)?)\t(?P<TOTAL_READS>\d+)\t(?P<PF_READS>\d+)\t(?P<PF_UNIQUE_READS>\d+)\t(?P<PCT_PF_READS>\d+(\,\d+)?)\t(?P<PCT_PF_UQ_READS>\d+(\,\d+)?)\t(?P<PF_UQ_READS_ALIGNED>\d+)\t(?P<PCT_PF_UQ_READS_ALIGNED>\d+(\,\d+)?)\t(?P<PF_UQ_BASES_ALIGNED>\d+)\t(?P<ON_BAIT_BASES>\d+)\t(?P<NEAR_BAIT_BASES>\d+)\t(?P<OFF_BAIT_BASES>\d+)\t(?P<ON_TARGET_BASES>\d+)\t(?P<PCT_SELECTED_BASES>\d+(\,\d+)?)\t(?P<PCT_OFF_BAIT>\d+(\,\d+)?)\t(?P<ON_BAIT_VS_SELECTED>\d+(\,\d+)?)\t(?P<MEAN_BAIT_COVERAGE>\d+(\,\d+)?)\t(?P<MEAN_TARGET_COVERAGE>\d+(\,\d+)?)\t(?P<PCT_USABLE_BASES_ON_BAIT>\d+(\,\d+)?)\t(?P<PCT_USABLE_BASES_ON_TARGET>\d+(\,\d+)?)\t(?P<FOLD_ENRICHMENT>\d+(\,\d+)?)\t(?P<ZERO_CVG_TARGETS_PCT>\d+(\,\d+)?)\t(?P<FOLD_80_BASE_PENALTY>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_2X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_10X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_20X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_30X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_40X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_50X>(\?)|(\d+?(\,\d+)?))\t(?P<PCT_TARGET_BASES_100X>\d+(\,\d+)?)\t(?P<HS_LIBRARY_SIZE>(\s?)|(\d+(\,\d+)?))\t(?P<HS_PENALTY_10X>\d+(\,\d+)?)\t(?P<HS_PENALTY_20X>\d+(\,\d+)?)\t(?P<HS_PENALTY_30X>\d+(\,\d+)?)\t(?P<HS_PENALTY_40X>\d+(\,\d+)?)\t(?P<HS_PENALTY_50X>\d+(\,\d+)?)\t(?P<HS_PENALTY_100X>\d+(\,\d+)?)'
	stats['hs_metrics.summary'] = extractData(infile=self.dataPath+'/'+self.name+'.hs_metrics.summary.txt',pattern=pattern)

	# make sums
	import time
	import sys
	if not list(self.getFastqs()):
	    AnalysisPipe.logfile.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# No fastq files found for sample: '+self.name+' continuing with next sample.\n')
	    sys.stderr.write('#WARNING#'+time.strftime("%Y-%m-%d:%H:%M:%S",time.localtime())+'# No fastq files found for sample: '+self.name+' continuing with next sample.\n')
	    self.stats = stats
	    return
	for program in ['illuminaAndNexteraTrimming','malbacWgaTrimming','qualityTrimming','rubiconWgaTrimming']:
	    try:
    		sums = {variable:0 for variable in stats[program][self.getFastqs().next()[0]]['r1'].keys()}
		for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
		    for read in ['r1','r2']:
			for variable, value in stats[program][filePairId][read].iteritems(): sums[variable]+=float(value)
		stats[program]['sum']= sums
	    except AttributeError: pass
	for program in ['removeEmptyReads','bowtie2']:
	    try:
		sums = {variable:0 for variable in stats[program][self.getFastqs().next()[0]].keys()}
		for filePairId,readCount,fastq1,fastq2,sampleId in self.getFastqs():
		    for variable, value in stats[program][filePairId].iteritems(): sums[variable]+=float(value)
		stats[program]['sum']= sums
	    except AttributeError: pass
	    
	self.stats = stats

	# debug output
	#print "\n######## "+self.name+" ######## "
	#for key,value in stats.iteritems():
	#    print '    ',key
	#    try:
	#	for key2,value2 in value.iteritems():
	#	    assert type(value2) == dict
	#	    print '        ',key2,value2
	#    except:print '        ',value
	
	return 0

if __name__ == "__main__": main()


# Should add
# program checks
# exome coverage
# HS metrics summary

# Possible picard tols to utilize
# BamIndexStats
# CollectMultipleMetrics
#    CollectAlignmentSummaryMetrics
#    CollectInsertSizeMetrics
# CollectGcBiasMetrics
# EstimateLibraryComplexity
# CollectWgsMetrics or CalculateHsMetrics
# GenotypeConcordance

	### OUTFILES TO ADD CHECHS FOR LATER:
	#self.logPath+'/stderr.trimming.'+self.name+'.'+str(filePairId)+'.txt'
	#self.logPath+'/stdout.trimming.'+self.name+'.'+str(filePairId)+'.txt'
	#self.fastqcPath+'/\n'
	#self.logPath+'/stderr.mapping.'+self.name+'.'+str(filePairId)+'.txt'
	#self.logPath+'/stdout.mapping.'+self.name+'.'+str(filePairId)+'.txt'
	#self.logPath+'/stderr.merge.'+self.name+'.txt'
	#self.logPath+'/stdout.merge.'+self.name+'.txt'
	#self.logPath+'/stderr.filterAndFix.'+self.name+'.txt'
	#self.logPath+'/stdout.filterAndFix.'+self.name+'.txt'
	#self.logPath+'/stderr.sam2bam.'+self.name+'.txt'
	#self.logPath+'/stderr.sortBam.'+self.name+'.txt'
	#self.logPath+'/stderr.markDuplicates.'+self.name+'.txt'
	#self.logPath+'/stderr.addAndReplaceReadGroups.'+self.name+'.txt'
	#self.logPath+'/stderr.realTC.'+self.name+'.txt'
	#self.logPath+'/stdout.realTC.'+self.name+'.txt'
	#self.logPath+'/stderr.RealignerTargetCreator.'+self.name+'.txt;'
	#self.logPath+'/stderr.reAlign.'+self.name+'.txt'
	#self.logPath+'/stdout.reAlign.'+self.name+'.txt'
	#self.logPath+'/stderr.indelRealigner.'+self.name+'.txt;'+''
	#self.logPath+'/stderr.baseRecalibrator.'+self.name+'.txt;'+''
	#self.logPath+'/stderr.printreads.txt
	#self.logPath+'/stderr.samtoolsView.removeUnmap.'+self.name+'.txt'
	#self.logPath+'/stderr.samtoolsView.qualFilter.'+self.name+'.txt'
	#self.logPath+'/stderr.samtoolsView.removeDups.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex1.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex2.'+self.name+'.txt'
	#self.logPath+'/stderr.buildIndex3.'+self.name+'.txt'
	#self.logPath+'/stderr.haplotypeCallerGatk.'+self.name+'.txt'
	#self.logPath+'/stderr.haplotypeCalling.'+self.name+'.txt'
	#self.logPath+'/stdout.haplotypeCalling.'+self.name+'.txt'
	#self.dataPath+'/'+self.name+'.gvcf '
	#self.logPath+'/stderr.qcSteps.'+self.name+'.txt'
	#self.logPath+'/stdout.qcSteps.'+self.name+'.txt'
	#self.dataPath+'/'+self.name+'.hs_metrics.perTargetCoverage.txt '
	#self.dataPath+'/'+str(filePairId)+'.sam'
	#self.dataPath+'/'+self.name+'.merged.sam'
	#self.dataPath+'/'+self.name+'.reAlignemntTargetIntervals.bed '
	#self.dataPath+'/'+self.name+'.BQSR.grp'+' '
	#self.dataPath+'/'+self.name+'.reCalibrated.bam\n'
	#self.dataPath+'/'+self.name+'.unmapRemoved.bam\n'
	#self.dataPath+'/'+self.name+'.qualFiltered.bam\n'
	#self.dataPath+'/'+self.name+'.noDuplicates.bam '
	#self.dataPath+'/'+self.name+'.callableLociSummary.txt '
	#self.dataPath+'/'+self.name+'.callableLoci.bed '
